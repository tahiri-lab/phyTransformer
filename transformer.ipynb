{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239334d3",
   "metadata": {},
   "source": [
    "Fine Tuning Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03631b2c-7816-4e1e-9b6e-e5b74e353abe",
   "metadata": {},
   "source": [
    "Install Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26cd99d5-ee0c-4c2e-b178-abdf4b3fb1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.43.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tokenizers) (0.24.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
      "Requirement already satisfied: requests in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: dill in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (2024.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (0.24.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (0.24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (2.4.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (0.33.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from peft) (4.43.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers->peft) (0.19.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-win_amd64.whl (11.0 MB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install tokenizers\n",
    "%pip install torch\n",
    "%pip install evaluate\n",
    "%pip install peft\n",
    "%pip install numpy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77fa65-3cfa-4e23-801f-c7a3821c1800",
   "metadata": {},
   "source": [
    "*import* libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646138c1-fa3e-410d-beb9-9dfbe5830666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94033cc6-0013-482b-af6d-6baf0c6e5368",
   "metadata": {},
   "source": [
    "Picking and DL a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cc6251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.81k/7.81k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0M/21.0M [00:01<00:00, 15.3MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 20.6MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.0M/42.0M [00:02<00:00, 20.3MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 190391.94 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 211835.68 examples/s]\n",
      "Generating unsupervised split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:00<00:00, 247590.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# how dataset was generated\n",
    "\n",
    "# load imdb data\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# define subsample size\n",
    "N = 1000 \n",
    "# generate indexes for random subsample\n",
    "rand_idx = np.random.randint(24999, size=N) \n",
    "\n",
    "# extract train and test data\n",
    "x_train = imdb_dataset['train'][rand_idx]['text']\n",
    "y_train = imdb_dataset['train'][rand_idx]['label']\n",
    "\n",
    "x_test = imdb_dataset['test'][rand_idx]['text']\n",
    "y_test = imdb_dataset['test'][rand_idx]['label']\n",
    "\n",
    "# create new dataset\n",
    "dataset = DatasetDict({'train':Dataset.from_dict({'label':y_train,'text':x_train}),\n",
    "                            'validation':Dataset.from_dict({'label':y_test,'text':x_test})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec18b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.497"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display % of training data with label=1\n",
    "np.array(dataset['train']['label']).sum()/len(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e9554",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7780dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Positive\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "# model_checkpoint = 'roberta-base' # you can alternatively use roberta-base but this model is bigger thus training will take longer\n",
    "\n",
    "# define label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "# generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de65ce43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bd5dd",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f730d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c1bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecf4841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1886.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2139.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23857f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a295310",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fb5d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c00f8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f061b",
   "metadata": {},
   "source": [
    "Apply untrained model to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1a36f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "It was good. - Positive\n",
      "Not a fan, don't recommed. - Positive\n",
      "Better than the first one. - Positive\n",
      "This is not worth watching even once. - Positive\n",
      "This one is a pass. - Positive\n"
     ]
    }
   ],
   "source": [
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7e93a",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3737a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['q_lin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9003b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=4, target_modules={'q_lin'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d3303dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e64a2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b054184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d47f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,500\n",
      "  Number of trainable parameters = 628,994\n",
      " 10%|â–ˆ         | 250/2500 [26:33<4:33:50,  7.30s/it]The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "                                                    \n",
      " 10%|â–ˆ         | 250/2500 [38:27<4:33:50,  7.30s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38785070180892944, 'eval_accuracy': {'accuracy': 0.869}, 'eval_runtime': 713.7462, 'eval_samples_per_second': 1.401, 'eval_steps_per_second': 0.35, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-250\\special_tokens_map.json\n",
      " 20%|â–ˆâ–ˆ        | 500/2500 [22:41:39<2:54:32,  5.24s/it]       The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3872, 'grad_norm': 0.44143182039260864, 'learning_rate': 0.0008, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 20%|â–ˆâ–ˆ        | 500/2500 [22:52:54<2:54:32,  5.24s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5043162703514099, 'eval_accuracy': {'accuracy': 0.861}, 'eval_runtime': 675.2604, 'eval_samples_per_second': 1.481, 'eval_steps_per_second': 0.37, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-500\\special_tokens_map.json\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 750/2500 [23:17:58<3:50:56,  7.92s/it]   The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "                                                       \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 750/2500 [23:30:06<3:50:56,  7.92s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7326446175575256, 'eval_accuracy': {'accuracy': 0.864}, 'eval_runtime': 728.4528, 'eval_samples_per_second': 1.373, 'eval_steps_per_second': 0.343, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-750\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-750\\special_tokens_map.json\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1000/2500 [23:57:01<3:06:57,  7.48s/it]  The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1756, 'grad_norm': 0.34345734119415283, 'learning_rate': 0.0006, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1000/2500 [24:08:15<3:06:57,  7.48s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8404889702796936, 'eval_accuracy': {'accuracy': 0.863}, 'eval_runtime': 673.0433, 'eval_samples_per_second': 1.486, 'eval_steps_per_second': 0.371, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1000\\special_tokens_map.json\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [70:50:12<2:48:17,  8.08s/it]       The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "                                                        \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [71:03:42<2:48:17,  8.08s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8373122811317444, 'eval_accuracy': {'accuracy': 0.868}, 'eval_runtime': 810.7918, 'eval_samples_per_second': 1.233, 'eval_steps_per_second': 0.308, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1250\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1250\\special_tokens_map.json\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1500/2500 [71:34:31<1:48:47,  6.53s/it]  The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.059, 'grad_norm': 0.00022015796275809407, 'learning_rate': 0.0004, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1500/2500 [71:48:45<1:48:47,  6.53s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9502548575401306, 'eval_accuracy': {'accuracy': 0.865}, 'eval_runtime': 853.9081, 'eval_samples_per_second': 1.171, 'eval_steps_per_second': 0.293, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1500\\special_tokens_map.json\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1750/2500 [72:17:20<1:21:23,  6.51s/it]  The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "                                                        \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1750/2500 [72:56:51<1:21:23,  6.51s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-1750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.100731372833252, 'eval_accuracy': {'accuracy': 0.866}, 'eval_runtime': 2371.2149, 'eval_samples_per_second': 0.422, 'eval_steps_per_second': 0.105, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1750\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-1750\\special_tokens_map.json\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2000/2500 [90:42:11<42:03,  5.05s/it]        The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0215, 'grad_norm': 0.15198169648647308, 'learning_rate': 0.0002, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2000/2500 [90:58:30<42:03,  5.05s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1344313621520996, 'eval_accuracy': {'accuracy': 0.867}, 'eval_runtime': 979.1018, 'eval_samples_per_second': 1.021, 'eval_steps_per_second': 0.255, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2000\\special_tokens_map.json\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2250/2500 [91:19:23<19:44,  4.74s/it]    The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "                                                      \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2250/2500 [91:27:59<19:44,  4.74s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1214934587478638, 'eval_accuracy': {'accuracy': 0.869}, 'eval_runtime': 516.2964, 'eval_samples_per_second': 1.937, 'eval_steps_per_second': 0.484, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2250\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2250\\special_tokens_map.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [91:57:58<00:00,  7.18s/it]    Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0047, 'grad_norm': 0.0022133863531053066, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\peft\\utils\\other.py:619: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b3f7692a-494a-4570-931e-f9aaec8ac603)') - silently ignoring the lookup for the file config.json in distilbert-base-uncased.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\peft\\utils\\save_and_load.py:218: UserWarning: Could not find a config file in distilbert-base-uncased - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text. If text are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "                                                      \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [92:08:08<00:00,  7.18s/it]Saving model checkpoint to distilbert-base-uncased-lora-text-classification\\checkpoint-2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.124175786972046, 'eval_accuracy': {'accuracy': 0.864}, 'eval_runtime': 599.0985, 'eval_samples_per_second': 1.669, 'eval_steps_per_second': 0.417, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer config file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-lora-text-classification\\checkpoint-2500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilbert-base-uncased-lora-text-classification\\checkpoint-250 (score: 0.38785070180892944).\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [92:08:11<00:00, 132.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 331691.5365, 'train_samples_per_second': 0.03, 'train_steps_per_second': 0.008, 'train_loss': 0.12962016286849976, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.12962016286849976, metrics={'train_runtime': 331691.5365, 'train_samples_per_second': 0.03, 'train_steps_per_second': 0.008, 'total_flos': 1103872775004480.0, 'train_loss': 0.12962016286849976, 'epoch': 10.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e64b7",
   "metadata": {},
   "source": [
    "Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb9a6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "It was good. - Positive\n",
      "Not a fan, don't recommed. - Negative\n",
      "Better than the first one. - Positive\n",
      "This is not worth watching even once. - Negative\n",
      "This one is a pass. - Negative\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu') # moving to mps for Mac (can alternatively do 'cpu')\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\") # moving to mps for Mac (can alternatively do 'cpu')\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12987f",
   "metadata": {},
   "source": [
    "Optional: push model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43554909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: notebook login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # ensure token gives write access\n",
    "\n",
    "# # option 2: key login\n",
    "# from huggingface_hub import login\n",
    "# write_key = 'hf_' # paste token here\n",
    "# login(write_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69629ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_name = 'shawhin' # your hf username or org name\n",
    "model_id = hf_name + \"/\" + model_checkpoint + \"-lora-text-classification\" # you can name the model whatever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(model_id) # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(model_id) # save trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63f59d",
   "metadata": {},
   "source": [
    "Optional: load peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ede7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to load peft model from hub for inference\n",
    "config = PeftConfig.from_pretrained(model_id)\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(inference_model, model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
