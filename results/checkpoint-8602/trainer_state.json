{
  "best_metric": 0.1433054357767105,
  "best_model_checkpoint": "./results\\checkpoint-8602",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 8602,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0011625203441060219,
      "grad_norm": 308.53497314453125,
      "learning_rate": 1.999224986437263e-05,
      "loss": 13.1543,
      "step": 10
    },
    {
      "epoch": 0.0023250406882120438,
      "grad_norm": 234.5973663330078,
      "learning_rate": 1.9984499728745256e-05,
      "loss": 12.989,
      "step": 20
    },
    {
      "epoch": 0.0034875610323180655,
      "grad_norm": 293.43304443359375,
      "learning_rate": 1.9976749593117883e-05,
      "loss": 12.3428,
      "step": 30
    },
    {
      "epoch": 0.0046500813764240876,
      "grad_norm": 329.8389892578125,
      "learning_rate": 1.9968999457490506e-05,
      "loss": 13.4335,
      "step": 40
    },
    {
      "epoch": 0.005812601720530109,
      "grad_norm": 347.05389404296875,
      "learning_rate": 1.9961249321863133e-05,
      "loss": 12.36,
      "step": 50
    },
    {
      "epoch": 0.006975122064636131,
      "grad_norm": 279.29364013671875,
      "learning_rate": 1.995349918623576e-05,
      "loss": 12.2141,
      "step": 60
    },
    {
      "epoch": 0.008137642408742153,
      "grad_norm": 287.16754150390625,
      "learning_rate": 1.9945749050608387e-05,
      "loss": 12.676,
      "step": 70
    },
    {
      "epoch": 0.009300162752848175,
      "grad_norm": 267.51153564453125,
      "learning_rate": 1.9937998914981015e-05,
      "loss": 10.0185,
      "step": 80
    },
    {
      "epoch": 0.010462683096954197,
      "grad_norm": 265.15740966796875,
      "learning_rate": 1.993024877935364e-05,
      "loss": 12.2024,
      "step": 90
    },
    {
      "epoch": 0.011625203441060218,
      "grad_norm": 284.8945617675781,
      "learning_rate": 1.9922498643726265e-05,
      "loss": 11.2048,
      "step": 100
    },
    {
      "epoch": 0.01278772378516624,
      "grad_norm": 146.5081787109375,
      "learning_rate": 1.9914748508098892e-05,
      "loss": 10.6161,
      "step": 110
    },
    {
      "epoch": 0.013950244129272262,
      "grad_norm": 272.9507751464844,
      "learning_rate": 1.990699837247152e-05,
      "loss": 7.9122,
      "step": 120
    },
    {
      "epoch": 0.015112764473378284,
      "grad_norm": 255.82931518554688,
      "learning_rate": 1.9899248236844146e-05,
      "loss": 7.1584,
      "step": 130
    },
    {
      "epoch": 0.016275284817484307,
      "grad_norm": 252.21963500976562,
      "learning_rate": 1.9891498101216773e-05,
      "loss": 8.4378,
      "step": 140
    },
    {
      "epoch": 0.01743780516159033,
      "grad_norm": 222.2823028564453,
      "learning_rate": 1.98837479655894e-05,
      "loss": 8.4397,
      "step": 150
    },
    {
      "epoch": 0.01860032550569635,
      "grad_norm": 158.05487060546875,
      "learning_rate": 1.9875997829962027e-05,
      "loss": 7.8908,
      "step": 160
    },
    {
      "epoch": 0.019762845849802372,
      "grad_norm": 203.68826293945312,
      "learning_rate": 1.986824769433465e-05,
      "loss": 5.8962,
      "step": 170
    },
    {
      "epoch": 0.020925366193908394,
      "grad_norm": 96.50814819335938,
      "learning_rate": 1.9860497558707278e-05,
      "loss": 8.1443,
      "step": 180
    },
    {
      "epoch": 0.022087886538014415,
      "grad_norm": 70.19296264648438,
      "learning_rate": 1.9852747423079905e-05,
      "loss": 6.1576,
      "step": 190
    },
    {
      "epoch": 0.023250406882120437,
      "grad_norm": 149.7716522216797,
      "learning_rate": 1.9844997287452532e-05,
      "loss": 4.6457,
      "step": 200
    },
    {
      "epoch": 0.02441292722622646,
      "grad_norm": 118.0990982055664,
      "learning_rate": 1.983724715182516e-05,
      "loss": 4.8969,
      "step": 210
    },
    {
      "epoch": 0.02557544757033248,
      "grad_norm": 83.66304779052734,
      "learning_rate": 1.9829497016197786e-05,
      "loss": 4.9567,
      "step": 220
    },
    {
      "epoch": 0.026737967914438502,
      "grad_norm": 53.30412673950195,
      "learning_rate": 1.9821746880570413e-05,
      "loss": 4.5653,
      "step": 230
    },
    {
      "epoch": 0.027900488258544524,
      "grad_norm": 104.2159652709961,
      "learning_rate": 1.9813996744943037e-05,
      "loss": 5.1014,
      "step": 240
    },
    {
      "epoch": 0.029063008602650545,
      "grad_norm": 48.25702667236328,
      "learning_rate": 1.9806246609315664e-05,
      "loss": 4.3439,
      "step": 250
    },
    {
      "epoch": 0.030225528946756567,
      "grad_norm": 99.10784912109375,
      "learning_rate": 1.979849647368829e-05,
      "loss": 4.5456,
      "step": 260
    },
    {
      "epoch": 0.03138804929086259,
      "grad_norm": 53.324092864990234,
      "learning_rate": 1.9790746338060918e-05,
      "loss": 4.6958,
      "step": 270
    },
    {
      "epoch": 0.032550569634968614,
      "grad_norm": 90.61752319335938,
      "learning_rate": 1.9782996202433545e-05,
      "loss": 4.6866,
      "step": 280
    },
    {
      "epoch": 0.033713089979074636,
      "grad_norm": 57.519474029541016,
      "learning_rate": 1.9775246066806172e-05,
      "loss": 3.6027,
      "step": 290
    },
    {
      "epoch": 0.03487561032318066,
      "grad_norm": 55.03780746459961,
      "learning_rate": 1.9767495931178796e-05,
      "loss": 4.4234,
      "step": 300
    },
    {
      "epoch": 0.03603813066728668,
      "grad_norm": 64.72797393798828,
      "learning_rate": 1.9759745795551423e-05,
      "loss": 4.7332,
      "step": 310
    },
    {
      "epoch": 0.0372006510113927,
      "grad_norm": 50.86366653442383,
      "learning_rate": 1.975199565992405e-05,
      "loss": 4.2982,
      "step": 320
    },
    {
      "epoch": 0.03836317135549872,
      "grad_norm": 65.185791015625,
      "learning_rate": 1.9744245524296677e-05,
      "loss": 3.9092,
      "step": 330
    },
    {
      "epoch": 0.039525691699604744,
      "grad_norm": 48.894142150878906,
      "learning_rate": 1.9736495388669304e-05,
      "loss": 3.8893,
      "step": 340
    },
    {
      "epoch": 0.040688212043710766,
      "grad_norm": 68.31568145751953,
      "learning_rate": 1.972874525304193e-05,
      "loss": 3.9546,
      "step": 350
    },
    {
      "epoch": 0.04185073238781679,
      "grad_norm": 90.7765884399414,
      "learning_rate": 1.9720995117414558e-05,
      "loss": 3.8866,
      "step": 360
    },
    {
      "epoch": 0.04301325273192281,
      "grad_norm": 81.31062316894531,
      "learning_rate": 1.971324498178718e-05,
      "loss": 4.0363,
      "step": 370
    },
    {
      "epoch": 0.04417577307602883,
      "grad_norm": 48.4810905456543,
      "learning_rate": 1.970549484615981e-05,
      "loss": 4.2251,
      "step": 380
    },
    {
      "epoch": 0.04533829342013485,
      "grad_norm": 40.5975341796875,
      "learning_rate": 1.9697744710532436e-05,
      "loss": 4.4393,
      "step": 390
    },
    {
      "epoch": 0.046500813764240874,
      "grad_norm": 46.8026008605957,
      "learning_rate": 1.9689994574905063e-05,
      "loss": 3.6945,
      "step": 400
    },
    {
      "epoch": 0.047663334108346896,
      "grad_norm": 56.157508850097656,
      "learning_rate": 1.968224443927769e-05,
      "loss": 4.0982,
      "step": 410
    },
    {
      "epoch": 0.04882585445245292,
      "grad_norm": 48.9852180480957,
      "learning_rate": 1.9674494303650317e-05,
      "loss": 3.5384,
      "step": 420
    },
    {
      "epoch": 0.04998837479655894,
      "grad_norm": 43.61091995239258,
      "learning_rate": 1.966674416802294e-05,
      "loss": 4.0512,
      "step": 430
    },
    {
      "epoch": 0.05115089514066496,
      "grad_norm": 52.77862548828125,
      "learning_rate": 1.9658994032395567e-05,
      "loss": 4.0387,
      "step": 440
    },
    {
      "epoch": 0.05231341548477098,
      "grad_norm": 49.27355194091797,
      "learning_rate": 1.9651243896768194e-05,
      "loss": 3.3324,
      "step": 450
    },
    {
      "epoch": 0.053475935828877004,
      "grad_norm": 66.89582824707031,
      "learning_rate": 1.964349376114082e-05,
      "loss": 3.8581,
      "step": 460
    },
    {
      "epoch": 0.054638456172983026,
      "grad_norm": 68.3743896484375,
      "learning_rate": 1.963574362551345e-05,
      "loss": 3.3596,
      "step": 470
    },
    {
      "epoch": 0.05580097651708905,
      "grad_norm": 49.7534294128418,
      "learning_rate": 1.9627993489886076e-05,
      "loss": 3.0063,
      "step": 480
    },
    {
      "epoch": 0.05696349686119507,
      "grad_norm": 77.73548889160156,
      "learning_rate": 1.9620243354258703e-05,
      "loss": 3.9707,
      "step": 490
    },
    {
      "epoch": 0.05812601720530109,
      "grad_norm": 36.81357955932617,
      "learning_rate": 1.9612493218631326e-05,
      "loss": 3.916,
      "step": 500
    },
    {
      "epoch": 0.05928853754940711,
      "grad_norm": 57.94560241699219,
      "learning_rate": 1.9604743083003953e-05,
      "loss": 3.8292,
      "step": 510
    },
    {
      "epoch": 0.060451057893513134,
      "grad_norm": 49.00947189331055,
      "learning_rate": 1.959699294737658e-05,
      "loss": 3.3116,
      "step": 520
    },
    {
      "epoch": 0.061613578237619156,
      "grad_norm": 38.645668029785156,
      "learning_rate": 1.9589242811749207e-05,
      "loss": 3.8517,
      "step": 530
    },
    {
      "epoch": 0.06277609858172518,
      "grad_norm": 124.49882507324219,
      "learning_rate": 1.9581492676121834e-05,
      "loss": 4.0036,
      "step": 540
    },
    {
      "epoch": 0.0639386189258312,
      "grad_norm": 68.3646240234375,
      "learning_rate": 1.957374254049446e-05,
      "loss": 3.5726,
      "step": 550
    },
    {
      "epoch": 0.06510113926993723,
      "grad_norm": 50.52486801147461,
      "learning_rate": 1.956599240486709e-05,
      "loss": 4.306,
      "step": 560
    },
    {
      "epoch": 0.06626365961404325,
      "grad_norm": 51.2120361328125,
      "learning_rate": 1.9558242269239712e-05,
      "loss": 3.8721,
      "step": 570
    },
    {
      "epoch": 0.06742617995814927,
      "grad_norm": 52.49514389038086,
      "learning_rate": 1.955049213361234e-05,
      "loss": 3.6583,
      "step": 580
    },
    {
      "epoch": 0.06858870030225529,
      "grad_norm": 46.3749885559082,
      "learning_rate": 1.9542741997984966e-05,
      "loss": 3.3572,
      "step": 590
    },
    {
      "epoch": 0.06975122064636131,
      "grad_norm": 43.01927947998047,
      "learning_rate": 1.9534991862357593e-05,
      "loss": 3.3417,
      "step": 600
    },
    {
      "epoch": 0.07091374099046734,
      "grad_norm": 48.422054290771484,
      "learning_rate": 1.952724172673022e-05,
      "loss": 3.3603,
      "step": 610
    },
    {
      "epoch": 0.07207626133457336,
      "grad_norm": 31.899606704711914,
      "learning_rate": 1.9519491591102847e-05,
      "loss": 3.3334,
      "step": 620
    },
    {
      "epoch": 0.07323878167867938,
      "grad_norm": 32.44663619995117,
      "learning_rate": 1.951174145547547e-05,
      "loss": 2.7296,
      "step": 630
    },
    {
      "epoch": 0.0744013020227854,
      "grad_norm": 44.70305252075195,
      "learning_rate": 1.9503991319848098e-05,
      "loss": 3.1456,
      "step": 640
    },
    {
      "epoch": 0.07556382236689142,
      "grad_norm": 41.84536361694336,
      "learning_rate": 1.9496241184220725e-05,
      "loss": 3.0092,
      "step": 650
    },
    {
      "epoch": 0.07672634271099744,
      "grad_norm": 49.7757682800293,
      "learning_rate": 1.9488491048593352e-05,
      "loss": 3.6008,
      "step": 660
    },
    {
      "epoch": 0.07788886305510347,
      "grad_norm": 50.518009185791016,
      "learning_rate": 1.948074091296598e-05,
      "loss": 3.5504,
      "step": 670
    },
    {
      "epoch": 0.07905138339920949,
      "grad_norm": 61.91917037963867,
      "learning_rate": 1.9472990777338606e-05,
      "loss": 3.1987,
      "step": 680
    },
    {
      "epoch": 0.08021390374331551,
      "grad_norm": 32.720802307128906,
      "learning_rate": 1.9465240641711233e-05,
      "loss": 2.9162,
      "step": 690
    },
    {
      "epoch": 0.08137642408742153,
      "grad_norm": 43.053558349609375,
      "learning_rate": 1.9457490506083857e-05,
      "loss": 3.0375,
      "step": 700
    },
    {
      "epoch": 0.08253894443152755,
      "grad_norm": 51.349853515625,
      "learning_rate": 1.9449740370456484e-05,
      "loss": 3.0805,
      "step": 710
    },
    {
      "epoch": 0.08370146477563357,
      "grad_norm": 49.667320251464844,
      "learning_rate": 1.944199023482911e-05,
      "loss": 3.0815,
      "step": 720
    },
    {
      "epoch": 0.0848639851197396,
      "grad_norm": 37.868953704833984,
      "learning_rate": 1.9434240099201738e-05,
      "loss": 3.2861,
      "step": 730
    },
    {
      "epoch": 0.08602650546384562,
      "grad_norm": 39.71622848510742,
      "learning_rate": 1.9426489963574365e-05,
      "loss": 2.6187,
      "step": 740
    },
    {
      "epoch": 0.08718902580795164,
      "grad_norm": 49.06291198730469,
      "learning_rate": 1.9418739827946992e-05,
      "loss": 2.7372,
      "step": 750
    },
    {
      "epoch": 0.08835154615205766,
      "grad_norm": 47.44770812988281,
      "learning_rate": 1.9410989692319616e-05,
      "loss": 2.6374,
      "step": 760
    },
    {
      "epoch": 0.08951406649616368,
      "grad_norm": 39.00138854980469,
      "learning_rate": 1.9403239556692243e-05,
      "loss": 3.0569,
      "step": 770
    },
    {
      "epoch": 0.0906765868402697,
      "grad_norm": 26.797685623168945,
      "learning_rate": 1.939548942106487e-05,
      "loss": 3.0035,
      "step": 780
    },
    {
      "epoch": 0.09183910718437573,
      "grad_norm": 47.91740798950195,
      "learning_rate": 1.9387739285437497e-05,
      "loss": 2.9434,
      "step": 790
    },
    {
      "epoch": 0.09300162752848175,
      "grad_norm": 23.736072540283203,
      "learning_rate": 1.9379989149810124e-05,
      "loss": 2.6781,
      "step": 800
    },
    {
      "epoch": 0.09416414787258777,
      "grad_norm": 50.65470504760742,
      "learning_rate": 1.937223901418275e-05,
      "loss": 2.5426,
      "step": 810
    },
    {
      "epoch": 0.09532666821669379,
      "grad_norm": 51.805572509765625,
      "learning_rate": 1.9364488878555378e-05,
      "loss": 3.804,
      "step": 820
    },
    {
      "epoch": 0.09648918856079981,
      "grad_norm": 43.83583068847656,
      "learning_rate": 1.9356738742928e-05,
      "loss": 3.784,
      "step": 830
    },
    {
      "epoch": 0.09765170890490583,
      "grad_norm": 46.44221115112305,
      "learning_rate": 1.934898860730063e-05,
      "loss": 3.3325,
      "step": 840
    },
    {
      "epoch": 0.09881422924901186,
      "grad_norm": 47.84907531738281,
      "learning_rate": 1.9341238471673256e-05,
      "loss": 2.6913,
      "step": 850
    },
    {
      "epoch": 0.09997674959311788,
      "grad_norm": 48.2289924621582,
      "learning_rate": 1.9333488336045883e-05,
      "loss": 3.8673,
      "step": 860
    },
    {
      "epoch": 0.1011392699372239,
      "grad_norm": 43.29848098754883,
      "learning_rate": 1.932573820041851e-05,
      "loss": 2.6387,
      "step": 870
    },
    {
      "epoch": 0.10230179028132992,
      "grad_norm": 24.956619262695312,
      "learning_rate": 1.9317988064791137e-05,
      "loss": 3.0772,
      "step": 880
    },
    {
      "epoch": 0.10346431062543594,
      "grad_norm": 67.13114929199219,
      "learning_rate": 1.9310237929163764e-05,
      "loss": 2.7506,
      "step": 890
    },
    {
      "epoch": 0.10462683096954196,
      "grad_norm": 61.78084182739258,
      "learning_rate": 1.9302487793536387e-05,
      "loss": 4.264,
      "step": 900
    },
    {
      "epoch": 0.10578935131364799,
      "grad_norm": 53.277137756347656,
      "learning_rate": 1.9294737657909014e-05,
      "loss": 3.1576,
      "step": 910
    },
    {
      "epoch": 0.10695187165775401,
      "grad_norm": 42.64341354370117,
      "learning_rate": 1.928698752228164e-05,
      "loss": 2.1178,
      "step": 920
    },
    {
      "epoch": 0.10811439200186003,
      "grad_norm": 46.53891372680664,
      "learning_rate": 1.927923738665427e-05,
      "loss": 4.2017,
      "step": 930
    },
    {
      "epoch": 0.10927691234596605,
      "grad_norm": 39.942623138427734,
      "learning_rate": 1.9271487251026895e-05,
      "loss": 2.9373,
      "step": 940
    },
    {
      "epoch": 0.11043943269007207,
      "grad_norm": 31.09065818786621,
      "learning_rate": 1.9263737115399523e-05,
      "loss": 2.9237,
      "step": 950
    },
    {
      "epoch": 0.1116019530341781,
      "grad_norm": 48.15889358520508,
      "learning_rate": 1.9255986979772146e-05,
      "loss": 3.6569,
      "step": 960
    },
    {
      "epoch": 0.11276447337828412,
      "grad_norm": 44.4670524597168,
      "learning_rate": 1.9248236844144773e-05,
      "loss": 3.5629,
      "step": 970
    },
    {
      "epoch": 0.11392699372239014,
      "grad_norm": 38.14333724975586,
      "learning_rate": 1.92404867085174e-05,
      "loss": 3.8117,
      "step": 980
    },
    {
      "epoch": 0.11508951406649616,
      "grad_norm": 35.15195083618164,
      "learning_rate": 1.9232736572890027e-05,
      "loss": 3.8278,
      "step": 990
    },
    {
      "epoch": 0.11625203441060218,
      "grad_norm": 47.181217193603516,
      "learning_rate": 1.9224986437262654e-05,
      "loss": 3.1846,
      "step": 1000
    },
    {
      "epoch": 0.1174145547547082,
      "grad_norm": 42.05508804321289,
      "learning_rate": 1.921723630163528e-05,
      "loss": 3.2181,
      "step": 1010
    },
    {
      "epoch": 0.11857707509881422,
      "grad_norm": 68.5627212524414,
      "learning_rate": 1.920948616600791e-05,
      "loss": 3.7154,
      "step": 1020
    },
    {
      "epoch": 0.11973959544292025,
      "grad_norm": 44.13667678833008,
      "learning_rate": 1.9201736030380532e-05,
      "loss": 2.9488,
      "step": 1030
    },
    {
      "epoch": 0.12090211578702627,
      "grad_norm": 68.32272338867188,
      "learning_rate": 1.919398589475316e-05,
      "loss": 2.7806,
      "step": 1040
    },
    {
      "epoch": 0.12206463613113229,
      "grad_norm": 34.26401901245117,
      "learning_rate": 1.9186235759125786e-05,
      "loss": 3.2457,
      "step": 1050
    },
    {
      "epoch": 0.12322715647523831,
      "grad_norm": 40.54258346557617,
      "learning_rate": 1.9178485623498413e-05,
      "loss": 3.8698,
      "step": 1060
    },
    {
      "epoch": 0.12438967681934433,
      "grad_norm": 43.092281341552734,
      "learning_rate": 1.917073548787104e-05,
      "loss": 3.8853,
      "step": 1070
    },
    {
      "epoch": 0.12555219716345037,
      "grad_norm": 53.95992660522461,
      "learning_rate": 1.9162985352243667e-05,
      "loss": 2.3472,
      "step": 1080
    },
    {
      "epoch": 0.1267147175075564,
      "grad_norm": 80.62300109863281,
      "learning_rate": 1.915523521661629e-05,
      "loss": 2.7544,
      "step": 1090
    },
    {
      "epoch": 0.1278772378516624,
      "grad_norm": 44.67237091064453,
      "learning_rate": 1.9147485080988918e-05,
      "loss": 3.3621,
      "step": 1100
    },
    {
      "epoch": 0.12903975819576843,
      "grad_norm": 35.962547302246094,
      "learning_rate": 1.9139734945361545e-05,
      "loss": 2.8382,
      "step": 1110
    },
    {
      "epoch": 0.13020227853987446,
      "grad_norm": 44.4796257019043,
      "learning_rate": 1.9131984809734172e-05,
      "loss": 3.0318,
      "step": 1120
    },
    {
      "epoch": 0.13136479888398048,
      "grad_norm": 33.25881576538086,
      "learning_rate": 1.91242346741068e-05,
      "loss": 3.1154,
      "step": 1130
    },
    {
      "epoch": 0.1325273192280865,
      "grad_norm": 37.10981369018555,
      "learning_rate": 1.9116484538479426e-05,
      "loss": 3.3697,
      "step": 1140
    },
    {
      "epoch": 0.13368983957219252,
      "grad_norm": 43.003604888916016,
      "learning_rate": 1.9108734402852053e-05,
      "loss": 3.1592,
      "step": 1150
    },
    {
      "epoch": 0.13485235991629854,
      "grad_norm": 42.96521759033203,
      "learning_rate": 1.9100984267224677e-05,
      "loss": 2.4403,
      "step": 1160
    },
    {
      "epoch": 0.13601488026040456,
      "grad_norm": 49.0849609375,
      "learning_rate": 1.9093234131597304e-05,
      "loss": 2.2715,
      "step": 1170
    },
    {
      "epoch": 0.13717740060451059,
      "grad_norm": 40.63064956665039,
      "learning_rate": 1.908548399596993e-05,
      "loss": 3.3686,
      "step": 1180
    },
    {
      "epoch": 0.1383399209486166,
      "grad_norm": 47.79220962524414,
      "learning_rate": 1.9077733860342558e-05,
      "loss": 2.523,
      "step": 1190
    },
    {
      "epoch": 0.13950244129272263,
      "grad_norm": 59.20427703857422,
      "learning_rate": 1.9069983724715185e-05,
      "loss": 2.955,
      "step": 1200
    },
    {
      "epoch": 0.14066496163682865,
      "grad_norm": 35.5565185546875,
      "learning_rate": 1.9062233589087812e-05,
      "loss": 3.2779,
      "step": 1210
    },
    {
      "epoch": 0.14182748198093467,
      "grad_norm": 42.054466247558594,
      "learning_rate": 1.905448345346044e-05,
      "loss": 2.8497,
      "step": 1220
    },
    {
      "epoch": 0.1429900023250407,
      "grad_norm": 35.66536331176758,
      "learning_rate": 1.9046733317833063e-05,
      "loss": 2.4434,
      "step": 1230
    },
    {
      "epoch": 0.14415252266914672,
      "grad_norm": 39.22233581542969,
      "learning_rate": 1.903898318220569e-05,
      "loss": 3.134,
      "step": 1240
    },
    {
      "epoch": 0.14531504301325274,
      "grad_norm": 36.00245666503906,
      "learning_rate": 1.9031233046578317e-05,
      "loss": 3.6584,
      "step": 1250
    },
    {
      "epoch": 0.14647756335735876,
      "grad_norm": 33.80014419555664,
      "learning_rate": 1.9023482910950944e-05,
      "loss": 3.8119,
      "step": 1260
    },
    {
      "epoch": 0.14764008370146478,
      "grad_norm": 41.29130554199219,
      "learning_rate": 1.901573277532357e-05,
      "loss": 3.0743,
      "step": 1270
    },
    {
      "epoch": 0.1488026040455708,
      "grad_norm": 36.70635986328125,
      "learning_rate": 1.9007982639696198e-05,
      "loss": 4.8513,
      "step": 1280
    },
    {
      "epoch": 0.14996512438967682,
      "grad_norm": 33.8948974609375,
      "learning_rate": 1.900023250406882e-05,
      "loss": 3.9141,
      "step": 1290
    },
    {
      "epoch": 0.15112764473378285,
      "grad_norm": 55.10498046875,
      "learning_rate": 1.899248236844145e-05,
      "loss": 2.6354,
      "step": 1300
    },
    {
      "epoch": 0.15229016507788887,
      "grad_norm": 35.837013244628906,
      "learning_rate": 1.8984732232814075e-05,
      "loss": 3.1978,
      "step": 1310
    },
    {
      "epoch": 0.1534526854219949,
      "grad_norm": 29.4664363861084,
      "learning_rate": 1.8976982097186702e-05,
      "loss": 2.6436,
      "step": 1320
    },
    {
      "epoch": 0.1546152057661009,
      "grad_norm": 30.946430206298828,
      "learning_rate": 1.896923196155933e-05,
      "loss": 2.8596,
      "step": 1330
    },
    {
      "epoch": 0.15577772611020693,
      "grad_norm": 38.60599899291992,
      "learning_rate": 1.8961481825931957e-05,
      "loss": 3.2378,
      "step": 1340
    },
    {
      "epoch": 0.15694024645431295,
      "grad_norm": 44.17336654663086,
      "learning_rate": 1.8953731690304584e-05,
      "loss": 3.622,
      "step": 1350
    },
    {
      "epoch": 0.15810276679841898,
      "grad_norm": 28.618223190307617,
      "learning_rate": 1.8945981554677207e-05,
      "loss": 3.8029,
      "step": 1360
    },
    {
      "epoch": 0.159265287142525,
      "grad_norm": 47.90302658081055,
      "learning_rate": 1.8938231419049834e-05,
      "loss": 3.2952,
      "step": 1370
    },
    {
      "epoch": 0.16042780748663102,
      "grad_norm": 33.89387130737305,
      "learning_rate": 1.893048128342246e-05,
      "loss": 2.4615,
      "step": 1380
    },
    {
      "epoch": 0.16159032783073704,
      "grad_norm": 43.47142028808594,
      "learning_rate": 1.892273114779509e-05,
      "loss": 3.8857,
      "step": 1390
    },
    {
      "epoch": 0.16275284817484306,
      "grad_norm": 52.7403450012207,
      "learning_rate": 1.8914981012167715e-05,
      "loss": 2.8602,
      "step": 1400
    },
    {
      "epoch": 0.16391536851894908,
      "grad_norm": 35.084434509277344,
      "learning_rate": 1.8907230876540342e-05,
      "loss": 3.2781,
      "step": 1410
    },
    {
      "epoch": 0.1650778888630551,
      "grad_norm": 40.21784210205078,
      "learning_rate": 1.8899480740912966e-05,
      "loss": 3.6766,
      "step": 1420
    },
    {
      "epoch": 0.16624040920716113,
      "grad_norm": 28.00094985961914,
      "learning_rate": 1.8891730605285593e-05,
      "loss": 2.2672,
      "step": 1430
    },
    {
      "epoch": 0.16740292955126715,
      "grad_norm": 33.77674102783203,
      "learning_rate": 1.888398046965822e-05,
      "loss": 2.4267,
      "step": 1440
    },
    {
      "epoch": 0.16856544989537317,
      "grad_norm": 62.596805572509766,
      "learning_rate": 1.8876230334030847e-05,
      "loss": 4.1687,
      "step": 1450
    },
    {
      "epoch": 0.1697279702394792,
      "grad_norm": 35.728389739990234,
      "learning_rate": 1.8868480198403474e-05,
      "loss": 2.9572,
      "step": 1460
    },
    {
      "epoch": 0.17089049058358521,
      "grad_norm": 30.700937271118164,
      "learning_rate": 1.88607300627761e-05,
      "loss": 3.1925,
      "step": 1470
    },
    {
      "epoch": 0.17205301092769124,
      "grad_norm": 36.69464874267578,
      "learning_rate": 1.8852979927148728e-05,
      "loss": 3.1997,
      "step": 1480
    },
    {
      "epoch": 0.17321553127179726,
      "grad_norm": 42.40373229980469,
      "learning_rate": 1.8845229791521352e-05,
      "loss": 2.3651,
      "step": 1490
    },
    {
      "epoch": 0.17437805161590328,
      "grad_norm": 35.602420806884766,
      "learning_rate": 1.883747965589398e-05,
      "loss": 2.4709,
      "step": 1500
    },
    {
      "epoch": 0.1755405719600093,
      "grad_norm": 28.421119689941406,
      "learning_rate": 1.8829729520266606e-05,
      "loss": 2.9217,
      "step": 1510
    },
    {
      "epoch": 0.17670309230411532,
      "grad_norm": 42.7942008972168,
      "learning_rate": 1.8821979384639233e-05,
      "loss": 3.6065,
      "step": 1520
    },
    {
      "epoch": 0.17786561264822134,
      "grad_norm": 32.805503845214844,
      "learning_rate": 1.881422924901186e-05,
      "loss": 2.7103,
      "step": 1530
    },
    {
      "epoch": 0.17902813299232737,
      "grad_norm": 47.133338928222656,
      "learning_rate": 1.8806479113384487e-05,
      "loss": 2.9743,
      "step": 1540
    },
    {
      "epoch": 0.1801906533364334,
      "grad_norm": 38.46220397949219,
      "learning_rate": 1.8798728977757114e-05,
      "loss": 3.4773,
      "step": 1550
    },
    {
      "epoch": 0.1813531736805394,
      "grad_norm": 38.224571228027344,
      "learning_rate": 1.8790978842129738e-05,
      "loss": 2.7837,
      "step": 1560
    },
    {
      "epoch": 0.18251569402464543,
      "grad_norm": 41.07950210571289,
      "learning_rate": 1.8783228706502365e-05,
      "loss": 2.3888,
      "step": 1570
    },
    {
      "epoch": 0.18367821436875145,
      "grad_norm": 38.12181091308594,
      "learning_rate": 1.8775478570874992e-05,
      "loss": 2.8337,
      "step": 1580
    },
    {
      "epoch": 0.18484073471285747,
      "grad_norm": 44.32191467285156,
      "learning_rate": 1.876772843524762e-05,
      "loss": 3.1028,
      "step": 1590
    },
    {
      "epoch": 0.1860032550569635,
      "grad_norm": 33.65934371948242,
      "learning_rate": 1.8759978299620246e-05,
      "loss": 3.0587,
      "step": 1600
    },
    {
      "epoch": 0.18716577540106952,
      "grad_norm": 30.943523406982422,
      "learning_rate": 1.8752228163992873e-05,
      "loss": 2.3839,
      "step": 1610
    },
    {
      "epoch": 0.18832829574517554,
      "grad_norm": 36.654640197753906,
      "learning_rate": 1.8744478028365497e-05,
      "loss": 2.6758,
      "step": 1620
    },
    {
      "epoch": 0.18949081608928156,
      "grad_norm": 40.352317810058594,
      "learning_rate": 1.8736727892738124e-05,
      "loss": 3.4021,
      "step": 1630
    },
    {
      "epoch": 0.19065333643338758,
      "grad_norm": 43.678836822509766,
      "learning_rate": 1.872897775711075e-05,
      "loss": 3.96,
      "step": 1640
    },
    {
      "epoch": 0.1918158567774936,
      "grad_norm": 45.16188049316406,
      "learning_rate": 1.8721227621483378e-05,
      "loss": 3.2895,
      "step": 1650
    },
    {
      "epoch": 0.19297837712159963,
      "grad_norm": 35.08848190307617,
      "learning_rate": 1.8713477485856005e-05,
      "loss": 3.413,
      "step": 1660
    },
    {
      "epoch": 0.19414089746570565,
      "grad_norm": 31.97042465209961,
      "learning_rate": 1.8705727350228632e-05,
      "loss": 3.3802,
      "step": 1670
    },
    {
      "epoch": 0.19530341780981167,
      "grad_norm": 36.03877639770508,
      "learning_rate": 1.869797721460126e-05,
      "loss": 2.4214,
      "step": 1680
    },
    {
      "epoch": 0.1964659381539177,
      "grad_norm": 37.36772918701172,
      "learning_rate": 1.8690227078973882e-05,
      "loss": 3.3718,
      "step": 1690
    },
    {
      "epoch": 0.1976284584980237,
      "grad_norm": 30.044389724731445,
      "learning_rate": 1.868247694334651e-05,
      "loss": 3.4936,
      "step": 1700
    },
    {
      "epoch": 0.19879097884212973,
      "grad_norm": 33.3907585144043,
      "learning_rate": 1.8674726807719137e-05,
      "loss": 2.6532,
      "step": 1710
    },
    {
      "epoch": 0.19995349918623576,
      "grad_norm": 38.58320617675781,
      "learning_rate": 1.8666976672091764e-05,
      "loss": 3.0659,
      "step": 1720
    },
    {
      "epoch": 0.20111601953034178,
      "grad_norm": 26.9334659576416,
      "learning_rate": 1.865922653646439e-05,
      "loss": 3.9241,
      "step": 1730
    },
    {
      "epoch": 0.2022785398744478,
      "grad_norm": 30.540956497192383,
      "learning_rate": 1.8651476400837018e-05,
      "loss": 3.0934,
      "step": 1740
    },
    {
      "epoch": 0.20344106021855382,
      "grad_norm": 21.72470474243164,
      "learning_rate": 1.864372626520964e-05,
      "loss": 2.3187,
      "step": 1750
    },
    {
      "epoch": 0.20460358056265984,
      "grad_norm": 28.072404861450195,
      "learning_rate": 1.863597612958227e-05,
      "loss": 2.8675,
      "step": 1760
    },
    {
      "epoch": 0.20576610090676586,
      "grad_norm": 35.0494270324707,
      "learning_rate": 1.8628225993954895e-05,
      "loss": 3.5572,
      "step": 1770
    },
    {
      "epoch": 0.20692862125087189,
      "grad_norm": 45.18695068359375,
      "learning_rate": 1.8620475858327522e-05,
      "loss": 2.9583,
      "step": 1780
    },
    {
      "epoch": 0.2080911415949779,
      "grad_norm": 35.42127227783203,
      "learning_rate": 1.861272572270015e-05,
      "loss": 3.3792,
      "step": 1790
    },
    {
      "epoch": 0.20925366193908393,
      "grad_norm": 36.283145904541016,
      "learning_rate": 1.8604975587072776e-05,
      "loss": 3.8306,
      "step": 1800
    },
    {
      "epoch": 0.21041618228318995,
      "grad_norm": 27.688020706176758,
      "learning_rate": 1.8597225451445403e-05,
      "loss": 3.2973,
      "step": 1810
    },
    {
      "epoch": 0.21157870262729597,
      "grad_norm": 30.172393798828125,
      "learning_rate": 1.8589475315818027e-05,
      "loss": 3.0814,
      "step": 1820
    },
    {
      "epoch": 0.212741222971402,
      "grad_norm": 35.91153335571289,
      "learning_rate": 1.8581725180190654e-05,
      "loss": 2.6427,
      "step": 1830
    },
    {
      "epoch": 0.21390374331550802,
      "grad_norm": 37.27772521972656,
      "learning_rate": 1.857397504456328e-05,
      "loss": 2.8218,
      "step": 1840
    },
    {
      "epoch": 0.21506626365961404,
      "grad_norm": 35.634918212890625,
      "learning_rate": 1.8566224908935908e-05,
      "loss": 4.0111,
      "step": 1850
    },
    {
      "epoch": 0.21622878400372006,
      "grad_norm": 39.63915252685547,
      "learning_rate": 1.8558474773308535e-05,
      "loss": 2.4515,
      "step": 1860
    },
    {
      "epoch": 0.21739130434782608,
      "grad_norm": 35.701236724853516,
      "learning_rate": 1.8550724637681162e-05,
      "loss": 3.539,
      "step": 1870
    },
    {
      "epoch": 0.2185538246919321,
      "grad_norm": 45.46437454223633,
      "learning_rate": 1.854297450205379e-05,
      "loss": 3.1089,
      "step": 1880
    },
    {
      "epoch": 0.21971634503603812,
      "grad_norm": 28.061996459960938,
      "learning_rate": 1.8535224366426413e-05,
      "loss": 2.3326,
      "step": 1890
    },
    {
      "epoch": 0.22087886538014415,
      "grad_norm": 35.32801055908203,
      "learning_rate": 1.852747423079904e-05,
      "loss": 2.7328,
      "step": 1900
    },
    {
      "epoch": 0.22204138572425017,
      "grad_norm": 32.74826431274414,
      "learning_rate": 1.8519724095171667e-05,
      "loss": 3.6513,
      "step": 1910
    },
    {
      "epoch": 0.2232039060683562,
      "grad_norm": 30.50882911682129,
      "learning_rate": 1.8511973959544294e-05,
      "loss": 3.86,
      "step": 1920
    },
    {
      "epoch": 0.2243664264124622,
      "grad_norm": 30.381467819213867,
      "learning_rate": 1.850422382391692e-05,
      "loss": 3.1362,
      "step": 1930
    },
    {
      "epoch": 0.22552894675656823,
      "grad_norm": 24.512372970581055,
      "learning_rate": 1.8496473688289548e-05,
      "loss": 2.4096,
      "step": 1940
    },
    {
      "epoch": 0.22669146710067425,
      "grad_norm": 30.90060806274414,
      "learning_rate": 1.8488723552662172e-05,
      "loss": 4.1335,
      "step": 1950
    },
    {
      "epoch": 0.22785398744478028,
      "grad_norm": 32.17118453979492,
      "learning_rate": 1.84809734170348e-05,
      "loss": 3.2888,
      "step": 1960
    },
    {
      "epoch": 0.2290165077888863,
      "grad_norm": 36.774993896484375,
      "learning_rate": 1.8473223281407426e-05,
      "loss": 2.7288,
      "step": 1970
    },
    {
      "epoch": 0.23017902813299232,
      "grad_norm": 40.02469253540039,
      "learning_rate": 1.8465473145780053e-05,
      "loss": 2.6279,
      "step": 1980
    },
    {
      "epoch": 0.23134154847709834,
      "grad_norm": 36.46562957763672,
      "learning_rate": 1.845772301015268e-05,
      "loss": 2.4531,
      "step": 1990
    },
    {
      "epoch": 0.23250406882120436,
      "grad_norm": 30.83980941772461,
      "learning_rate": 1.8449972874525307e-05,
      "loss": 3.3884,
      "step": 2000
    },
    {
      "epoch": 0.23366658916531038,
      "grad_norm": 33.049373626708984,
      "learning_rate": 1.8442222738897934e-05,
      "loss": 2.6213,
      "step": 2010
    },
    {
      "epoch": 0.2348291095094164,
      "grad_norm": 23.19053077697754,
      "learning_rate": 1.8434472603270558e-05,
      "loss": 2.747,
      "step": 2020
    },
    {
      "epoch": 0.23599162985352243,
      "grad_norm": 34.313194274902344,
      "learning_rate": 1.8426722467643185e-05,
      "loss": 3.9422,
      "step": 2030
    },
    {
      "epoch": 0.23715415019762845,
      "grad_norm": 38.23518371582031,
      "learning_rate": 1.8418972332015812e-05,
      "loss": 4.4014,
      "step": 2040
    },
    {
      "epoch": 0.23831667054173447,
      "grad_norm": 29.82108497619629,
      "learning_rate": 1.841122219638844e-05,
      "loss": 2.5535,
      "step": 2050
    },
    {
      "epoch": 0.2394791908858405,
      "grad_norm": 27.800344467163086,
      "learning_rate": 1.8403472060761066e-05,
      "loss": 3.1778,
      "step": 2060
    },
    {
      "epoch": 0.24064171122994651,
      "grad_norm": 23.59233856201172,
      "learning_rate": 1.8395721925133693e-05,
      "loss": 2.9787,
      "step": 2070
    },
    {
      "epoch": 0.24180423157405254,
      "grad_norm": 30.91176414489746,
      "learning_rate": 1.8387971789506316e-05,
      "loss": 3.0497,
      "step": 2080
    },
    {
      "epoch": 0.24296675191815856,
      "grad_norm": 30.578577041625977,
      "learning_rate": 1.8380221653878944e-05,
      "loss": 3.5953,
      "step": 2090
    },
    {
      "epoch": 0.24412927226226458,
      "grad_norm": 44.26344680786133,
      "learning_rate": 1.837247151825157e-05,
      "loss": 2.7207,
      "step": 2100
    },
    {
      "epoch": 0.2452917926063706,
      "grad_norm": 31.40186309814453,
      "learning_rate": 1.8364721382624198e-05,
      "loss": 2.6894,
      "step": 2110
    },
    {
      "epoch": 0.24645431295047662,
      "grad_norm": 34.56763458251953,
      "learning_rate": 1.8356971246996825e-05,
      "loss": 3.7383,
      "step": 2120
    },
    {
      "epoch": 0.24761683329458264,
      "grad_norm": 30.405624389648438,
      "learning_rate": 1.834922111136945e-05,
      "loss": 4.1087,
      "step": 2130
    },
    {
      "epoch": 0.24877935363868867,
      "grad_norm": 35.5006217956543,
      "learning_rate": 1.834147097574208e-05,
      "loss": 2.7201,
      "step": 2140
    },
    {
      "epoch": 0.2499418739827947,
      "grad_norm": 38.6473274230957,
      "learning_rate": 1.8333720840114702e-05,
      "loss": 2.6028,
      "step": 2150
    },
    {
      "epoch": 0.25110439432690074,
      "grad_norm": 29.008342742919922,
      "learning_rate": 1.832597070448733e-05,
      "loss": 2.9275,
      "step": 2160
    },
    {
      "epoch": 0.25226691467100676,
      "grad_norm": 25.756364822387695,
      "learning_rate": 1.8318220568859956e-05,
      "loss": 2.6916,
      "step": 2170
    },
    {
      "epoch": 0.2534294350151128,
      "grad_norm": 33.24795913696289,
      "learning_rate": 1.8310470433232583e-05,
      "loss": 2.9242,
      "step": 2180
    },
    {
      "epoch": 0.2545919553592188,
      "grad_norm": 28.678091049194336,
      "learning_rate": 1.830272029760521e-05,
      "loss": 3.1201,
      "step": 2190
    },
    {
      "epoch": 0.2557544757033248,
      "grad_norm": 35.56521224975586,
      "learning_rate": 1.8294970161977838e-05,
      "loss": 2.8197,
      "step": 2200
    },
    {
      "epoch": 0.25691699604743085,
      "grad_norm": 26.928754806518555,
      "learning_rate": 1.8287220026350465e-05,
      "loss": 2.2197,
      "step": 2210
    },
    {
      "epoch": 0.25807951639153687,
      "grad_norm": 28.986183166503906,
      "learning_rate": 1.8279469890723088e-05,
      "loss": 3.2336,
      "step": 2220
    },
    {
      "epoch": 0.2592420367356429,
      "grad_norm": 28.651586532592773,
      "learning_rate": 1.8271719755095715e-05,
      "loss": 3.1347,
      "step": 2230
    },
    {
      "epoch": 0.2604045570797489,
      "grad_norm": 27.26072120666504,
      "learning_rate": 1.8263969619468342e-05,
      "loss": 3.2117,
      "step": 2240
    },
    {
      "epoch": 0.26156707742385493,
      "grad_norm": 27.44863510131836,
      "learning_rate": 1.825621948384097e-05,
      "loss": 2.6662,
      "step": 2250
    },
    {
      "epoch": 0.26272959776796095,
      "grad_norm": 44.973087310791016,
      "learning_rate": 1.8248469348213596e-05,
      "loss": 3.5642,
      "step": 2260
    },
    {
      "epoch": 0.263892118112067,
      "grad_norm": 30.15986442565918,
      "learning_rate": 1.8240719212586223e-05,
      "loss": 2.4498,
      "step": 2270
    },
    {
      "epoch": 0.265054638456173,
      "grad_norm": 35.4075927734375,
      "learning_rate": 1.8232969076958847e-05,
      "loss": 2.5393,
      "step": 2280
    },
    {
      "epoch": 0.266217158800279,
      "grad_norm": 34.150142669677734,
      "learning_rate": 1.8225218941331474e-05,
      "loss": 2.7499,
      "step": 2290
    },
    {
      "epoch": 0.26737967914438504,
      "grad_norm": 43.0833625793457,
      "learning_rate": 1.82174688057041e-05,
      "loss": 3.2719,
      "step": 2300
    },
    {
      "epoch": 0.26854219948849106,
      "grad_norm": 35.238197326660156,
      "learning_rate": 1.8209718670076728e-05,
      "loss": 3.935,
      "step": 2310
    },
    {
      "epoch": 0.2697047198325971,
      "grad_norm": 38.04978561401367,
      "learning_rate": 1.8201968534449355e-05,
      "loss": 3.1697,
      "step": 2320
    },
    {
      "epoch": 0.2708672401767031,
      "grad_norm": 26.544057846069336,
      "learning_rate": 1.8194218398821982e-05,
      "loss": 3.549,
      "step": 2330
    },
    {
      "epoch": 0.2720297605208091,
      "grad_norm": 28.74562644958496,
      "learning_rate": 1.818646826319461e-05,
      "loss": 2.7389,
      "step": 2340
    },
    {
      "epoch": 0.27319228086491515,
      "grad_norm": 36.88220977783203,
      "learning_rate": 1.8178718127567233e-05,
      "loss": 2.9675,
      "step": 2350
    },
    {
      "epoch": 0.27435480120902117,
      "grad_norm": 35.92779541015625,
      "learning_rate": 1.817096799193986e-05,
      "loss": 3.7659,
      "step": 2360
    },
    {
      "epoch": 0.2755173215531272,
      "grad_norm": 33.403385162353516,
      "learning_rate": 1.8163217856312487e-05,
      "loss": 3.6214,
      "step": 2370
    },
    {
      "epoch": 0.2766798418972332,
      "grad_norm": 33.531654357910156,
      "learning_rate": 1.8155467720685114e-05,
      "loss": 3.5931,
      "step": 2380
    },
    {
      "epoch": 0.27784236224133924,
      "grad_norm": 36.581329345703125,
      "learning_rate": 1.814771758505774e-05,
      "loss": 3.6963,
      "step": 2390
    },
    {
      "epoch": 0.27900488258544526,
      "grad_norm": 26.471004486083984,
      "learning_rate": 1.8139967449430368e-05,
      "loss": 2.5771,
      "step": 2400
    },
    {
      "epoch": 0.2801674029295513,
      "grad_norm": 38.28287887573242,
      "learning_rate": 1.8132217313802992e-05,
      "loss": 3.0456,
      "step": 2410
    },
    {
      "epoch": 0.2813299232736573,
      "grad_norm": 31.61033821105957,
      "learning_rate": 1.812446717817562e-05,
      "loss": 3.3581,
      "step": 2420
    },
    {
      "epoch": 0.2824924436177633,
      "grad_norm": 32.18943405151367,
      "learning_rate": 1.8116717042548246e-05,
      "loss": 3.0262,
      "step": 2430
    },
    {
      "epoch": 0.28365496396186934,
      "grad_norm": 33.68183517456055,
      "learning_rate": 1.8108966906920873e-05,
      "loss": 2.5497,
      "step": 2440
    },
    {
      "epoch": 0.28481748430597537,
      "grad_norm": 31.008024215698242,
      "learning_rate": 1.81012167712935e-05,
      "loss": 2.6539,
      "step": 2450
    },
    {
      "epoch": 0.2859800046500814,
      "grad_norm": 30.326509475708008,
      "learning_rate": 1.8093466635666127e-05,
      "loss": 2.4467,
      "step": 2460
    },
    {
      "epoch": 0.2871425249941874,
      "grad_norm": 29.479719161987305,
      "learning_rate": 1.8085716500038754e-05,
      "loss": 2.8626,
      "step": 2470
    },
    {
      "epoch": 0.28830504533829343,
      "grad_norm": 38.687923431396484,
      "learning_rate": 1.8077966364411378e-05,
      "loss": 2.2987,
      "step": 2480
    },
    {
      "epoch": 0.28946756568239945,
      "grad_norm": 27.47718048095703,
      "learning_rate": 1.8070216228784005e-05,
      "loss": 2.1825,
      "step": 2490
    },
    {
      "epoch": 0.2906300860265055,
      "grad_norm": 33.91324234008789,
      "learning_rate": 1.806246609315663e-05,
      "loss": 2.5644,
      "step": 2500
    },
    {
      "epoch": 0.2917926063706115,
      "grad_norm": 35.1933708190918,
      "learning_rate": 1.805471595752926e-05,
      "loss": 2.2685,
      "step": 2510
    },
    {
      "epoch": 0.2929551267147175,
      "grad_norm": 34.39760971069336,
      "learning_rate": 1.8046965821901886e-05,
      "loss": 3.7689,
      "step": 2520
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 35.293243408203125,
      "learning_rate": 1.8039215686274513e-05,
      "loss": 3.6667,
      "step": 2530
    },
    {
      "epoch": 0.29528016740292956,
      "grad_norm": 23.170299530029297,
      "learning_rate": 1.803146555064714e-05,
      "loss": 3.5229,
      "step": 2540
    },
    {
      "epoch": 0.2964426877470356,
      "grad_norm": 26.29698944091797,
      "learning_rate": 1.8023715415019763e-05,
      "loss": 2.8203,
      "step": 2550
    },
    {
      "epoch": 0.2976052080911416,
      "grad_norm": 36.503746032714844,
      "learning_rate": 1.801596527939239e-05,
      "loss": 2.5444,
      "step": 2560
    },
    {
      "epoch": 0.2987677284352476,
      "grad_norm": 33.32461929321289,
      "learning_rate": 1.8008215143765017e-05,
      "loss": 2.6941,
      "step": 2570
    },
    {
      "epoch": 0.29993024877935365,
      "grad_norm": 33.3935661315918,
      "learning_rate": 1.8000465008137645e-05,
      "loss": 2.6581,
      "step": 2580
    },
    {
      "epoch": 0.30109276912345967,
      "grad_norm": 29.161590576171875,
      "learning_rate": 1.799271487251027e-05,
      "loss": 3.2037,
      "step": 2590
    },
    {
      "epoch": 0.3022552894675657,
      "grad_norm": 30.366891860961914,
      "learning_rate": 1.79849647368829e-05,
      "loss": 2.3721,
      "step": 2600
    },
    {
      "epoch": 0.3034178098116717,
      "grad_norm": 30.173799514770508,
      "learning_rate": 1.7977214601255522e-05,
      "loss": 2.7476,
      "step": 2610
    },
    {
      "epoch": 0.30458033015577773,
      "grad_norm": 34.069725036621094,
      "learning_rate": 1.796946446562815e-05,
      "loss": 2.6813,
      "step": 2620
    },
    {
      "epoch": 0.30574285049988376,
      "grad_norm": 39.01866149902344,
      "learning_rate": 1.7961714330000776e-05,
      "loss": 3.5578,
      "step": 2630
    },
    {
      "epoch": 0.3069053708439898,
      "grad_norm": 34.13138961791992,
      "learning_rate": 1.7953964194373403e-05,
      "loss": 3.8437,
      "step": 2640
    },
    {
      "epoch": 0.3080678911880958,
      "grad_norm": 34.1519889831543,
      "learning_rate": 1.794621405874603e-05,
      "loss": 3.8689,
      "step": 2650
    },
    {
      "epoch": 0.3092304115322018,
      "grad_norm": 27.281639099121094,
      "learning_rate": 1.7938463923118657e-05,
      "loss": 2.5328,
      "step": 2660
    },
    {
      "epoch": 0.31039293187630784,
      "grad_norm": 35.362937927246094,
      "learning_rate": 1.7930713787491284e-05,
      "loss": 2.8966,
      "step": 2670
    },
    {
      "epoch": 0.31155545222041386,
      "grad_norm": 39.56540298461914,
      "learning_rate": 1.7922963651863908e-05,
      "loss": 2.593,
      "step": 2680
    },
    {
      "epoch": 0.3127179725645199,
      "grad_norm": 39.40513610839844,
      "learning_rate": 1.7915213516236535e-05,
      "loss": 3.1194,
      "step": 2690
    },
    {
      "epoch": 0.3138804929086259,
      "grad_norm": 31.418018341064453,
      "learning_rate": 1.7907463380609162e-05,
      "loss": 3.3791,
      "step": 2700
    },
    {
      "epoch": 0.31504301325273193,
      "grad_norm": 26.36483383178711,
      "learning_rate": 1.789971324498179e-05,
      "loss": 2.5084,
      "step": 2710
    },
    {
      "epoch": 0.31620553359683795,
      "grad_norm": 44.124595642089844,
      "learning_rate": 1.7891963109354416e-05,
      "loss": 2.1979,
      "step": 2720
    },
    {
      "epoch": 0.317368053940944,
      "grad_norm": 33.69402313232422,
      "learning_rate": 1.7884212973727043e-05,
      "loss": 3.6378,
      "step": 2730
    },
    {
      "epoch": 0.31853057428505,
      "grad_norm": 33.01067352294922,
      "learning_rate": 1.7876462838099667e-05,
      "loss": 3.5669,
      "step": 2740
    },
    {
      "epoch": 0.319693094629156,
      "grad_norm": 26.700349807739258,
      "learning_rate": 1.7868712702472294e-05,
      "loss": 3.7125,
      "step": 2750
    },
    {
      "epoch": 0.32085561497326204,
      "grad_norm": 49.123172760009766,
      "learning_rate": 1.786096256684492e-05,
      "loss": 2.8379,
      "step": 2760
    },
    {
      "epoch": 0.32201813531736806,
      "grad_norm": 30.26146697998047,
      "learning_rate": 1.7853212431217548e-05,
      "loss": 2.6719,
      "step": 2770
    },
    {
      "epoch": 0.3231806556614741,
      "grad_norm": 34.123966217041016,
      "learning_rate": 1.7845462295590175e-05,
      "loss": 2.9305,
      "step": 2780
    },
    {
      "epoch": 0.3243431760055801,
      "grad_norm": 32.121795654296875,
      "learning_rate": 1.7837712159962802e-05,
      "loss": 2.3729,
      "step": 2790
    },
    {
      "epoch": 0.3255056963496861,
      "grad_norm": 32.440250396728516,
      "learning_rate": 1.782996202433543e-05,
      "loss": 3.0639,
      "step": 2800
    },
    {
      "epoch": 0.32666821669379215,
      "grad_norm": 30.37938117980957,
      "learning_rate": 1.7822211888708053e-05,
      "loss": 3.5261,
      "step": 2810
    },
    {
      "epoch": 0.32783073703789817,
      "grad_norm": 52.77425003051758,
      "learning_rate": 1.781446175308068e-05,
      "loss": 3.5401,
      "step": 2820
    },
    {
      "epoch": 0.3289932573820042,
      "grad_norm": 33.815032958984375,
      "learning_rate": 1.7806711617453307e-05,
      "loss": 2.2239,
      "step": 2830
    },
    {
      "epoch": 0.3301557777261102,
      "grad_norm": 39.946739196777344,
      "learning_rate": 1.7798961481825934e-05,
      "loss": 4.0199,
      "step": 2840
    },
    {
      "epoch": 0.33131829807021623,
      "grad_norm": 33.593231201171875,
      "learning_rate": 1.779121134619856e-05,
      "loss": 2.9862,
      "step": 2850
    },
    {
      "epoch": 0.33248081841432225,
      "grad_norm": 32.23411560058594,
      "learning_rate": 1.7783461210571188e-05,
      "loss": 2.69,
      "step": 2860
    },
    {
      "epoch": 0.3336433387584283,
      "grad_norm": 29.596906661987305,
      "learning_rate": 1.7775711074943815e-05,
      "loss": 3.0666,
      "step": 2870
    },
    {
      "epoch": 0.3348058591025343,
      "grad_norm": 26.14747428894043,
      "learning_rate": 1.776796093931644e-05,
      "loss": 3.0422,
      "step": 2880
    },
    {
      "epoch": 0.3359683794466403,
      "grad_norm": 32.03240203857422,
      "learning_rate": 1.7760210803689066e-05,
      "loss": 2.5135,
      "step": 2890
    },
    {
      "epoch": 0.33713089979074634,
      "grad_norm": 28.45098114013672,
      "learning_rate": 1.7752460668061693e-05,
      "loss": 4.1054,
      "step": 2900
    },
    {
      "epoch": 0.33829342013485236,
      "grad_norm": 35.16387939453125,
      "learning_rate": 1.774471053243432e-05,
      "loss": 2.6303,
      "step": 2910
    },
    {
      "epoch": 0.3394559404789584,
      "grad_norm": 32.729759216308594,
      "learning_rate": 1.7736960396806947e-05,
      "loss": 3.4089,
      "step": 2920
    },
    {
      "epoch": 0.3406184608230644,
      "grad_norm": 38.1572380065918,
      "learning_rate": 1.7729210261179574e-05,
      "loss": 2.0727,
      "step": 2930
    },
    {
      "epoch": 0.34178098116717043,
      "grad_norm": 44.176265716552734,
      "learning_rate": 1.7721460125552197e-05,
      "loss": 2.8442,
      "step": 2940
    },
    {
      "epoch": 0.34294350151127645,
      "grad_norm": 34.96951675415039,
      "learning_rate": 1.7713709989924824e-05,
      "loss": 3.828,
      "step": 2950
    },
    {
      "epoch": 0.34410602185538247,
      "grad_norm": 29.617961883544922,
      "learning_rate": 1.770595985429745e-05,
      "loss": 3.0582,
      "step": 2960
    },
    {
      "epoch": 0.3452685421994885,
      "grad_norm": 35.48814392089844,
      "learning_rate": 1.769820971867008e-05,
      "loss": 3.6585,
      "step": 2970
    },
    {
      "epoch": 0.3464310625435945,
      "grad_norm": 33.108551025390625,
      "learning_rate": 1.7690459583042706e-05,
      "loss": 3.1146,
      "step": 2980
    },
    {
      "epoch": 0.34759358288770054,
      "grad_norm": 22.01619529724121,
      "learning_rate": 1.7682709447415333e-05,
      "loss": 2.7687,
      "step": 2990
    },
    {
      "epoch": 0.34875610323180656,
      "grad_norm": 26.67781639099121,
      "learning_rate": 1.767495931178796e-05,
      "loss": 2.5388,
      "step": 3000
    },
    {
      "epoch": 0.3499186235759126,
      "grad_norm": 34.933780670166016,
      "learning_rate": 1.7667209176160583e-05,
      "loss": 2.6377,
      "step": 3010
    },
    {
      "epoch": 0.3510811439200186,
      "grad_norm": 42.40118408203125,
      "learning_rate": 1.765945904053321e-05,
      "loss": 3.5611,
      "step": 3020
    },
    {
      "epoch": 0.3522436642641246,
      "grad_norm": 30.04883575439453,
      "learning_rate": 1.7651708904905837e-05,
      "loss": 3.0665,
      "step": 3030
    },
    {
      "epoch": 0.35340618460823064,
      "grad_norm": 29.225114822387695,
      "learning_rate": 1.7643958769278464e-05,
      "loss": 3.1388,
      "step": 3040
    },
    {
      "epoch": 0.35456870495233667,
      "grad_norm": 29.411540985107422,
      "learning_rate": 1.763620863365109e-05,
      "loss": 2.4879,
      "step": 3050
    },
    {
      "epoch": 0.3557312252964427,
      "grad_norm": 24.047931671142578,
      "learning_rate": 1.762845849802372e-05,
      "loss": 2.7964,
      "step": 3060
    },
    {
      "epoch": 0.3568937456405487,
      "grad_norm": 32.21699905395508,
      "learning_rate": 1.7620708362396342e-05,
      "loss": 3.1803,
      "step": 3070
    },
    {
      "epoch": 0.35805626598465473,
      "grad_norm": 29.74698829650879,
      "learning_rate": 1.761295822676897e-05,
      "loss": 2.4154,
      "step": 3080
    },
    {
      "epoch": 0.35921878632876075,
      "grad_norm": 26.766321182250977,
      "learning_rate": 1.7605208091141596e-05,
      "loss": 2.5456,
      "step": 3090
    },
    {
      "epoch": 0.3603813066728668,
      "grad_norm": 34.117645263671875,
      "learning_rate": 1.7597457955514223e-05,
      "loss": 2.9225,
      "step": 3100
    },
    {
      "epoch": 0.3615438270169728,
      "grad_norm": 28.508817672729492,
      "learning_rate": 1.758970781988685e-05,
      "loss": 2.5523,
      "step": 3110
    },
    {
      "epoch": 0.3627063473610788,
      "grad_norm": 36.326751708984375,
      "learning_rate": 1.7581957684259477e-05,
      "loss": 2.8077,
      "step": 3120
    },
    {
      "epoch": 0.36386886770518484,
      "grad_norm": 32.84896469116211,
      "learning_rate": 1.7574207548632104e-05,
      "loss": 3.1071,
      "step": 3130
    },
    {
      "epoch": 0.36503138804929086,
      "grad_norm": 28.042753219604492,
      "learning_rate": 1.7566457413004728e-05,
      "loss": 3.6419,
      "step": 3140
    },
    {
      "epoch": 0.3661939083933969,
      "grad_norm": 34.17769241333008,
      "learning_rate": 1.7558707277377355e-05,
      "loss": 3.1501,
      "step": 3150
    },
    {
      "epoch": 0.3673564287375029,
      "grad_norm": 30.113323211669922,
      "learning_rate": 1.7550957141749982e-05,
      "loss": 2.4689,
      "step": 3160
    },
    {
      "epoch": 0.3685189490816089,
      "grad_norm": 33.20883560180664,
      "learning_rate": 1.754320700612261e-05,
      "loss": 2.8069,
      "step": 3170
    },
    {
      "epoch": 0.36968146942571495,
      "grad_norm": 41.570980072021484,
      "learning_rate": 1.7535456870495236e-05,
      "loss": 3.7007,
      "step": 3180
    },
    {
      "epoch": 0.37084398976982097,
      "grad_norm": 31.720375061035156,
      "learning_rate": 1.7527706734867863e-05,
      "loss": 3.0231,
      "step": 3190
    },
    {
      "epoch": 0.372006510113927,
      "grad_norm": 28.51862144470215,
      "learning_rate": 1.751995659924049e-05,
      "loss": 2.4306,
      "step": 3200
    },
    {
      "epoch": 0.373169030458033,
      "grad_norm": 27.17192268371582,
      "learning_rate": 1.7512206463613114e-05,
      "loss": 3.7692,
      "step": 3210
    },
    {
      "epoch": 0.37433155080213903,
      "grad_norm": 31.328109741210938,
      "learning_rate": 1.750445632798574e-05,
      "loss": 3.2252,
      "step": 3220
    },
    {
      "epoch": 0.37549407114624506,
      "grad_norm": 31.16164207458496,
      "learning_rate": 1.7496706192358368e-05,
      "loss": 2.7186,
      "step": 3230
    },
    {
      "epoch": 0.3766565914903511,
      "grad_norm": 24.848901748657227,
      "learning_rate": 1.7488956056730995e-05,
      "loss": 3.1193,
      "step": 3240
    },
    {
      "epoch": 0.3778191118344571,
      "grad_norm": 35.95059585571289,
      "learning_rate": 1.7481205921103622e-05,
      "loss": 3.0823,
      "step": 3250
    },
    {
      "epoch": 0.3789816321785631,
      "grad_norm": 22.419593811035156,
      "learning_rate": 1.747345578547625e-05,
      "loss": 3.1765,
      "step": 3260
    },
    {
      "epoch": 0.38014415252266914,
      "grad_norm": 28.80109977722168,
      "learning_rate": 1.7465705649848873e-05,
      "loss": 2.8204,
      "step": 3270
    },
    {
      "epoch": 0.38130667286677516,
      "grad_norm": 39.87251663208008,
      "learning_rate": 1.74579555142215e-05,
      "loss": 2.935,
      "step": 3280
    },
    {
      "epoch": 0.3824691932108812,
      "grad_norm": 35.169166564941406,
      "learning_rate": 1.7450205378594127e-05,
      "loss": 2.7914,
      "step": 3290
    },
    {
      "epoch": 0.3836317135549872,
      "grad_norm": 33.56083679199219,
      "learning_rate": 1.7442455242966754e-05,
      "loss": 2.939,
      "step": 3300
    },
    {
      "epoch": 0.38479423389909323,
      "grad_norm": 30.32440185546875,
      "learning_rate": 1.743470510733938e-05,
      "loss": 3.8812,
      "step": 3310
    },
    {
      "epoch": 0.38595675424319925,
      "grad_norm": 38.21529006958008,
      "learning_rate": 1.7426954971712008e-05,
      "loss": 3.0577,
      "step": 3320
    },
    {
      "epoch": 0.3871192745873053,
      "grad_norm": 34.88441467285156,
      "learning_rate": 1.7419204836084635e-05,
      "loss": 3.2375,
      "step": 3330
    },
    {
      "epoch": 0.3882817949314113,
      "grad_norm": 26.28209686279297,
      "learning_rate": 1.741145470045726e-05,
      "loss": 3.0502,
      "step": 3340
    },
    {
      "epoch": 0.3894443152755173,
      "grad_norm": 26.402429580688477,
      "learning_rate": 1.7403704564829886e-05,
      "loss": 3.4106,
      "step": 3350
    },
    {
      "epoch": 0.39060683561962334,
      "grad_norm": 25.78201675415039,
      "learning_rate": 1.7395954429202513e-05,
      "loss": 2.8941,
      "step": 3360
    },
    {
      "epoch": 0.39176935596372936,
      "grad_norm": 30.13326644897461,
      "learning_rate": 1.738820429357514e-05,
      "loss": 2.8342,
      "step": 3370
    },
    {
      "epoch": 0.3929318763078354,
      "grad_norm": 36.254234313964844,
      "learning_rate": 1.7380454157947767e-05,
      "loss": 3.5762,
      "step": 3380
    },
    {
      "epoch": 0.3940943966519414,
      "grad_norm": 28.198551177978516,
      "learning_rate": 1.7372704022320394e-05,
      "loss": 2.7885,
      "step": 3390
    },
    {
      "epoch": 0.3952569169960474,
      "grad_norm": 37.36881637573242,
      "learning_rate": 1.7364953886693017e-05,
      "loss": 2.9939,
      "step": 3400
    },
    {
      "epoch": 0.39641943734015345,
      "grad_norm": 26.10585594177246,
      "learning_rate": 1.7357203751065644e-05,
      "loss": 2.5095,
      "step": 3410
    },
    {
      "epoch": 0.39758195768425947,
      "grad_norm": 32.05379104614258,
      "learning_rate": 1.734945361543827e-05,
      "loss": 2.8465,
      "step": 3420
    },
    {
      "epoch": 0.3987444780283655,
      "grad_norm": 35.225460052490234,
      "learning_rate": 1.73417034798109e-05,
      "loss": 2.7035,
      "step": 3430
    },
    {
      "epoch": 0.3999069983724715,
      "grad_norm": 26.529403686523438,
      "learning_rate": 1.7333953344183525e-05,
      "loss": 2.7789,
      "step": 3440
    },
    {
      "epoch": 0.40106951871657753,
      "grad_norm": 54.42803955078125,
      "learning_rate": 1.7326203208556153e-05,
      "loss": 2.999,
      "step": 3450
    },
    {
      "epoch": 0.40223203906068355,
      "grad_norm": 26.546222686767578,
      "learning_rate": 1.731845307292878e-05,
      "loss": 2.3256,
      "step": 3460
    },
    {
      "epoch": 0.4033945594047896,
      "grad_norm": 33.769874572753906,
      "learning_rate": 1.7310702937301403e-05,
      "loss": 3.462,
      "step": 3470
    },
    {
      "epoch": 0.4045570797488956,
      "grad_norm": 34.10395431518555,
      "learning_rate": 1.730295280167403e-05,
      "loss": 2.4789,
      "step": 3480
    },
    {
      "epoch": 0.4057196000930016,
      "grad_norm": 32.874332427978516,
      "learning_rate": 1.7295202666046657e-05,
      "loss": 2.7791,
      "step": 3490
    },
    {
      "epoch": 0.40688212043710764,
      "grad_norm": 30.415624618530273,
      "learning_rate": 1.7287452530419284e-05,
      "loss": 3.8475,
      "step": 3500
    },
    {
      "epoch": 0.40804464078121366,
      "grad_norm": 27.491846084594727,
      "learning_rate": 1.727970239479191e-05,
      "loss": 3.943,
      "step": 3510
    },
    {
      "epoch": 0.4092071611253197,
      "grad_norm": 33.75550842285156,
      "learning_rate": 1.727195225916454e-05,
      "loss": 2.3559,
      "step": 3520
    },
    {
      "epoch": 0.4103696814694257,
      "grad_norm": 32.52035903930664,
      "learning_rate": 1.7264202123537165e-05,
      "loss": 2.365,
      "step": 3530
    },
    {
      "epoch": 0.41153220181353173,
      "grad_norm": 38.45967102050781,
      "learning_rate": 1.725645198790979e-05,
      "loss": 3.2142,
      "step": 3540
    },
    {
      "epoch": 0.41269472215763775,
      "grad_norm": 34.36201477050781,
      "learning_rate": 1.7248701852282416e-05,
      "loss": 3.2744,
      "step": 3550
    },
    {
      "epoch": 0.41385724250174377,
      "grad_norm": 32.507144927978516,
      "learning_rate": 1.7240951716655043e-05,
      "loss": 3.4824,
      "step": 3560
    },
    {
      "epoch": 0.4150197628458498,
      "grad_norm": 32.670475006103516,
      "learning_rate": 1.723320158102767e-05,
      "loss": 3.1869,
      "step": 3570
    },
    {
      "epoch": 0.4161822831899558,
      "grad_norm": 27.46954917907715,
      "learning_rate": 1.7225451445400297e-05,
      "loss": 2.5541,
      "step": 3580
    },
    {
      "epoch": 0.41734480353406184,
      "grad_norm": 34.184539794921875,
      "learning_rate": 1.7217701309772924e-05,
      "loss": 3.8998,
      "step": 3590
    },
    {
      "epoch": 0.41850732387816786,
      "grad_norm": 36.68737030029297,
      "learning_rate": 1.7209951174145548e-05,
      "loss": 2.7144,
      "step": 3600
    },
    {
      "epoch": 0.4196698442222739,
      "grad_norm": 29.018346786499023,
      "learning_rate": 1.7202201038518175e-05,
      "loss": 3.2808,
      "step": 3610
    },
    {
      "epoch": 0.4208323645663799,
      "grad_norm": 24.155921936035156,
      "learning_rate": 1.7194450902890802e-05,
      "loss": 2.4998,
      "step": 3620
    },
    {
      "epoch": 0.4219948849104859,
      "grad_norm": 25.693933486938477,
      "learning_rate": 1.718670076726343e-05,
      "loss": 3.1443,
      "step": 3630
    },
    {
      "epoch": 0.42315740525459195,
      "grad_norm": 32.6126594543457,
      "learning_rate": 1.7178950631636056e-05,
      "loss": 3.0206,
      "step": 3640
    },
    {
      "epoch": 0.42431992559869797,
      "grad_norm": 24.95969009399414,
      "learning_rate": 1.7171200496008683e-05,
      "loss": 2.9825,
      "step": 3650
    },
    {
      "epoch": 0.425482445942804,
      "grad_norm": 26.28480339050293,
      "learning_rate": 1.716345036038131e-05,
      "loss": 2.6913,
      "step": 3660
    },
    {
      "epoch": 0.42664496628691,
      "grad_norm": 42.1234130859375,
      "learning_rate": 1.7155700224753934e-05,
      "loss": 3.0822,
      "step": 3670
    },
    {
      "epoch": 0.42780748663101603,
      "grad_norm": 31.75450897216797,
      "learning_rate": 1.714795008912656e-05,
      "loss": 3.126,
      "step": 3680
    },
    {
      "epoch": 0.42897000697512205,
      "grad_norm": 23.40302848815918,
      "learning_rate": 1.7140199953499188e-05,
      "loss": 2.8112,
      "step": 3690
    },
    {
      "epoch": 0.4301325273192281,
      "grad_norm": 25.230920791625977,
      "learning_rate": 1.7132449817871815e-05,
      "loss": 3.212,
      "step": 3700
    },
    {
      "epoch": 0.4312950476633341,
      "grad_norm": 37.14276123046875,
      "learning_rate": 1.7124699682244442e-05,
      "loss": 3.2548,
      "step": 3710
    },
    {
      "epoch": 0.4324575680074401,
      "grad_norm": 38.35721969604492,
      "learning_rate": 1.711694954661707e-05,
      "loss": 3.3311,
      "step": 3720
    },
    {
      "epoch": 0.43362008835154614,
      "grad_norm": 40.61144256591797,
      "learning_rate": 1.7109199410989693e-05,
      "loss": 2.3558,
      "step": 3730
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 34.51778793334961,
      "learning_rate": 1.710144927536232e-05,
      "loss": 2.909,
      "step": 3740
    },
    {
      "epoch": 0.4359451290397582,
      "grad_norm": 32.99738693237305,
      "learning_rate": 1.7093699139734947e-05,
      "loss": 2.3035,
      "step": 3750
    },
    {
      "epoch": 0.4371076493838642,
      "grad_norm": 21.040742874145508,
      "learning_rate": 1.7085949004107574e-05,
      "loss": 2.8968,
      "step": 3760
    },
    {
      "epoch": 0.4382701697279702,
      "grad_norm": 65.2520980834961,
      "learning_rate": 1.70781988684802e-05,
      "loss": 3.9623,
      "step": 3770
    },
    {
      "epoch": 0.43943269007207625,
      "grad_norm": 40.37104034423828,
      "learning_rate": 1.7070448732852828e-05,
      "loss": 3.5643,
      "step": 3780
    },
    {
      "epoch": 0.44059521041618227,
      "grad_norm": 32.64963150024414,
      "learning_rate": 1.7062698597225455e-05,
      "loss": 2.822,
      "step": 3790
    },
    {
      "epoch": 0.4417577307602883,
      "grad_norm": 46.63291549682617,
      "learning_rate": 1.705494846159808e-05,
      "loss": 3.4154,
      "step": 3800
    },
    {
      "epoch": 0.4429202511043943,
      "grad_norm": 26.038610458374023,
      "learning_rate": 1.7047198325970705e-05,
      "loss": 2.4682,
      "step": 3810
    },
    {
      "epoch": 0.44408277144850034,
      "grad_norm": 25.072181701660156,
      "learning_rate": 1.7039448190343332e-05,
      "loss": 2.3582,
      "step": 3820
    },
    {
      "epoch": 0.44524529179260636,
      "grad_norm": 21.025028228759766,
      "learning_rate": 1.703169805471596e-05,
      "loss": 3.1259,
      "step": 3830
    },
    {
      "epoch": 0.4464078121367124,
      "grad_norm": 46.1961555480957,
      "learning_rate": 1.7023947919088587e-05,
      "loss": 3.5591,
      "step": 3840
    },
    {
      "epoch": 0.4475703324808184,
      "grad_norm": 32.75589370727539,
      "learning_rate": 1.7016197783461214e-05,
      "loss": 3.1156,
      "step": 3850
    },
    {
      "epoch": 0.4487328528249244,
      "grad_norm": 40.433101654052734,
      "learning_rate": 1.700844764783384e-05,
      "loss": 2.3421,
      "step": 3860
    },
    {
      "epoch": 0.44989537316903044,
      "grad_norm": 28.308908462524414,
      "learning_rate": 1.7000697512206464e-05,
      "loss": 5.1345,
      "step": 3870
    },
    {
      "epoch": 0.45105789351313647,
      "grad_norm": 26.00977325439453,
      "learning_rate": 1.699294737657909e-05,
      "loss": 2.9465,
      "step": 3880
    },
    {
      "epoch": 0.4522204138572425,
      "grad_norm": 25.395002365112305,
      "learning_rate": 1.698519724095172e-05,
      "loss": 2.9745,
      "step": 3890
    },
    {
      "epoch": 0.4533829342013485,
      "grad_norm": 30.152877807617188,
      "learning_rate": 1.6977447105324345e-05,
      "loss": 2.482,
      "step": 3900
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 30.613767623901367,
      "learning_rate": 1.6969696969696972e-05,
      "loss": 2.4058,
      "step": 3910
    },
    {
      "epoch": 0.45570797488956055,
      "grad_norm": 39.15092849731445,
      "learning_rate": 1.69619468340696e-05,
      "loss": 3.1557,
      "step": 3920
    },
    {
      "epoch": 0.4568704952336666,
      "grad_norm": 30.665918350219727,
      "learning_rate": 1.6954196698442223e-05,
      "loss": 2.854,
      "step": 3930
    },
    {
      "epoch": 0.4580330155777726,
      "grad_norm": 26.02781105041504,
      "learning_rate": 1.694644656281485e-05,
      "loss": 2.4121,
      "step": 3940
    },
    {
      "epoch": 0.4591955359218786,
      "grad_norm": 35.97167205810547,
      "learning_rate": 1.6938696427187477e-05,
      "loss": 3.3368,
      "step": 3950
    },
    {
      "epoch": 0.46035805626598464,
      "grad_norm": 23.416988372802734,
      "learning_rate": 1.6930946291560104e-05,
      "loss": 2.6639,
      "step": 3960
    },
    {
      "epoch": 0.46152057661009066,
      "grad_norm": 36.65801239013672,
      "learning_rate": 1.692319615593273e-05,
      "loss": 2.6545,
      "step": 3970
    },
    {
      "epoch": 0.4626830969541967,
      "grad_norm": 41.75541305541992,
      "learning_rate": 1.6915446020305358e-05,
      "loss": 2.9541,
      "step": 3980
    },
    {
      "epoch": 0.4638456172983027,
      "grad_norm": 34.54867935180664,
      "learning_rate": 1.6907695884677985e-05,
      "loss": 2.9364,
      "step": 3990
    },
    {
      "epoch": 0.4650081376424087,
      "grad_norm": 33.883872985839844,
      "learning_rate": 1.689994574905061e-05,
      "loss": 3.5074,
      "step": 4000
    },
    {
      "epoch": 0.46617065798651475,
      "grad_norm": 42.07759475708008,
      "learning_rate": 1.6892195613423236e-05,
      "loss": 2.4917,
      "step": 4010
    },
    {
      "epoch": 0.46733317833062077,
      "grad_norm": 31.286802291870117,
      "learning_rate": 1.6884445477795863e-05,
      "loss": 3.6776,
      "step": 4020
    },
    {
      "epoch": 0.4684956986747268,
      "grad_norm": 43.351837158203125,
      "learning_rate": 1.687669534216849e-05,
      "loss": 2.7156,
      "step": 4030
    },
    {
      "epoch": 0.4696582190188328,
      "grad_norm": 40.00191879272461,
      "learning_rate": 1.6868945206541117e-05,
      "loss": 2.6677,
      "step": 4040
    },
    {
      "epoch": 0.47082073936293883,
      "grad_norm": 32.31223678588867,
      "learning_rate": 1.6861195070913744e-05,
      "loss": 2.9424,
      "step": 4050
    },
    {
      "epoch": 0.47198325970704486,
      "grad_norm": 30.762962341308594,
      "learning_rate": 1.6853444935286368e-05,
      "loss": 2.9698,
      "step": 4060
    },
    {
      "epoch": 0.4731457800511509,
      "grad_norm": 37.39851379394531,
      "learning_rate": 1.6845694799658995e-05,
      "loss": 3.0185,
      "step": 4070
    },
    {
      "epoch": 0.4743083003952569,
      "grad_norm": 32.78266143798828,
      "learning_rate": 1.6837944664031622e-05,
      "loss": 3.2607,
      "step": 4080
    },
    {
      "epoch": 0.4754708207393629,
      "grad_norm": 29.209524154663086,
      "learning_rate": 1.683019452840425e-05,
      "loss": 4.1629,
      "step": 4090
    },
    {
      "epoch": 0.47663334108346894,
      "grad_norm": 40.26601791381836,
      "learning_rate": 1.6822444392776876e-05,
      "loss": 3.8314,
      "step": 4100
    },
    {
      "epoch": 0.47779586142757496,
      "grad_norm": 31.603925704956055,
      "learning_rate": 1.6814694257149503e-05,
      "loss": 3.247,
      "step": 4110
    },
    {
      "epoch": 0.478958381771681,
      "grad_norm": 27.083446502685547,
      "learning_rate": 1.680694412152213e-05,
      "loss": 2.3701,
      "step": 4120
    },
    {
      "epoch": 0.480120902115787,
      "grad_norm": 26.098831176757812,
      "learning_rate": 1.6799193985894754e-05,
      "loss": 3.5292,
      "step": 4130
    },
    {
      "epoch": 0.48128342245989303,
      "grad_norm": 36.12290954589844,
      "learning_rate": 1.679144385026738e-05,
      "loss": 2.8018,
      "step": 4140
    },
    {
      "epoch": 0.48244594280399905,
      "grad_norm": 25.943622589111328,
      "learning_rate": 1.6783693714640008e-05,
      "loss": 2.7288,
      "step": 4150
    },
    {
      "epoch": 0.48360846314810507,
      "grad_norm": 26.648263931274414,
      "learning_rate": 1.6775943579012635e-05,
      "loss": 2.3811,
      "step": 4160
    },
    {
      "epoch": 0.4847709834922111,
      "grad_norm": 23.975147247314453,
      "learning_rate": 1.6768193443385262e-05,
      "loss": 2.4952,
      "step": 4170
    },
    {
      "epoch": 0.4859335038363171,
      "grad_norm": 29.097415924072266,
      "learning_rate": 1.676044330775789e-05,
      "loss": 3.1755,
      "step": 4180
    },
    {
      "epoch": 0.48709602418042314,
      "grad_norm": 24.825998306274414,
      "learning_rate": 1.6752693172130516e-05,
      "loss": 2.695,
      "step": 4190
    },
    {
      "epoch": 0.48825854452452916,
      "grad_norm": 26.59138298034668,
      "learning_rate": 1.674494303650314e-05,
      "loss": 3.5637,
      "step": 4200
    },
    {
      "epoch": 0.4894210648686352,
      "grad_norm": 53.91144561767578,
      "learning_rate": 1.6737192900875767e-05,
      "loss": 3.3087,
      "step": 4210
    },
    {
      "epoch": 0.4905835852127412,
      "grad_norm": 43.776756286621094,
      "learning_rate": 1.6729442765248394e-05,
      "loss": 3.2652,
      "step": 4220
    },
    {
      "epoch": 0.4917461055568472,
      "grad_norm": 40.4121208190918,
      "learning_rate": 1.672169262962102e-05,
      "loss": 3.1741,
      "step": 4230
    },
    {
      "epoch": 0.49290862590095325,
      "grad_norm": 36.739585876464844,
      "learning_rate": 1.6713942493993648e-05,
      "loss": 3.2415,
      "step": 4240
    },
    {
      "epoch": 0.49407114624505927,
      "grad_norm": 40.70240020751953,
      "learning_rate": 1.6706192358366275e-05,
      "loss": 3.1088,
      "step": 4250
    },
    {
      "epoch": 0.4952336665891653,
      "grad_norm": 43.02499771118164,
      "learning_rate": 1.66984422227389e-05,
      "loss": 4.0566,
      "step": 4260
    },
    {
      "epoch": 0.4963961869332713,
      "grad_norm": 28.88859748840332,
      "learning_rate": 1.6690692087111525e-05,
      "loss": 3.1562,
      "step": 4270
    },
    {
      "epoch": 0.49755870727737733,
      "grad_norm": 32.229248046875,
      "learning_rate": 1.6682941951484152e-05,
      "loss": 3.3482,
      "step": 4280
    },
    {
      "epoch": 0.49872122762148335,
      "grad_norm": 27.964805603027344,
      "learning_rate": 1.667519181585678e-05,
      "loss": 2.9598,
      "step": 4290
    },
    {
      "epoch": 0.4998837479655894,
      "grad_norm": 25.566553115844727,
      "learning_rate": 1.6667441680229406e-05,
      "loss": 2.7068,
      "step": 4300
    },
    {
      "epoch": 0.5010462683096955,
      "grad_norm": 23.46303367614746,
      "learning_rate": 1.6659691544602033e-05,
      "loss": 3.5165,
      "step": 4310
    },
    {
      "epoch": 0.5022087886538015,
      "grad_norm": 29.687759399414062,
      "learning_rate": 1.665194140897466e-05,
      "loss": 2.7153,
      "step": 4320
    },
    {
      "epoch": 0.5033713089979075,
      "grad_norm": 25.0517635345459,
      "learning_rate": 1.6644191273347284e-05,
      "loss": 3.0066,
      "step": 4330
    },
    {
      "epoch": 0.5045338293420135,
      "grad_norm": 27.743654251098633,
      "learning_rate": 1.663644113771991e-05,
      "loss": 2.432,
      "step": 4340
    },
    {
      "epoch": 0.5056963496861195,
      "grad_norm": 24.418563842773438,
      "learning_rate": 1.6628691002092538e-05,
      "loss": 2.1788,
      "step": 4350
    },
    {
      "epoch": 0.5068588700302256,
      "grad_norm": 27.178388595581055,
      "learning_rate": 1.6620940866465165e-05,
      "loss": 2.9074,
      "step": 4360
    },
    {
      "epoch": 0.5080213903743316,
      "grad_norm": 31.857830047607422,
      "learning_rate": 1.6613190730837792e-05,
      "loss": 3.0728,
      "step": 4370
    },
    {
      "epoch": 0.5091839107184376,
      "grad_norm": 22.9548282623291,
      "learning_rate": 1.660544059521042e-05,
      "loss": 2.4613,
      "step": 4380
    },
    {
      "epoch": 0.5103464310625436,
      "grad_norm": 38.764320373535156,
      "learning_rate": 1.6597690459583043e-05,
      "loss": 3.0394,
      "step": 4390
    },
    {
      "epoch": 0.5115089514066496,
      "grad_norm": 24.600357055664062,
      "learning_rate": 1.658994032395567e-05,
      "loss": 3.2173,
      "step": 4400
    },
    {
      "epoch": 0.5126714717507557,
      "grad_norm": 35.19279479980469,
      "learning_rate": 1.6582190188328297e-05,
      "loss": 2.5042,
      "step": 4410
    },
    {
      "epoch": 0.5138339920948617,
      "grad_norm": 29.855506896972656,
      "learning_rate": 1.6574440052700924e-05,
      "loss": 3.2886,
      "step": 4420
    },
    {
      "epoch": 0.5149965124389677,
      "grad_norm": 33.72856521606445,
      "learning_rate": 1.656668991707355e-05,
      "loss": 2.8828,
      "step": 4430
    },
    {
      "epoch": 0.5161590327830737,
      "grad_norm": 26.066492080688477,
      "learning_rate": 1.6558939781446178e-05,
      "loss": 4.6178,
      "step": 4440
    },
    {
      "epoch": 0.5173215531271798,
      "grad_norm": 21.945152282714844,
      "learning_rate": 1.6551189645818805e-05,
      "loss": 3.0171,
      "step": 4450
    },
    {
      "epoch": 0.5184840734712858,
      "grad_norm": 28.852584838867188,
      "learning_rate": 1.654343951019143e-05,
      "loss": 3.4683,
      "step": 4460
    },
    {
      "epoch": 0.5196465938153918,
      "grad_norm": 36.4653434753418,
      "learning_rate": 1.6535689374564056e-05,
      "loss": 3.6337,
      "step": 4470
    },
    {
      "epoch": 0.5208091141594978,
      "grad_norm": 28.8327693939209,
      "learning_rate": 1.6527939238936683e-05,
      "loss": 2.7521,
      "step": 4480
    },
    {
      "epoch": 0.5219716345036038,
      "grad_norm": 30.723121643066406,
      "learning_rate": 1.652018910330931e-05,
      "loss": 4.0234,
      "step": 4490
    },
    {
      "epoch": 0.5231341548477099,
      "grad_norm": 36.301429748535156,
      "learning_rate": 1.6512438967681937e-05,
      "loss": 2.9685,
      "step": 4500
    },
    {
      "epoch": 0.5242966751918159,
      "grad_norm": 25.137508392333984,
      "learning_rate": 1.6504688832054564e-05,
      "loss": 3.4614,
      "step": 4510
    },
    {
      "epoch": 0.5254591955359219,
      "grad_norm": 25.25156593322754,
      "learning_rate": 1.649693869642719e-05,
      "loss": 2.64,
      "step": 4520
    },
    {
      "epoch": 0.5266217158800279,
      "grad_norm": 28.434833526611328,
      "learning_rate": 1.6489188560799815e-05,
      "loss": 2.8211,
      "step": 4530
    },
    {
      "epoch": 0.527784236224134,
      "grad_norm": 37.82254409790039,
      "learning_rate": 1.6481438425172442e-05,
      "loss": 2.2888,
      "step": 4540
    },
    {
      "epoch": 0.52894675656824,
      "grad_norm": 30.014169692993164,
      "learning_rate": 1.647368828954507e-05,
      "loss": 3.2806,
      "step": 4550
    },
    {
      "epoch": 0.530109276912346,
      "grad_norm": 27.0945987701416,
      "learning_rate": 1.6465938153917696e-05,
      "loss": 2.6733,
      "step": 4560
    },
    {
      "epoch": 0.531271797256452,
      "grad_norm": 25.622474670410156,
      "learning_rate": 1.6458188018290323e-05,
      "loss": 4.2946,
      "step": 4570
    },
    {
      "epoch": 0.532434317600558,
      "grad_norm": 29.487085342407227,
      "learning_rate": 1.645043788266295e-05,
      "loss": 2.103,
      "step": 4580
    },
    {
      "epoch": 0.5335968379446641,
      "grad_norm": 26.55564308166504,
      "learning_rate": 1.6442687747035574e-05,
      "loss": 2.2998,
      "step": 4590
    },
    {
      "epoch": 0.5347593582887701,
      "grad_norm": 45.82554626464844,
      "learning_rate": 1.64349376114082e-05,
      "loss": 3.7851,
      "step": 4600
    },
    {
      "epoch": 0.5359218786328761,
      "grad_norm": 34.46561050415039,
      "learning_rate": 1.6427187475780828e-05,
      "loss": 3.2413,
      "step": 4610
    },
    {
      "epoch": 0.5370843989769821,
      "grad_norm": 27.297657012939453,
      "learning_rate": 1.6419437340153455e-05,
      "loss": 3.0393,
      "step": 4620
    },
    {
      "epoch": 0.5382469193210881,
      "grad_norm": 32.20542907714844,
      "learning_rate": 1.641168720452608e-05,
      "loss": 3.1568,
      "step": 4630
    },
    {
      "epoch": 0.5394094396651942,
      "grad_norm": 32.57666778564453,
      "learning_rate": 1.640393706889871e-05,
      "loss": 2.8337,
      "step": 4640
    },
    {
      "epoch": 0.5405719600093002,
      "grad_norm": 25.22452735900879,
      "learning_rate": 1.6396186933271336e-05,
      "loss": 2.9427,
      "step": 4650
    },
    {
      "epoch": 0.5417344803534062,
      "grad_norm": 35.31978988647461,
      "learning_rate": 1.638843679764396e-05,
      "loss": 3.0443,
      "step": 4660
    },
    {
      "epoch": 0.5428970006975122,
      "grad_norm": 33.556495666503906,
      "learning_rate": 1.6380686662016586e-05,
      "loss": 2.4234,
      "step": 4670
    },
    {
      "epoch": 0.5440595210416183,
      "grad_norm": 46.47304153442383,
      "learning_rate": 1.6372936526389213e-05,
      "loss": 3.26,
      "step": 4680
    },
    {
      "epoch": 0.5452220413857243,
      "grad_norm": 39.83522033691406,
      "learning_rate": 1.636518639076184e-05,
      "loss": 3.528,
      "step": 4690
    },
    {
      "epoch": 0.5463845617298303,
      "grad_norm": 26.16843032836914,
      "learning_rate": 1.6357436255134468e-05,
      "loss": 2.5111,
      "step": 4700
    },
    {
      "epoch": 0.5475470820739363,
      "grad_norm": 38.57838439941406,
      "learning_rate": 1.6349686119507095e-05,
      "loss": 2.9481,
      "step": 4710
    },
    {
      "epoch": 0.5487096024180423,
      "grad_norm": 33.792930603027344,
      "learning_rate": 1.6341935983879718e-05,
      "loss": 2.6844,
      "step": 4720
    },
    {
      "epoch": 0.5498721227621484,
      "grad_norm": 23.35915756225586,
      "learning_rate": 1.6334185848252345e-05,
      "loss": 2.8346,
      "step": 4730
    },
    {
      "epoch": 0.5510346431062544,
      "grad_norm": 41.173099517822266,
      "learning_rate": 1.6326435712624972e-05,
      "loss": 2.5871,
      "step": 4740
    },
    {
      "epoch": 0.5521971634503604,
      "grad_norm": 47.21916961669922,
      "learning_rate": 1.63186855769976e-05,
      "loss": 4.6851,
      "step": 4750
    },
    {
      "epoch": 0.5533596837944664,
      "grad_norm": 23.732770919799805,
      "learning_rate": 1.6310935441370226e-05,
      "loss": 2.7265,
      "step": 4760
    },
    {
      "epoch": 0.5545222041385724,
      "grad_norm": 27.75726890563965,
      "learning_rate": 1.6303185305742853e-05,
      "loss": 2.2111,
      "step": 4770
    },
    {
      "epoch": 0.5556847244826785,
      "grad_norm": 32.1191520690918,
      "learning_rate": 1.629543517011548e-05,
      "loss": 3.2371,
      "step": 4780
    },
    {
      "epoch": 0.5568472448267845,
      "grad_norm": 39.349491119384766,
      "learning_rate": 1.6287685034488104e-05,
      "loss": 2.7516,
      "step": 4790
    },
    {
      "epoch": 0.5580097651708905,
      "grad_norm": 25.978208541870117,
      "learning_rate": 1.627993489886073e-05,
      "loss": 2.5842,
      "step": 4800
    },
    {
      "epoch": 0.5591722855149965,
      "grad_norm": 18.975059509277344,
      "learning_rate": 1.6272184763233358e-05,
      "loss": 3.0078,
      "step": 4810
    },
    {
      "epoch": 0.5603348058591026,
      "grad_norm": 35.90692138671875,
      "learning_rate": 1.6264434627605985e-05,
      "loss": 2.7772,
      "step": 4820
    },
    {
      "epoch": 0.5614973262032086,
      "grad_norm": 29.301706314086914,
      "learning_rate": 1.6256684491978612e-05,
      "loss": 2.3419,
      "step": 4830
    },
    {
      "epoch": 0.5626598465473146,
      "grad_norm": 37.84168243408203,
      "learning_rate": 1.624893435635124e-05,
      "loss": 2.5636,
      "step": 4840
    },
    {
      "epoch": 0.5638223668914206,
      "grad_norm": 23.43922233581543,
      "learning_rate": 1.6241184220723863e-05,
      "loss": 2.4435,
      "step": 4850
    },
    {
      "epoch": 0.5649848872355266,
      "grad_norm": 19.837936401367188,
      "learning_rate": 1.623343408509649e-05,
      "loss": 3.2888,
      "step": 4860
    },
    {
      "epoch": 0.5661474075796327,
      "grad_norm": 22.778507232666016,
      "learning_rate": 1.6225683949469117e-05,
      "loss": 2.5409,
      "step": 4870
    },
    {
      "epoch": 0.5673099279237387,
      "grad_norm": 42.457462310791016,
      "learning_rate": 1.6217933813841744e-05,
      "loss": 4.1223,
      "step": 4880
    },
    {
      "epoch": 0.5684724482678447,
      "grad_norm": 38.729759216308594,
      "learning_rate": 1.621018367821437e-05,
      "loss": 2.6121,
      "step": 4890
    },
    {
      "epoch": 0.5696349686119507,
      "grad_norm": 30.497766494750977,
      "learning_rate": 1.6202433542586998e-05,
      "loss": 3.0348,
      "step": 4900
    },
    {
      "epoch": 0.5707974889560568,
      "grad_norm": 22.390138626098633,
      "learning_rate": 1.6194683406959625e-05,
      "loss": 3.8915,
      "step": 4910
    },
    {
      "epoch": 0.5719600093001628,
      "grad_norm": 27.848630905151367,
      "learning_rate": 1.618693327133225e-05,
      "loss": 2.8801,
      "step": 4920
    },
    {
      "epoch": 0.5731225296442688,
      "grad_norm": 31.194852828979492,
      "learning_rate": 1.6179183135704876e-05,
      "loss": 3.2366,
      "step": 4930
    },
    {
      "epoch": 0.5742850499883748,
      "grad_norm": 21.34173583984375,
      "learning_rate": 1.6171433000077503e-05,
      "loss": 3.6568,
      "step": 4940
    },
    {
      "epoch": 0.5754475703324808,
      "grad_norm": 29.627094268798828,
      "learning_rate": 1.616368286445013e-05,
      "loss": 3.3491,
      "step": 4950
    },
    {
      "epoch": 0.5766100906765869,
      "grad_norm": 24.15519142150879,
      "learning_rate": 1.6155932728822757e-05,
      "loss": 3.8497,
      "step": 4960
    },
    {
      "epoch": 0.5777726110206929,
      "grad_norm": 20.2622013092041,
      "learning_rate": 1.6148182593195384e-05,
      "loss": 2.6527,
      "step": 4970
    },
    {
      "epoch": 0.5789351313647989,
      "grad_norm": 32.108795166015625,
      "learning_rate": 1.614043245756801e-05,
      "loss": 2.7208,
      "step": 4980
    },
    {
      "epoch": 0.5800976517089049,
      "grad_norm": 20.155071258544922,
      "learning_rate": 1.6132682321940635e-05,
      "loss": 2.4308,
      "step": 4990
    },
    {
      "epoch": 0.581260172053011,
      "grad_norm": 24.375362396240234,
      "learning_rate": 1.612493218631326e-05,
      "loss": 2.5278,
      "step": 5000
    },
    {
      "epoch": 0.582422692397117,
      "grad_norm": 27.08806037902832,
      "learning_rate": 1.611718205068589e-05,
      "loss": 4.3734,
      "step": 5010
    },
    {
      "epoch": 0.583585212741223,
      "grad_norm": 27.81964111328125,
      "learning_rate": 1.6109431915058516e-05,
      "loss": 3.9014,
      "step": 5020
    },
    {
      "epoch": 0.584747733085329,
      "grad_norm": 28.372114181518555,
      "learning_rate": 1.6101681779431143e-05,
      "loss": 2.7087,
      "step": 5030
    },
    {
      "epoch": 0.585910253429435,
      "grad_norm": 28.87789535522461,
      "learning_rate": 1.609393164380377e-05,
      "loss": 3.557,
      "step": 5040
    },
    {
      "epoch": 0.5870727737735411,
      "grad_norm": 30.698781967163086,
      "learning_rate": 1.6086181508176393e-05,
      "loss": 3.6251,
      "step": 5050
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 36.02765655517578,
      "learning_rate": 1.607843137254902e-05,
      "loss": 3.6299,
      "step": 5060
    },
    {
      "epoch": 0.5893978144617531,
      "grad_norm": 27.427547454833984,
      "learning_rate": 1.6070681236921647e-05,
      "loss": 2.304,
      "step": 5070
    },
    {
      "epoch": 0.5905603348058591,
      "grad_norm": 26.70867919921875,
      "learning_rate": 1.6062931101294275e-05,
      "loss": 3.1148,
      "step": 5080
    },
    {
      "epoch": 0.5917228551499651,
      "grad_norm": 26.99342155456543,
      "learning_rate": 1.60551809656669e-05,
      "loss": 4.1486,
      "step": 5090
    },
    {
      "epoch": 0.5928853754940712,
      "grad_norm": 44.878299713134766,
      "learning_rate": 1.604743083003953e-05,
      "loss": 3.691,
      "step": 5100
    },
    {
      "epoch": 0.5940478958381772,
      "grad_norm": 35.67900848388672,
      "learning_rate": 1.6039680694412156e-05,
      "loss": 3.4657,
      "step": 5110
    },
    {
      "epoch": 0.5952104161822832,
      "grad_norm": 27.432435989379883,
      "learning_rate": 1.603193055878478e-05,
      "loss": 2.5854,
      "step": 5120
    },
    {
      "epoch": 0.5963729365263892,
      "grad_norm": 40.10652542114258,
      "learning_rate": 1.6024180423157406e-05,
      "loss": 3.5621,
      "step": 5130
    },
    {
      "epoch": 0.5975354568704953,
      "grad_norm": 32.02836227416992,
      "learning_rate": 1.6016430287530033e-05,
      "loss": 3.3965,
      "step": 5140
    },
    {
      "epoch": 0.5986979772146013,
      "grad_norm": 31.051115036010742,
      "learning_rate": 1.600868015190266e-05,
      "loss": 3.2046,
      "step": 5150
    },
    {
      "epoch": 0.5998604975587073,
      "grad_norm": 29.933813095092773,
      "learning_rate": 1.6000930016275287e-05,
      "loss": 2.4616,
      "step": 5160
    },
    {
      "epoch": 0.6010230179028133,
      "grad_norm": 31.251026153564453,
      "learning_rate": 1.5993179880647914e-05,
      "loss": 3.4275,
      "step": 5170
    },
    {
      "epoch": 0.6021855382469193,
      "grad_norm": 31.91175651550293,
      "learning_rate": 1.5985429745020538e-05,
      "loss": 2.5033,
      "step": 5180
    },
    {
      "epoch": 0.6033480585910254,
      "grad_norm": 32.709590911865234,
      "learning_rate": 1.5977679609393165e-05,
      "loss": 4.5578,
      "step": 5190
    },
    {
      "epoch": 0.6045105789351314,
      "grad_norm": 40.6065559387207,
      "learning_rate": 1.5969929473765792e-05,
      "loss": 3.1872,
      "step": 5200
    },
    {
      "epoch": 0.6056730992792374,
      "grad_norm": 37.44639587402344,
      "learning_rate": 1.596217933813842e-05,
      "loss": 2.8537,
      "step": 5210
    },
    {
      "epoch": 0.6068356196233434,
      "grad_norm": 37.459415435791016,
      "learning_rate": 1.5954429202511046e-05,
      "loss": 3.3424,
      "step": 5220
    },
    {
      "epoch": 0.6079981399674494,
      "grad_norm": 27.746103286743164,
      "learning_rate": 1.5946679066883673e-05,
      "loss": 3.1187,
      "step": 5230
    },
    {
      "epoch": 0.6091606603115555,
      "grad_norm": 30.64653205871582,
      "learning_rate": 1.59389289312563e-05,
      "loss": 2.3393,
      "step": 5240
    },
    {
      "epoch": 0.6103231806556615,
      "grad_norm": 28.76519012451172,
      "learning_rate": 1.5931178795628924e-05,
      "loss": 3.0004,
      "step": 5250
    },
    {
      "epoch": 0.6114857009997675,
      "grad_norm": 20.17160987854004,
      "learning_rate": 1.592342866000155e-05,
      "loss": 3.8111,
      "step": 5260
    },
    {
      "epoch": 0.6126482213438735,
      "grad_norm": 37.88634490966797,
      "learning_rate": 1.5915678524374178e-05,
      "loss": 2.658,
      "step": 5270
    },
    {
      "epoch": 0.6138107416879796,
      "grad_norm": 25.32897186279297,
      "learning_rate": 1.5907928388746805e-05,
      "loss": 2.9661,
      "step": 5280
    },
    {
      "epoch": 0.6149732620320856,
      "grad_norm": 30.631423950195312,
      "learning_rate": 1.5900178253119432e-05,
      "loss": 2.851,
      "step": 5290
    },
    {
      "epoch": 0.6161357823761916,
      "grad_norm": 39.77838134765625,
      "learning_rate": 1.589242811749206e-05,
      "loss": 3.0948,
      "step": 5300
    },
    {
      "epoch": 0.6172983027202976,
      "grad_norm": 29.943201065063477,
      "learning_rate": 1.5884677981864686e-05,
      "loss": 2.2961,
      "step": 5310
    },
    {
      "epoch": 0.6184608230644036,
      "grad_norm": 26.953556060791016,
      "learning_rate": 1.587692784623731e-05,
      "loss": 2.2815,
      "step": 5320
    },
    {
      "epoch": 0.6196233434085097,
      "grad_norm": 46.32576370239258,
      "learning_rate": 1.5869177710609937e-05,
      "loss": 4.1178,
      "step": 5330
    },
    {
      "epoch": 0.6207858637526157,
      "grad_norm": 21.24477195739746,
      "learning_rate": 1.5861427574982564e-05,
      "loss": 3.3984,
      "step": 5340
    },
    {
      "epoch": 0.6219483840967217,
      "grad_norm": 35.61503219604492,
      "learning_rate": 1.585367743935519e-05,
      "loss": 3.8946,
      "step": 5350
    },
    {
      "epoch": 0.6231109044408277,
      "grad_norm": 26.373493194580078,
      "learning_rate": 1.5845927303727818e-05,
      "loss": 2.3236,
      "step": 5360
    },
    {
      "epoch": 0.6242734247849338,
      "grad_norm": 36.47246170043945,
      "learning_rate": 1.5838177168100445e-05,
      "loss": 2.9813,
      "step": 5370
    },
    {
      "epoch": 0.6254359451290398,
      "grad_norm": 41.40970993041992,
      "learning_rate": 1.583042703247307e-05,
      "loss": 2.4531,
      "step": 5380
    },
    {
      "epoch": 0.6265984654731458,
      "grad_norm": 21.346752166748047,
      "learning_rate": 1.5822676896845696e-05,
      "loss": 3.1503,
      "step": 5390
    },
    {
      "epoch": 0.6277609858172518,
      "grad_norm": 29.368144989013672,
      "learning_rate": 1.5814926761218323e-05,
      "loss": 2.8558,
      "step": 5400
    },
    {
      "epoch": 0.6289235061613578,
      "grad_norm": 30.472166061401367,
      "learning_rate": 1.580717662559095e-05,
      "loss": 2.8034,
      "step": 5410
    },
    {
      "epoch": 0.6300860265054639,
      "grad_norm": 34.27297592163086,
      "learning_rate": 1.5799426489963577e-05,
      "loss": 3.1732,
      "step": 5420
    },
    {
      "epoch": 0.6312485468495699,
      "grad_norm": 42.28227615356445,
      "learning_rate": 1.5791676354336204e-05,
      "loss": 3.2331,
      "step": 5430
    },
    {
      "epoch": 0.6324110671936759,
      "grad_norm": 27.32302474975586,
      "learning_rate": 1.578392621870883e-05,
      "loss": 2.4845,
      "step": 5440
    },
    {
      "epoch": 0.6335735875377819,
      "grad_norm": 26.36339569091797,
      "learning_rate": 1.5776176083081454e-05,
      "loss": 2.2522,
      "step": 5450
    },
    {
      "epoch": 0.634736107881888,
      "grad_norm": 41.702117919921875,
      "learning_rate": 1.576842594745408e-05,
      "loss": 2.7417,
      "step": 5460
    },
    {
      "epoch": 0.635898628225994,
      "grad_norm": 28.76569938659668,
      "learning_rate": 1.576067581182671e-05,
      "loss": 3.5105,
      "step": 5470
    },
    {
      "epoch": 0.6370611485701,
      "grad_norm": 37.26204299926758,
      "learning_rate": 1.5752925676199336e-05,
      "loss": 2.9355,
      "step": 5480
    },
    {
      "epoch": 0.638223668914206,
      "grad_norm": 32.04623794555664,
      "learning_rate": 1.5745175540571963e-05,
      "loss": 3.0328,
      "step": 5490
    },
    {
      "epoch": 0.639386189258312,
      "grad_norm": 28.90808868408203,
      "learning_rate": 1.573742540494459e-05,
      "loss": 2.9849,
      "step": 5500
    },
    {
      "epoch": 0.640548709602418,
      "grad_norm": 40.01094436645508,
      "learning_rate": 1.5729675269317213e-05,
      "loss": 3.7023,
      "step": 5510
    },
    {
      "epoch": 0.6417112299465241,
      "grad_norm": 30.429807662963867,
      "learning_rate": 1.572192513368984e-05,
      "loss": 3.2928,
      "step": 5520
    },
    {
      "epoch": 0.6428737502906301,
      "grad_norm": 21.475292205810547,
      "learning_rate": 1.5714174998062467e-05,
      "loss": 3.0512,
      "step": 5530
    },
    {
      "epoch": 0.6440362706347361,
      "grad_norm": 28.94291877746582,
      "learning_rate": 1.5706424862435094e-05,
      "loss": 2.5143,
      "step": 5540
    },
    {
      "epoch": 0.6451987909788421,
      "grad_norm": 37.71289825439453,
      "learning_rate": 1.569867472680772e-05,
      "loss": 2.8765,
      "step": 5550
    },
    {
      "epoch": 0.6463613113229482,
      "grad_norm": 27.15548324584961,
      "learning_rate": 1.569092459118035e-05,
      "loss": 2.7219,
      "step": 5560
    },
    {
      "epoch": 0.6475238316670542,
      "grad_norm": 24.21993637084961,
      "learning_rate": 1.5683174455552976e-05,
      "loss": 3.3735,
      "step": 5570
    },
    {
      "epoch": 0.6486863520111602,
      "grad_norm": 25.57848358154297,
      "learning_rate": 1.56754243199256e-05,
      "loss": 2.9256,
      "step": 5580
    },
    {
      "epoch": 0.6498488723552662,
      "grad_norm": 23.222740173339844,
      "learning_rate": 1.5667674184298226e-05,
      "loss": 2.6201,
      "step": 5590
    },
    {
      "epoch": 0.6510113926993722,
      "grad_norm": 26.56859588623047,
      "learning_rate": 1.5659924048670853e-05,
      "loss": 2.3747,
      "step": 5600
    },
    {
      "epoch": 0.6521739130434783,
      "grad_norm": 28.294137954711914,
      "learning_rate": 1.565217391304348e-05,
      "loss": 4.6563,
      "step": 5610
    },
    {
      "epoch": 0.6533364333875843,
      "grad_norm": 31.38463592529297,
      "learning_rate": 1.5644423777416107e-05,
      "loss": 2.8396,
      "step": 5620
    },
    {
      "epoch": 0.6544989537316903,
      "grad_norm": 27.24254035949707,
      "learning_rate": 1.5636673641788734e-05,
      "loss": 3.2595,
      "step": 5630
    },
    {
      "epoch": 0.6556614740757963,
      "grad_norm": 23.98681640625,
      "learning_rate": 1.562892350616136e-05,
      "loss": 2.6298,
      "step": 5640
    },
    {
      "epoch": 0.6568239944199024,
      "grad_norm": 17.75348472595215,
      "learning_rate": 1.5621173370533985e-05,
      "loss": 2.7275,
      "step": 5650
    },
    {
      "epoch": 0.6579865147640084,
      "grad_norm": 33.93280029296875,
      "learning_rate": 1.5613423234906612e-05,
      "loss": 3.3696,
      "step": 5660
    },
    {
      "epoch": 0.6591490351081144,
      "grad_norm": 27.91224479675293,
      "learning_rate": 1.560567309927924e-05,
      "loss": 3.016,
      "step": 5670
    },
    {
      "epoch": 0.6603115554522204,
      "grad_norm": 24.666677474975586,
      "learning_rate": 1.5597922963651866e-05,
      "loss": 3.7707,
      "step": 5680
    },
    {
      "epoch": 0.6614740757963264,
      "grad_norm": 35.054832458496094,
      "learning_rate": 1.5590172828024493e-05,
      "loss": 2.8244,
      "step": 5690
    },
    {
      "epoch": 0.6626365961404325,
      "grad_norm": 29.132503509521484,
      "learning_rate": 1.558242269239712e-05,
      "loss": 3.1892,
      "step": 5700
    },
    {
      "epoch": 0.6637991164845385,
      "grad_norm": 29.3184871673584,
      "learning_rate": 1.5574672556769744e-05,
      "loss": 3.1401,
      "step": 5710
    },
    {
      "epoch": 0.6649616368286445,
      "grad_norm": 29.899974822998047,
      "learning_rate": 1.556692242114237e-05,
      "loss": 2.726,
      "step": 5720
    },
    {
      "epoch": 0.6661241571727505,
      "grad_norm": 44.24229049682617,
      "learning_rate": 1.5559172285514998e-05,
      "loss": 2.4942,
      "step": 5730
    },
    {
      "epoch": 0.6672866775168566,
      "grad_norm": 24.520998001098633,
      "learning_rate": 1.5551422149887625e-05,
      "loss": 3.3842,
      "step": 5740
    },
    {
      "epoch": 0.6684491978609626,
      "grad_norm": 30.70489501953125,
      "learning_rate": 1.5543672014260252e-05,
      "loss": 2.9427,
      "step": 5750
    },
    {
      "epoch": 0.6696117182050686,
      "grad_norm": 30.859790802001953,
      "learning_rate": 1.553592187863288e-05,
      "loss": 2.6777,
      "step": 5760
    },
    {
      "epoch": 0.6707742385491746,
      "grad_norm": 21.606544494628906,
      "learning_rate": 1.5528171743005506e-05,
      "loss": 2.5977,
      "step": 5770
    },
    {
      "epoch": 0.6719367588932806,
      "grad_norm": 24.426706314086914,
      "learning_rate": 1.552042160737813e-05,
      "loss": 2.7509,
      "step": 5780
    },
    {
      "epoch": 0.6730992792373867,
      "grad_norm": 29.56556510925293,
      "learning_rate": 1.5512671471750757e-05,
      "loss": 2.5483,
      "step": 5790
    },
    {
      "epoch": 0.6742617995814927,
      "grad_norm": 35.59151077270508,
      "learning_rate": 1.5504921336123384e-05,
      "loss": 2.6195,
      "step": 5800
    },
    {
      "epoch": 0.6754243199255987,
      "grad_norm": 33.481040954589844,
      "learning_rate": 1.549717120049601e-05,
      "loss": 2.7701,
      "step": 5810
    },
    {
      "epoch": 0.6765868402697047,
      "grad_norm": 28.104448318481445,
      "learning_rate": 1.5489421064868638e-05,
      "loss": 3.0594,
      "step": 5820
    },
    {
      "epoch": 0.6777493606138107,
      "grad_norm": 41.57675552368164,
      "learning_rate": 1.5481670929241265e-05,
      "loss": 2.4496,
      "step": 5830
    },
    {
      "epoch": 0.6789118809579168,
      "grad_norm": 30.430540084838867,
      "learning_rate": 1.547392079361389e-05,
      "loss": 2.7265,
      "step": 5840
    },
    {
      "epoch": 0.6800744013020228,
      "grad_norm": 27.884981155395508,
      "learning_rate": 1.5466170657986516e-05,
      "loss": 2.7531,
      "step": 5850
    },
    {
      "epoch": 0.6812369216461288,
      "grad_norm": 31.458728790283203,
      "learning_rate": 1.5458420522359143e-05,
      "loss": 4.0759,
      "step": 5860
    },
    {
      "epoch": 0.6823994419902348,
      "grad_norm": 45.187686920166016,
      "learning_rate": 1.545067038673177e-05,
      "loss": 2.9133,
      "step": 5870
    },
    {
      "epoch": 0.6835619623343409,
      "grad_norm": 30.140087127685547,
      "learning_rate": 1.5442920251104397e-05,
      "loss": 3.2601,
      "step": 5880
    },
    {
      "epoch": 0.6847244826784469,
      "grad_norm": 29.11714744567871,
      "learning_rate": 1.5435170115477024e-05,
      "loss": 2.3747,
      "step": 5890
    },
    {
      "epoch": 0.6858870030225529,
      "grad_norm": 23.566177368164062,
      "learning_rate": 1.542741997984965e-05,
      "loss": 2.3923,
      "step": 5900
    },
    {
      "epoch": 0.6870495233666589,
      "grad_norm": 32.66679382324219,
      "learning_rate": 1.5419669844222274e-05,
      "loss": 2.6279,
      "step": 5910
    },
    {
      "epoch": 0.6882120437107649,
      "grad_norm": 27.661714553833008,
      "learning_rate": 1.54119197085949e-05,
      "loss": 3.5662,
      "step": 5920
    },
    {
      "epoch": 0.689374564054871,
      "grad_norm": 37.980926513671875,
      "learning_rate": 1.540416957296753e-05,
      "loss": 3.2492,
      "step": 5930
    },
    {
      "epoch": 0.690537084398977,
      "grad_norm": 24.98939323425293,
      "learning_rate": 1.5396419437340155e-05,
      "loss": 3.3743,
      "step": 5940
    },
    {
      "epoch": 0.691699604743083,
      "grad_norm": 27.806299209594727,
      "learning_rate": 1.5388669301712783e-05,
      "loss": 2.8021,
      "step": 5950
    },
    {
      "epoch": 0.692862125087189,
      "grad_norm": 28.496353149414062,
      "learning_rate": 1.538091916608541e-05,
      "loss": 2.3758,
      "step": 5960
    },
    {
      "epoch": 0.694024645431295,
      "grad_norm": 26.7783203125,
      "learning_rate": 1.5373169030458037e-05,
      "loss": 2.5733,
      "step": 5970
    },
    {
      "epoch": 0.6951871657754011,
      "grad_norm": 27.545164108276367,
      "learning_rate": 1.536541889483066e-05,
      "loss": 2.5364,
      "step": 5980
    },
    {
      "epoch": 0.6963496861195071,
      "grad_norm": 33.59083557128906,
      "learning_rate": 1.5357668759203287e-05,
      "loss": 3.16,
      "step": 5990
    },
    {
      "epoch": 0.6975122064636131,
      "grad_norm": 29.391525268554688,
      "learning_rate": 1.5349918623575914e-05,
      "loss": 3.1024,
      "step": 6000
    },
    {
      "epoch": 0.6986747268077191,
      "grad_norm": 34.50709533691406,
      "learning_rate": 1.534216848794854e-05,
      "loss": 3.6634,
      "step": 6010
    },
    {
      "epoch": 0.6998372471518252,
      "grad_norm": 25.227781295776367,
      "learning_rate": 1.533441835232117e-05,
      "loss": 2.7191,
      "step": 6020
    },
    {
      "epoch": 0.7009997674959312,
      "grad_norm": 33.190406799316406,
      "learning_rate": 1.5326668216693795e-05,
      "loss": 3.4678,
      "step": 6030
    },
    {
      "epoch": 0.7021622878400372,
      "grad_norm": 20.973556518554688,
      "learning_rate": 1.531891808106642e-05,
      "loss": 2.495,
      "step": 6040
    },
    {
      "epoch": 0.7033248081841432,
      "grad_norm": 30.234485626220703,
      "learning_rate": 1.5311167945439046e-05,
      "loss": 2.959,
      "step": 6050
    },
    {
      "epoch": 0.7044873285282492,
      "grad_norm": 30.5008487701416,
      "learning_rate": 1.5303417809811673e-05,
      "loss": 2.7448,
      "step": 6060
    },
    {
      "epoch": 0.7056498488723553,
      "grad_norm": 28.769630432128906,
      "learning_rate": 1.52956676741843e-05,
      "loss": 2.3704,
      "step": 6070
    },
    {
      "epoch": 0.7068123692164613,
      "grad_norm": 35.921119689941406,
      "learning_rate": 1.5287917538556927e-05,
      "loss": 2.5652,
      "step": 6080
    },
    {
      "epoch": 0.7079748895605673,
      "grad_norm": 35.172794342041016,
      "learning_rate": 1.5280167402929554e-05,
      "loss": 4.0353,
      "step": 6090
    },
    {
      "epoch": 0.7091374099046733,
      "grad_norm": 32.77566146850586,
      "learning_rate": 1.527241726730218e-05,
      "loss": 2.8704,
      "step": 6100
    },
    {
      "epoch": 0.7102999302487794,
      "grad_norm": 32.34416198730469,
      "learning_rate": 1.5264667131674805e-05,
      "loss": 3.0053,
      "step": 6110
    },
    {
      "epoch": 0.7114624505928854,
      "grad_norm": 51.5576171875,
      "learning_rate": 1.5256916996047434e-05,
      "loss": 2.2101,
      "step": 6120
    },
    {
      "epoch": 0.7126249709369914,
      "grad_norm": 37.348880767822266,
      "learning_rate": 1.5249166860420059e-05,
      "loss": 2.7331,
      "step": 6130
    },
    {
      "epoch": 0.7137874912810974,
      "grad_norm": 28.36870574951172,
      "learning_rate": 1.5241416724792686e-05,
      "loss": 3.903,
      "step": 6140
    },
    {
      "epoch": 0.7149500116252034,
      "grad_norm": 42.72369384765625,
      "learning_rate": 1.5233666589165313e-05,
      "loss": 2.2046,
      "step": 6150
    },
    {
      "epoch": 0.7161125319693095,
      "grad_norm": 31.473180770874023,
      "learning_rate": 1.5225916453537938e-05,
      "loss": 3.0181,
      "step": 6160
    },
    {
      "epoch": 0.7172750523134155,
      "grad_norm": 36.312599182128906,
      "learning_rate": 1.5218166317910565e-05,
      "loss": 2.7989,
      "step": 6170
    },
    {
      "epoch": 0.7184375726575215,
      "grad_norm": 28.40117645263672,
      "learning_rate": 1.5210416182283192e-05,
      "loss": 2.6045,
      "step": 6180
    },
    {
      "epoch": 0.7196000930016275,
      "grad_norm": 22.8549861907959,
      "learning_rate": 1.5202666046655818e-05,
      "loss": 3.5238,
      "step": 6190
    },
    {
      "epoch": 0.7207626133457335,
      "grad_norm": 28.250001907348633,
      "learning_rate": 1.5194915911028445e-05,
      "loss": 2.2533,
      "step": 6200
    },
    {
      "epoch": 0.7219251336898396,
      "grad_norm": 32.97951889038086,
      "learning_rate": 1.5187165775401072e-05,
      "loss": 2.9558,
      "step": 6210
    },
    {
      "epoch": 0.7230876540339456,
      "grad_norm": 34.7393913269043,
      "learning_rate": 1.5179415639773697e-05,
      "loss": 2.6058,
      "step": 6220
    },
    {
      "epoch": 0.7242501743780516,
      "grad_norm": 26.03693389892578,
      "learning_rate": 1.5171665504146324e-05,
      "loss": 2.4826,
      "step": 6230
    },
    {
      "epoch": 0.7254126947221576,
      "grad_norm": 28.084009170532227,
      "learning_rate": 1.5163915368518951e-05,
      "loss": 3.1734,
      "step": 6240
    },
    {
      "epoch": 0.7265752150662637,
      "grad_norm": 22.640342712402344,
      "learning_rate": 1.5156165232891578e-05,
      "loss": 2.1167,
      "step": 6250
    },
    {
      "epoch": 0.7277377354103697,
      "grad_norm": 24.532011032104492,
      "learning_rate": 1.5148415097264204e-05,
      "loss": 2.6833,
      "step": 6260
    },
    {
      "epoch": 0.7289002557544757,
      "grad_norm": 28.183170318603516,
      "learning_rate": 1.514066496163683e-05,
      "loss": 3.8511,
      "step": 6270
    },
    {
      "epoch": 0.7300627760985817,
      "grad_norm": 30.958280563354492,
      "learning_rate": 1.5132914826009458e-05,
      "loss": 3.7242,
      "step": 6280
    },
    {
      "epoch": 0.7312252964426877,
      "grad_norm": 22.716764450073242,
      "learning_rate": 1.5125164690382083e-05,
      "loss": 2.5929,
      "step": 6290
    },
    {
      "epoch": 0.7323878167867938,
      "grad_norm": 32.21988296508789,
      "learning_rate": 1.511741455475471e-05,
      "loss": 3.0028,
      "step": 6300
    },
    {
      "epoch": 0.7335503371308998,
      "grad_norm": 22.76768684387207,
      "learning_rate": 1.5109664419127337e-05,
      "loss": 3.8533,
      "step": 6310
    },
    {
      "epoch": 0.7347128574750058,
      "grad_norm": 23.90313720703125,
      "learning_rate": 1.5101914283499962e-05,
      "loss": 2.735,
      "step": 6320
    },
    {
      "epoch": 0.7358753778191118,
      "grad_norm": 27.976451873779297,
      "learning_rate": 1.509416414787259e-05,
      "loss": 3.2065,
      "step": 6330
    },
    {
      "epoch": 0.7370378981632179,
      "grad_norm": 26.687530517578125,
      "learning_rate": 1.5086414012245217e-05,
      "loss": 3.2652,
      "step": 6340
    },
    {
      "epoch": 0.7382004185073239,
      "grad_norm": 29.859777450561523,
      "learning_rate": 1.5078663876617844e-05,
      "loss": 3.2373,
      "step": 6350
    },
    {
      "epoch": 0.7393629388514299,
      "grad_norm": 24.27518653869629,
      "learning_rate": 1.5070913740990469e-05,
      "loss": 3.0806,
      "step": 6360
    },
    {
      "epoch": 0.7405254591955359,
      "grad_norm": 31.975872039794922,
      "learning_rate": 1.5063163605363096e-05,
      "loss": 2.5408,
      "step": 6370
    },
    {
      "epoch": 0.7416879795396419,
      "grad_norm": 44.46553039550781,
      "learning_rate": 1.5055413469735723e-05,
      "loss": 2.393,
      "step": 6380
    },
    {
      "epoch": 0.742850499883748,
      "grad_norm": 34.26624298095703,
      "learning_rate": 1.5047663334108348e-05,
      "loss": 2.4589,
      "step": 6390
    },
    {
      "epoch": 0.744013020227854,
      "grad_norm": 34.254154205322266,
      "learning_rate": 1.5039913198480975e-05,
      "loss": 2.7041,
      "step": 6400
    },
    {
      "epoch": 0.74517554057196,
      "grad_norm": 24.88010025024414,
      "learning_rate": 1.5032163062853602e-05,
      "loss": 4.2801,
      "step": 6410
    },
    {
      "epoch": 0.746338060916066,
      "grad_norm": 27.128398895263672,
      "learning_rate": 1.5024412927226228e-05,
      "loss": 2.367,
      "step": 6420
    },
    {
      "epoch": 0.747500581260172,
      "grad_norm": 30.014799118041992,
      "learning_rate": 1.5016662791598855e-05,
      "loss": 3.5442,
      "step": 6430
    },
    {
      "epoch": 0.7486631016042781,
      "grad_norm": 17.994525909423828,
      "learning_rate": 1.5008912655971482e-05,
      "loss": 2.8702,
      "step": 6440
    },
    {
      "epoch": 0.7498256219483841,
      "grad_norm": 30.430644989013672,
      "learning_rate": 1.5001162520344109e-05,
      "loss": 2.162,
      "step": 6450
    },
    {
      "epoch": 0.7509881422924901,
      "grad_norm": 29.62386131286621,
      "learning_rate": 1.4993412384716734e-05,
      "loss": 3.0382,
      "step": 6460
    },
    {
      "epoch": 0.7521506626365961,
      "grad_norm": 25.5085506439209,
      "learning_rate": 1.4985662249089361e-05,
      "loss": 3.1839,
      "step": 6470
    },
    {
      "epoch": 0.7533131829807022,
      "grad_norm": 23.967296600341797,
      "learning_rate": 1.4977912113461988e-05,
      "loss": 2.743,
      "step": 6480
    },
    {
      "epoch": 0.7544757033248082,
      "grad_norm": 29.44037437438965,
      "learning_rate": 1.4970161977834614e-05,
      "loss": 3.2564,
      "step": 6490
    },
    {
      "epoch": 0.7556382236689142,
      "grad_norm": 32.69945526123047,
      "learning_rate": 1.496241184220724e-05,
      "loss": 2.3262,
      "step": 6500
    },
    {
      "epoch": 0.7568007440130202,
      "grad_norm": 36.248390197753906,
      "learning_rate": 1.4954661706579868e-05,
      "loss": 2.7953,
      "step": 6510
    },
    {
      "epoch": 0.7579632643571262,
      "grad_norm": 37.2323112487793,
      "learning_rate": 1.4946911570952493e-05,
      "loss": 2.8472,
      "step": 6520
    },
    {
      "epoch": 0.7591257847012323,
      "grad_norm": 28.399526596069336,
      "learning_rate": 1.493916143532512e-05,
      "loss": 3.7984,
      "step": 6530
    },
    {
      "epoch": 0.7602883050453383,
      "grad_norm": 42.6761589050293,
      "learning_rate": 1.4931411299697747e-05,
      "loss": 2.7583,
      "step": 6540
    },
    {
      "epoch": 0.7614508253894443,
      "grad_norm": 27.61181640625,
      "learning_rate": 1.4923661164070372e-05,
      "loss": 2.3405,
      "step": 6550
    },
    {
      "epoch": 0.7626133457335503,
      "grad_norm": 25.654682159423828,
      "learning_rate": 1.4915911028443e-05,
      "loss": 2.1836,
      "step": 6560
    },
    {
      "epoch": 0.7637758660776564,
      "grad_norm": 30.77394676208496,
      "learning_rate": 1.4908160892815627e-05,
      "loss": 3.1008,
      "step": 6570
    },
    {
      "epoch": 0.7649383864217624,
      "grad_norm": 25.481399536132812,
      "learning_rate": 1.4900410757188254e-05,
      "loss": 2.9132,
      "step": 6580
    },
    {
      "epoch": 0.7661009067658684,
      "grad_norm": 30.789154052734375,
      "learning_rate": 1.4892660621560879e-05,
      "loss": 2.9916,
      "step": 6590
    },
    {
      "epoch": 0.7672634271099744,
      "grad_norm": 38.20836639404297,
      "learning_rate": 1.4884910485933506e-05,
      "loss": 2.5989,
      "step": 6600
    },
    {
      "epoch": 0.7684259474540804,
      "grad_norm": 28.188907623291016,
      "learning_rate": 1.4877160350306133e-05,
      "loss": 3.2867,
      "step": 6610
    },
    {
      "epoch": 0.7695884677981865,
      "grad_norm": 42.59140396118164,
      "learning_rate": 1.4869410214678758e-05,
      "loss": 3.5301,
      "step": 6620
    },
    {
      "epoch": 0.7707509881422925,
      "grad_norm": 41.26364517211914,
      "learning_rate": 1.4861660079051385e-05,
      "loss": 2.3573,
      "step": 6630
    },
    {
      "epoch": 0.7719135084863985,
      "grad_norm": 34.22455978393555,
      "learning_rate": 1.4853909943424012e-05,
      "loss": 2.854,
      "step": 6640
    },
    {
      "epoch": 0.7730760288305045,
      "grad_norm": 22.985868453979492,
      "learning_rate": 1.4846159807796638e-05,
      "loss": 3.1255,
      "step": 6650
    },
    {
      "epoch": 0.7742385491746105,
      "grad_norm": 36.20173645019531,
      "learning_rate": 1.4838409672169265e-05,
      "loss": 3.2009,
      "step": 6660
    },
    {
      "epoch": 0.7754010695187166,
      "grad_norm": 39.07079315185547,
      "learning_rate": 1.4830659536541892e-05,
      "loss": 3.5957,
      "step": 6670
    },
    {
      "epoch": 0.7765635898628226,
      "grad_norm": 36.17494201660156,
      "learning_rate": 1.4822909400914519e-05,
      "loss": 2.0815,
      "step": 6680
    },
    {
      "epoch": 0.7777261102069286,
      "grad_norm": 24.319908142089844,
      "learning_rate": 1.4815159265287144e-05,
      "loss": 2.1873,
      "step": 6690
    },
    {
      "epoch": 0.7788886305510346,
      "grad_norm": 30.521081924438477,
      "learning_rate": 1.4807409129659771e-05,
      "loss": 2.7253,
      "step": 6700
    },
    {
      "epoch": 0.7800511508951407,
      "grad_norm": 21.113231658935547,
      "learning_rate": 1.4799658994032398e-05,
      "loss": 3.5514,
      "step": 6710
    },
    {
      "epoch": 0.7812136712392467,
      "grad_norm": 26.927160263061523,
      "learning_rate": 1.4791908858405024e-05,
      "loss": 3.2394,
      "step": 6720
    },
    {
      "epoch": 0.7823761915833527,
      "grad_norm": 21.104022979736328,
      "learning_rate": 1.478415872277765e-05,
      "loss": 3.0351,
      "step": 6730
    },
    {
      "epoch": 0.7835387119274587,
      "grad_norm": 34.314170837402344,
      "learning_rate": 1.4776408587150278e-05,
      "loss": 4.2064,
      "step": 6740
    },
    {
      "epoch": 0.7847012322715647,
      "grad_norm": 34.18428421020508,
      "learning_rate": 1.4768658451522903e-05,
      "loss": 2.913,
      "step": 6750
    },
    {
      "epoch": 0.7858637526156708,
      "grad_norm": 26.190032958984375,
      "learning_rate": 1.476090831589553e-05,
      "loss": 2.8292,
      "step": 6760
    },
    {
      "epoch": 0.7870262729597768,
      "grad_norm": 27.738422393798828,
      "learning_rate": 1.4753158180268157e-05,
      "loss": 3.7455,
      "step": 6770
    },
    {
      "epoch": 0.7881887933038828,
      "grad_norm": 31.756242752075195,
      "learning_rate": 1.4745408044640784e-05,
      "loss": 2.2147,
      "step": 6780
    },
    {
      "epoch": 0.7893513136479888,
      "grad_norm": 25.608667373657227,
      "learning_rate": 1.473765790901341e-05,
      "loss": 3.5045,
      "step": 6790
    },
    {
      "epoch": 0.7905138339920948,
      "grad_norm": 27.355514526367188,
      "learning_rate": 1.4729907773386036e-05,
      "loss": 3.4536,
      "step": 6800
    },
    {
      "epoch": 0.7916763543362009,
      "grad_norm": 27.711139678955078,
      "learning_rate": 1.4722157637758663e-05,
      "loss": 2.5095,
      "step": 6810
    },
    {
      "epoch": 0.7928388746803069,
      "grad_norm": 28.936344146728516,
      "learning_rate": 1.4714407502131289e-05,
      "loss": 3.1279,
      "step": 6820
    },
    {
      "epoch": 0.7940013950244129,
      "grad_norm": 23.31390380859375,
      "learning_rate": 1.4706657366503916e-05,
      "loss": 2.6657,
      "step": 6830
    },
    {
      "epoch": 0.7951639153685189,
      "grad_norm": 21.396482467651367,
      "learning_rate": 1.4698907230876543e-05,
      "loss": 2.6942,
      "step": 6840
    },
    {
      "epoch": 0.796326435712625,
      "grad_norm": 26.171491622924805,
      "learning_rate": 1.4691157095249168e-05,
      "loss": 2.2412,
      "step": 6850
    },
    {
      "epoch": 0.797488956056731,
      "grad_norm": 44.12371826171875,
      "learning_rate": 1.4683406959621795e-05,
      "loss": 5.0657,
      "step": 6860
    },
    {
      "epoch": 0.798651476400837,
      "grad_norm": 30.91103744506836,
      "learning_rate": 1.4675656823994422e-05,
      "loss": 2.8449,
      "step": 6870
    },
    {
      "epoch": 0.799813996744943,
      "grad_norm": 34.09768295288086,
      "learning_rate": 1.4667906688367048e-05,
      "loss": 3.1457,
      "step": 6880
    },
    {
      "epoch": 0.800976517089049,
      "grad_norm": 27.338420867919922,
      "learning_rate": 1.4660156552739675e-05,
      "loss": 2.6953,
      "step": 6890
    },
    {
      "epoch": 0.8021390374331551,
      "grad_norm": 42.82953643798828,
      "learning_rate": 1.4652406417112302e-05,
      "loss": 3.407,
      "step": 6900
    },
    {
      "epoch": 0.8033015577772611,
      "grad_norm": 34.81926727294922,
      "learning_rate": 1.4644656281484929e-05,
      "loss": 2.8931,
      "step": 6910
    },
    {
      "epoch": 0.8044640781213671,
      "grad_norm": 39.91585922241211,
      "learning_rate": 1.4636906145857554e-05,
      "loss": 2.7944,
      "step": 6920
    },
    {
      "epoch": 0.8056265984654731,
      "grad_norm": 43.55312728881836,
      "learning_rate": 1.4629156010230181e-05,
      "loss": 3.5194,
      "step": 6930
    },
    {
      "epoch": 0.8067891188095792,
      "grad_norm": 37.83905792236328,
      "learning_rate": 1.4621405874602808e-05,
      "loss": 2.3534,
      "step": 6940
    },
    {
      "epoch": 0.8079516391536852,
      "grad_norm": 27.70489501953125,
      "learning_rate": 1.4613655738975434e-05,
      "loss": 2.9471,
      "step": 6950
    },
    {
      "epoch": 0.8091141594977912,
      "grad_norm": 29.276365280151367,
      "learning_rate": 1.460590560334806e-05,
      "loss": 2.7131,
      "step": 6960
    },
    {
      "epoch": 0.8102766798418972,
      "grad_norm": 27.28104019165039,
      "learning_rate": 1.4598155467720688e-05,
      "loss": 4.0007,
      "step": 6970
    },
    {
      "epoch": 0.8114392001860032,
      "grad_norm": 42.82164001464844,
      "learning_rate": 1.4590405332093313e-05,
      "loss": 3.0666,
      "step": 6980
    },
    {
      "epoch": 0.8126017205301093,
      "grad_norm": 28.074031829833984,
      "learning_rate": 1.458265519646594e-05,
      "loss": 2.8541,
      "step": 6990
    },
    {
      "epoch": 0.8137642408742153,
      "grad_norm": 26.582382202148438,
      "learning_rate": 1.4574905060838567e-05,
      "loss": 2.6137,
      "step": 7000
    },
    {
      "epoch": 0.8149267612183213,
      "grad_norm": 27.96321678161621,
      "learning_rate": 1.4567154925211194e-05,
      "loss": 2.9114,
      "step": 7010
    },
    {
      "epoch": 0.8160892815624273,
      "grad_norm": 28.656299591064453,
      "learning_rate": 1.455940478958382e-05,
      "loss": 2.5249,
      "step": 7020
    },
    {
      "epoch": 0.8172518019065333,
      "grad_norm": 33.63141632080078,
      "learning_rate": 1.4551654653956446e-05,
      "loss": 2.5962,
      "step": 7030
    },
    {
      "epoch": 0.8184143222506394,
      "grad_norm": 23.704374313354492,
      "learning_rate": 1.4543904518329073e-05,
      "loss": 3.0788,
      "step": 7040
    },
    {
      "epoch": 0.8195768425947454,
      "grad_norm": 37.839290618896484,
      "learning_rate": 1.4536154382701699e-05,
      "loss": 3.2075,
      "step": 7050
    },
    {
      "epoch": 0.8207393629388514,
      "grad_norm": 24.08723258972168,
      "learning_rate": 1.4528404247074326e-05,
      "loss": 2.4782,
      "step": 7060
    },
    {
      "epoch": 0.8219018832829574,
      "grad_norm": 41.8077278137207,
      "learning_rate": 1.4520654111446953e-05,
      "loss": 2.553,
      "step": 7070
    },
    {
      "epoch": 0.8230644036270635,
      "grad_norm": 38.6052131652832,
      "learning_rate": 1.4512903975819578e-05,
      "loss": 2.6388,
      "step": 7080
    },
    {
      "epoch": 0.8242269239711695,
      "grad_norm": 27.778491973876953,
      "learning_rate": 1.4505153840192205e-05,
      "loss": 2.3266,
      "step": 7090
    },
    {
      "epoch": 0.8253894443152755,
      "grad_norm": 20.822423934936523,
      "learning_rate": 1.4497403704564832e-05,
      "loss": 4.5448,
      "step": 7100
    },
    {
      "epoch": 0.8265519646593815,
      "grad_norm": 34.21199035644531,
      "learning_rate": 1.4489653568937458e-05,
      "loss": 3.9406,
      "step": 7110
    },
    {
      "epoch": 0.8277144850034875,
      "grad_norm": 37.83132553100586,
      "learning_rate": 1.4481903433310085e-05,
      "loss": 3.2739,
      "step": 7120
    },
    {
      "epoch": 0.8288770053475936,
      "grad_norm": 21.251243591308594,
      "learning_rate": 1.4474153297682712e-05,
      "loss": 3.5781,
      "step": 7130
    },
    {
      "epoch": 0.8300395256916996,
      "grad_norm": 26.880863189697266,
      "learning_rate": 1.4466403162055339e-05,
      "loss": 3.418,
      "step": 7140
    },
    {
      "epoch": 0.8312020460358056,
      "grad_norm": 34.94320297241211,
      "learning_rate": 1.4458653026427964e-05,
      "loss": 2.7499,
      "step": 7150
    },
    {
      "epoch": 0.8323645663799116,
      "grad_norm": 27.796222686767578,
      "learning_rate": 1.4450902890800591e-05,
      "loss": 2.3179,
      "step": 7160
    },
    {
      "epoch": 0.8335270867240177,
      "grad_norm": 35.66358947753906,
      "learning_rate": 1.4443152755173218e-05,
      "loss": 2.8148,
      "step": 7170
    },
    {
      "epoch": 0.8346896070681237,
      "grad_norm": 32.82597351074219,
      "learning_rate": 1.4435402619545843e-05,
      "loss": 3.2025,
      "step": 7180
    },
    {
      "epoch": 0.8358521274122297,
      "grad_norm": 26.649194717407227,
      "learning_rate": 1.442765248391847e-05,
      "loss": 3.176,
      "step": 7190
    },
    {
      "epoch": 0.8370146477563357,
      "grad_norm": 36.809600830078125,
      "learning_rate": 1.4419902348291098e-05,
      "loss": 2.8533,
      "step": 7200
    },
    {
      "epoch": 0.8381771681004417,
      "grad_norm": 26.728242874145508,
      "learning_rate": 1.4412152212663723e-05,
      "loss": 2.7186,
      "step": 7210
    },
    {
      "epoch": 0.8393396884445478,
      "grad_norm": 36.4923210144043,
      "learning_rate": 1.440440207703635e-05,
      "loss": 3.3959,
      "step": 7220
    },
    {
      "epoch": 0.8405022087886538,
      "grad_norm": 33.880558013916016,
      "learning_rate": 1.4396651941408977e-05,
      "loss": 3.3404,
      "step": 7230
    },
    {
      "epoch": 0.8416647291327598,
      "grad_norm": 22.21021842956543,
      "learning_rate": 1.4388901805781604e-05,
      "loss": 2.6814,
      "step": 7240
    },
    {
      "epoch": 0.8428272494768658,
      "grad_norm": 27.71785545349121,
      "learning_rate": 1.438115167015423e-05,
      "loss": 3.3166,
      "step": 7250
    },
    {
      "epoch": 0.8439897698209718,
      "grad_norm": 42.113861083984375,
      "learning_rate": 1.4373401534526856e-05,
      "loss": 4.1922,
      "step": 7260
    },
    {
      "epoch": 0.8451522901650779,
      "grad_norm": 28.491880416870117,
      "learning_rate": 1.4365651398899483e-05,
      "loss": 2.7471,
      "step": 7270
    },
    {
      "epoch": 0.8463148105091839,
      "grad_norm": 23.182846069335938,
      "learning_rate": 1.4357901263272109e-05,
      "loss": 3.6126,
      "step": 7280
    },
    {
      "epoch": 0.8474773308532899,
      "grad_norm": 29.397005081176758,
      "learning_rate": 1.4350151127644736e-05,
      "loss": 2.8788,
      "step": 7290
    },
    {
      "epoch": 0.8486398511973959,
      "grad_norm": 38.45520782470703,
      "learning_rate": 1.4342400992017363e-05,
      "loss": 3.2293,
      "step": 7300
    },
    {
      "epoch": 0.849802371541502,
      "grad_norm": 25.87125015258789,
      "learning_rate": 1.4334650856389988e-05,
      "loss": 2.4268,
      "step": 7310
    },
    {
      "epoch": 0.850964891885608,
      "grad_norm": 25.024112701416016,
      "learning_rate": 1.4326900720762615e-05,
      "loss": 2.7055,
      "step": 7320
    },
    {
      "epoch": 0.852127412229714,
      "grad_norm": 37.20816421508789,
      "learning_rate": 1.4319150585135242e-05,
      "loss": 3.4618,
      "step": 7330
    },
    {
      "epoch": 0.85328993257382,
      "grad_norm": 25.009733200073242,
      "learning_rate": 1.431140044950787e-05,
      "loss": 2.4597,
      "step": 7340
    },
    {
      "epoch": 0.854452452917926,
      "grad_norm": 27.18303680419922,
      "learning_rate": 1.4303650313880495e-05,
      "loss": 2.639,
      "step": 7350
    },
    {
      "epoch": 0.8556149732620321,
      "grad_norm": 25.232471466064453,
      "learning_rate": 1.4295900178253122e-05,
      "loss": 3.0204,
      "step": 7360
    },
    {
      "epoch": 0.8567774936061381,
      "grad_norm": 24.07147216796875,
      "learning_rate": 1.4288150042625749e-05,
      "loss": 3.1352,
      "step": 7370
    },
    {
      "epoch": 0.8579400139502441,
      "grad_norm": 24.50796127319336,
      "learning_rate": 1.4280399906998374e-05,
      "loss": 3.0106,
      "step": 7380
    },
    {
      "epoch": 0.8591025342943501,
      "grad_norm": 25.254837036132812,
      "learning_rate": 1.4272649771371001e-05,
      "loss": 2.49,
      "step": 7390
    },
    {
      "epoch": 0.8602650546384562,
      "grad_norm": 21.201507568359375,
      "learning_rate": 1.4264899635743628e-05,
      "loss": 2.4106,
      "step": 7400
    },
    {
      "epoch": 0.8614275749825622,
      "grad_norm": 36.0141487121582,
      "learning_rate": 1.4257149500116253e-05,
      "loss": 2.7388,
      "step": 7410
    },
    {
      "epoch": 0.8625900953266682,
      "grad_norm": 25.255704879760742,
      "learning_rate": 1.424939936448888e-05,
      "loss": 3.4593,
      "step": 7420
    },
    {
      "epoch": 0.8637526156707742,
      "grad_norm": 24.50728416442871,
      "learning_rate": 1.4241649228861507e-05,
      "loss": 2.9385,
      "step": 7430
    },
    {
      "epoch": 0.8649151360148802,
      "grad_norm": 40.333255767822266,
      "learning_rate": 1.4233899093234133e-05,
      "loss": 3.4714,
      "step": 7440
    },
    {
      "epoch": 0.8660776563589863,
      "grad_norm": 27.873672485351562,
      "learning_rate": 1.422614895760676e-05,
      "loss": 4.2532,
      "step": 7450
    },
    {
      "epoch": 0.8672401767030923,
      "grad_norm": 34.4297981262207,
      "learning_rate": 1.4218398821979387e-05,
      "loss": 2.9676,
      "step": 7460
    },
    {
      "epoch": 0.8684026970471983,
      "grad_norm": 22.903671264648438,
      "learning_rate": 1.4210648686352014e-05,
      "loss": 2.5589,
      "step": 7470
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 25.64692497253418,
      "learning_rate": 1.420289855072464e-05,
      "loss": 2.7356,
      "step": 7480
    },
    {
      "epoch": 0.8707277377354103,
      "grad_norm": 25.57411766052246,
      "learning_rate": 1.4195148415097266e-05,
      "loss": 2.8913,
      "step": 7490
    },
    {
      "epoch": 0.8718902580795164,
      "grad_norm": 28.15791130065918,
      "learning_rate": 1.4187398279469893e-05,
      "loss": 2.7606,
      "step": 7500
    },
    {
      "epoch": 0.8730527784236224,
      "grad_norm": 26.823192596435547,
      "learning_rate": 1.4179648143842519e-05,
      "loss": 2.6047,
      "step": 7510
    },
    {
      "epoch": 0.8742152987677284,
      "grad_norm": 33.94122314453125,
      "learning_rate": 1.4171898008215146e-05,
      "loss": 2.8119,
      "step": 7520
    },
    {
      "epoch": 0.8753778191118344,
      "grad_norm": 32.43562316894531,
      "learning_rate": 1.4164147872587773e-05,
      "loss": 2.4844,
      "step": 7530
    },
    {
      "epoch": 0.8765403394559405,
      "grad_norm": 25.262388229370117,
      "learning_rate": 1.4156397736960398e-05,
      "loss": 2.5475,
      "step": 7540
    },
    {
      "epoch": 0.8777028598000465,
      "grad_norm": 33.06882858276367,
      "learning_rate": 1.4148647601333025e-05,
      "loss": 2.9467,
      "step": 7550
    },
    {
      "epoch": 0.8788653801441525,
      "grad_norm": 35.818641662597656,
      "learning_rate": 1.4140897465705652e-05,
      "loss": 3.6613,
      "step": 7560
    },
    {
      "epoch": 0.8800279004882585,
      "grad_norm": 33.18102264404297,
      "learning_rate": 1.413314733007828e-05,
      "loss": 3.0206,
      "step": 7570
    },
    {
      "epoch": 0.8811904208323645,
      "grad_norm": 20.36775779724121,
      "learning_rate": 1.4125397194450905e-05,
      "loss": 2.4621,
      "step": 7580
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 31.066091537475586,
      "learning_rate": 1.4117647058823532e-05,
      "loss": 3.1582,
      "step": 7590
    },
    {
      "epoch": 0.8835154615205766,
      "grad_norm": 33.263336181640625,
      "learning_rate": 1.4109896923196159e-05,
      "loss": 2.5985,
      "step": 7600
    },
    {
      "epoch": 0.8846779818646826,
      "grad_norm": 24.390430450439453,
      "learning_rate": 1.4102146787568784e-05,
      "loss": 3.7621,
      "step": 7610
    },
    {
      "epoch": 0.8858405022087886,
      "grad_norm": 27.49161148071289,
      "learning_rate": 1.4094396651941411e-05,
      "loss": 3.0389,
      "step": 7620
    },
    {
      "epoch": 0.8870030225528946,
      "grad_norm": 25.8594913482666,
      "learning_rate": 1.4086646516314038e-05,
      "loss": 2.3095,
      "step": 7630
    },
    {
      "epoch": 0.8881655428970007,
      "grad_norm": 32.709651947021484,
      "learning_rate": 1.4078896380686663e-05,
      "loss": 3.7301,
      "step": 7640
    },
    {
      "epoch": 0.8893280632411067,
      "grad_norm": 23.587480545043945,
      "learning_rate": 1.407114624505929e-05,
      "loss": 3.0131,
      "step": 7650
    },
    {
      "epoch": 0.8904905835852127,
      "grad_norm": 25.42110824584961,
      "learning_rate": 1.4063396109431917e-05,
      "loss": 3.0958,
      "step": 7660
    },
    {
      "epoch": 0.8916531039293187,
      "grad_norm": 25.091732025146484,
      "learning_rate": 1.4055645973804544e-05,
      "loss": 3.4591,
      "step": 7670
    },
    {
      "epoch": 0.8928156242734248,
      "grad_norm": 24.59096336364746,
      "learning_rate": 1.404789583817717e-05,
      "loss": 2.6556,
      "step": 7680
    },
    {
      "epoch": 0.8939781446175308,
      "grad_norm": 32.17816162109375,
      "learning_rate": 1.4040145702549797e-05,
      "loss": 2.6974,
      "step": 7690
    },
    {
      "epoch": 0.8951406649616368,
      "grad_norm": 24.190834045410156,
      "learning_rate": 1.4032395566922424e-05,
      "loss": 3.2307,
      "step": 7700
    },
    {
      "epoch": 0.8963031853057428,
      "grad_norm": 23.047775268554688,
      "learning_rate": 1.402464543129505e-05,
      "loss": 2.5169,
      "step": 7710
    },
    {
      "epoch": 0.8974657056498488,
      "grad_norm": 32.50370788574219,
      "learning_rate": 1.4016895295667676e-05,
      "loss": 3.1256,
      "step": 7720
    },
    {
      "epoch": 0.8986282259939549,
      "grad_norm": 25.82661247253418,
      "learning_rate": 1.4009145160040303e-05,
      "loss": 2.3681,
      "step": 7730
    },
    {
      "epoch": 0.8997907463380609,
      "grad_norm": 22.048582077026367,
      "learning_rate": 1.4001395024412929e-05,
      "loss": 4.14,
      "step": 7740
    },
    {
      "epoch": 0.9009532666821669,
      "grad_norm": 24.170801162719727,
      "learning_rate": 1.3993644888785556e-05,
      "loss": 2.7844,
      "step": 7750
    },
    {
      "epoch": 0.9021157870262729,
      "grad_norm": 26.46138572692871,
      "learning_rate": 1.3985894753158183e-05,
      "loss": 2.9474,
      "step": 7760
    },
    {
      "epoch": 0.903278307370379,
      "grad_norm": 28.99726104736328,
      "learning_rate": 1.3978144617530808e-05,
      "loss": 2.1019,
      "step": 7770
    },
    {
      "epoch": 0.904440827714485,
      "grad_norm": 18.87712860107422,
      "learning_rate": 1.3970394481903435e-05,
      "loss": 2.6192,
      "step": 7780
    },
    {
      "epoch": 0.905603348058591,
      "grad_norm": 27.025426864624023,
      "learning_rate": 1.3962644346276062e-05,
      "loss": 3.7374,
      "step": 7790
    },
    {
      "epoch": 0.906765868402697,
      "grad_norm": 26.697750091552734,
      "learning_rate": 1.3954894210648689e-05,
      "loss": 2.986,
      "step": 7800
    },
    {
      "epoch": 0.907928388746803,
      "grad_norm": 36.64234161376953,
      "learning_rate": 1.3947144075021314e-05,
      "loss": 2.3042,
      "step": 7810
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 29.23582649230957,
      "learning_rate": 1.3939393939393942e-05,
      "loss": 2.9859,
      "step": 7820
    },
    {
      "epoch": 0.9102534294350151,
      "grad_norm": 34.062320709228516,
      "learning_rate": 1.3931643803766569e-05,
      "loss": 2.8004,
      "step": 7830
    },
    {
      "epoch": 0.9114159497791211,
      "grad_norm": 28.30223846435547,
      "learning_rate": 1.3923893668139194e-05,
      "loss": 2.6396,
      "step": 7840
    },
    {
      "epoch": 0.9125784701232271,
      "grad_norm": 27.239948272705078,
      "learning_rate": 1.3916143532511821e-05,
      "loss": 3.5945,
      "step": 7850
    },
    {
      "epoch": 0.9137409904673331,
      "grad_norm": 24.937536239624023,
      "learning_rate": 1.3908393396884448e-05,
      "loss": 2.7686,
      "step": 7860
    },
    {
      "epoch": 0.9149035108114392,
      "grad_norm": 25.455400466918945,
      "learning_rate": 1.3900643261257073e-05,
      "loss": 3.2267,
      "step": 7870
    },
    {
      "epoch": 0.9160660311555452,
      "grad_norm": 21.33922576904297,
      "learning_rate": 1.38928931256297e-05,
      "loss": 2.92,
      "step": 7880
    },
    {
      "epoch": 0.9172285514996512,
      "grad_norm": 30.271474838256836,
      "learning_rate": 1.3885142990002327e-05,
      "loss": 3.1182,
      "step": 7890
    },
    {
      "epoch": 0.9183910718437572,
      "grad_norm": 21.375865936279297,
      "learning_rate": 1.3877392854374954e-05,
      "loss": 2.7369,
      "step": 7900
    },
    {
      "epoch": 0.9195535921878633,
      "grad_norm": 22.38078498840332,
      "learning_rate": 1.386964271874758e-05,
      "loss": 2.3847,
      "step": 7910
    },
    {
      "epoch": 0.9207161125319693,
      "grad_norm": 27.547632217407227,
      "learning_rate": 1.3861892583120207e-05,
      "loss": 3.2778,
      "step": 7920
    },
    {
      "epoch": 0.9218786328760753,
      "grad_norm": 25.60124969482422,
      "learning_rate": 1.3854142447492834e-05,
      "loss": 2.5144,
      "step": 7930
    },
    {
      "epoch": 0.9230411532201813,
      "grad_norm": 24.880901336669922,
      "learning_rate": 1.384639231186546e-05,
      "loss": 3.2703,
      "step": 7940
    },
    {
      "epoch": 0.9242036735642873,
      "grad_norm": 25.280126571655273,
      "learning_rate": 1.3838642176238086e-05,
      "loss": 2.7653,
      "step": 7950
    },
    {
      "epoch": 0.9253661939083934,
      "grad_norm": 30.023677825927734,
      "learning_rate": 1.3830892040610713e-05,
      "loss": 3.9409,
      "step": 7960
    },
    {
      "epoch": 0.9265287142524994,
      "grad_norm": 20.729875564575195,
      "learning_rate": 1.3823141904983339e-05,
      "loss": 4.0005,
      "step": 7970
    },
    {
      "epoch": 0.9276912345966054,
      "grad_norm": 27.522428512573242,
      "learning_rate": 1.3815391769355966e-05,
      "loss": 2.4955,
      "step": 7980
    },
    {
      "epoch": 0.9288537549407114,
      "grad_norm": 26.98936653137207,
      "learning_rate": 1.3807641633728593e-05,
      "loss": 2.7413,
      "step": 7990
    },
    {
      "epoch": 0.9300162752848175,
      "grad_norm": 34.3154296875,
      "learning_rate": 1.379989149810122e-05,
      "loss": 2.991,
      "step": 8000
    },
    {
      "epoch": 0.9311787956289235,
      "grad_norm": 39.309043884277344,
      "learning_rate": 1.3792141362473845e-05,
      "loss": 3.3729,
      "step": 8010
    },
    {
      "epoch": 0.9323413159730295,
      "grad_norm": 25.951030731201172,
      "learning_rate": 1.3784391226846472e-05,
      "loss": 2.8318,
      "step": 8020
    },
    {
      "epoch": 0.9335038363171355,
      "grad_norm": 33.90656661987305,
      "learning_rate": 1.3776641091219099e-05,
      "loss": 3.2926,
      "step": 8030
    },
    {
      "epoch": 0.9346663566612415,
      "grad_norm": 47.04759979248047,
      "learning_rate": 1.3768890955591724e-05,
      "loss": 2.9309,
      "step": 8040
    },
    {
      "epoch": 0.9358288770053476,
      "grad_norm": 31.646894454956055,
      "learning_rate": 1.3761140819964351e-05,
      "loss": 2.6199,
      "step": 8050
    },
    {
      "epoch": 0.9369913973494536,
      "grad_norm": 19.131488800048828,
      "learning_rate": 1.3753390684336978e-05,
      "loss": 2.9856,
      "step": 8060
    },
    {
      "epoch": 0.9381539176935596,
      "grad_norm": 50.842960357666016,
      "learning_rate": 1.3745640548709604e-05,
      "loss": 3.9612,
      "step": 8070
    },
    {
      "epoch": 0.9393164380376656,
      "grad_norm": 42.27538299560547,
      "learning_rate": 1.3737890413082231e-05,
      "loss": 2.2464,
      "step": 8080
    },
    {
      "epoch": 0.9404789583817716,
      "grad_norm": 23.756858825683594,
      "learning_rate": 1.3730140277454858e-05,
      "loss": 2.8632,
      "step": 8090
    },
    {
      "epoch": 0.9416414787258777,
      "grad_norm": 32.81756591796875,
      "learning_rate": 1.3722390141827483e-05,
      "loss": 2.6623,
      "step": 8100
    },
    {
      "epoch": 0.9428039990699837,
      "grad_norm": 19.920629501342773,
      "learning_rate": 1.371464000620011e-05,
      "loss": 3.4111,
      "step": 8110
    },
    {
      "epoch": 0.9439665194140897,
      "grad_norm": 44.18490219116211,
      "learning_rate": 1.3706889870572737e-05,
      "loss": 2.9503,
      "step": 8120
    },
    {
      "epoch": 0.9451290397581957,
      "grad_norm": 24.91351890563965,
      "learning_rate": 1.3699139734945364e-05,
      "loss": 2.8947,
      "step": 8130
    },
    {
      "epoch": 0.9462915601023018,
      "grad_norm": 24.193294525146484,
      "learning_rate": 1.369138959931799e-05,
      "loss": 2.5262,
      "step": 8140
    },
    {
      "epoch": 0.9474540804464078,
      "grad_norm": 39.412593841552734,
      "learning_rate": 1.3683639463690617e-05,
      "loss": 3.5507,
      "step": 8150
    },
    {
      "epoch": 0.9486166007905138,
      "grad_norm": 25.366453170776367,
      "learning_rate": 1.3675889328063244e-05,
      "loss": 4.3157,
      "step": 8160
    },
    {
      "epoch": 0.9497791211346198,
      "grad_norm": 26.331989288330078,
      "learning_rate": 1.3668139192435869e-05,
      "loss": 2.638,
      "step": 8170
    },
    {
      "epoch": 0.9509416414787258,
      "grad_norm": 28.84100341796875,
      "learning_rate": 1.3660389056808496e-05,
      "loss": 2.5568,
      "step": 8180
    },
    {
      "epoch": 0.9521041618228319,
      "grad_norm": 37.887454986572266,
      "learning_rate": 1.3652638921181123e-05,
      "loss": 3.2179,
      "step": 8190
    },
    {
      "epoch": 0.9532666821669379,
      "grad_norm": 28.820066452026367,
      "learning_rate": 1.3644888785553749e-05,
      "loss": 2.549,
      "step": 8200
    },
    {
      "epoch": 0.9544292025110439,
      "grad_norm": 29.41775131225586,
      "learning_rate": 1.3637138649926376e-05,
      "loss": 3.2437,
      "step": 8210
    },
    {
      "epoch": 0.9555917228551499,
      "grad_norm": 32.771480560302734,
      "learning_rate": 1.3629388514299003e-05,
      "loss": 2.8943,
      "step": 8220
    },
    {
      "epoch": 0.956754243199256,
      "grad_norm": 25.053863525390625,
      "learning_rate": 1.362163837867163e-05,
      "loss": 2.5377,
      "step": 8230
    },
    {
      "epoch": 0.957916763543362,
      "grad_norm": 21.455299377441406,
      "learning_rate": 1.3613888243044255e-05,
      "loss": 2.4743,
      "step": 8240
    },
    {
      "epoch": 0.959079283887468,
      "grad_norm": 47.879783630371094,
      "learning_rate": 1.3606138107416882e-05,
      "loss": 2.839,
      "step": 8250
    },
    {
      "epoch": 0.960241804231574,
      "grad_norm": 31.06669807434082,
      "learning_rate": 1.3598387971789509e-05,
      "loss": 2.7703,
      "step": 8260
    },
    {
      "epoch": 0.96140432457568,
      "grad_norm": 38.150020599365234,
      "learning_rate": 1.3590637836162134e-05,
      "loss": 3.0275,
      "step": 8270
    },
    {
      "epoch": 0.9625668449197861,
      "grad_norm": 42.2947883605957,
      "learning_rate": 1.3582887700534761e-05,
      "loss": 2.4331,
      "step": 8280
    },
    {
      "epoch": 0.9637293652638921,
      "grad_norm": 28.12532615661621,
      "learning_rate": 1.3575137564907388e-05,
      "loss": 3.722,
      "step": 8290
    },
    {
      "epoch": 0.9648918856079981,
      "grad_norm": 35.62350082397461,
      "learning_rate": 1.3567387429280014e-05,
      "loss": 2.9265,
      "step": 8300
    },
    {
      "epoch": 0.9660544059521041,
      "grad_norm": 22.609760284423828,
      "learning_rate": 1.355963729365264e-05,
      "loss": 2.5004,
      "step": 8310
    },
    {
      "epoch": 0.9672169262962101,
      "grad_norm": 25.284759521484375,
      "learning_rate": 1.3551887158025268e-05,
      "loss": 2.5482,
      "step": 8320
    },
    {
      "epoch": 0.9683794466403162,
      "grad_norm": 45.33049392700195,
      "learning_rate": 1.3544137022397895e-05,
      "loss": 3.4087,
      "step": 8330
    },
    {
      "epoch": 0.9695419669844222,
      "grad_norm": 17.178421020507812,
      "learning_rate": 1.353638688677052e-05,
      "loss": 3.0823,
      "step": 8340
    },
    {
      "epoch": 0.9707044873285282,
      "grad_norm": 27.325939178466797,
      "learning_rate": 1.3528636751143147e-05,
      "loss": 2.8896,
      "step": 8350
    },
    {
      "epoch": 0.9718670076726342,
      "grad_norm": 37.417049407958984,
      "learning_rate": 1.3520886615515774e-05,
      "loss": 3.2864,
      "step": 8360
    },
    {
      "epoch": 0.9730295280167403,
      "grad_norm": 30.28375816345215,
      "learning_rate": 1.35131364798884e-05,
      "loss": 4.0735,
      "step": 8370
    },
    {
      "epoch": 0.9741920483608463,
      "grad_norm": 22.61736297607422,
      "learning_rate": 1.3505386344261027e-05,
      "loss": 4.0953,
      "step": 8380
    },
    {
      "epoch": 0.9753545687049523,
      "grad_norm": 22.470352172851562,
      "learning_rate": 1.3497636208633654e-05,
      "loss": 3.178,
      "step": 8390
    },
    {
      "epoch": 0.9765170890490583,
      "grad_norm": 35.22957992553711,
      "learning_rate": 1.3489886073006279e-05,
      "loss": 3.4923,
      "step": 8400
    },
    {
      "epoch": 0.9776796093931643,
      "grad_norm": 28.314144134521484,
      "learning_rate": 1.3482135937378906e-05,
      "loss": 3.0393,
      "step": 8410
    },
    {
      "epoch": 0.9788421297372704,
      "grad_norm": 30.66291046142578,
      "learning_rate": 1.3474385801751533e-05,
      "loss": 2.904,
      "step": 8420
    },
    {
      "epoch": 0.9800046500813764,
      "grad_norm": 38.23775100708008,
      "learning_rate": 1.3466635666124158e-05,
      "loss": 3.1201,
      "step": 8430
    },
    {
      "epoch": 0.9811671704254824,
      "grad_norm": 22.382125854492188,
      "learning_rate": 1.3458885530496785e-05,
      "loss": 2.7027,
      "step": 8440
    },
    {
      "epoch": 0.9823296907695884,
      "grad_norm": 28.457887649536133,
      "learning_rate": 1.3451135394869413e-05,
      "loss": 3.2483,
      "step": 8450
    },
    {
      "epoch": 0.9834922111136944,
      "grad_norm": 40.02101135253906,
      "learning_rate": 1.344338525924204e-05,
      "loss": 3.0622,
      "step": 8460
    },
    {
      "epoch": 0.9846547314578005,
      "grad_norm": 35.82646942138672,
      "learning_rate": 1.3435635123614665e-05,
      "loss": 2.6114,
      "step": 8470
    },
    {
      "epoch": 0.9858172518019065,
      "grad_norm": 29.827350616455078,
      "learning_rate": 1.3427884987987292e-05,
      "loss": 3.4455,
      "step": 8480
    },
    {
      "epoch": 0.9869797721460125,
      "grad_norm": 36.101016998291016,
      "learning_rate": 1.3420134852359919e-05,
      "loss": 2.6126,
      "step": 8490
    },
    {
      "epoch": 0.9881422924901185,
      "grad_norm": 35.70433807373047,
      "learning_rate": 1.3412384716732544e-05,
      "loss": 2.7459,
      "step": 8500
    },
    {
      "epoch": 0.9893048128342246,
      "grad_norm": 39.146995544433594,
      "learning_rate": 1.3404634581105171e-05,
      "loss": 3.9402,
      "step": 8510
    },
    {
      "epoch": 0.9904673331783306,
      "grad_norm": 21.550867080688477,
      "learning_rate": 1.3396884445477798e-05,
      "loss": 3.6804,
      "step": 8520
    },
    {
      "epoch": 0.9916298535224366,
      "grad_norm": 30.354074478149414,
      "learning_rate": 1.3389134309850424e-05,
      "loss": 4.0561,
      "step": 8530
    },
    {
      "epoch": 0.9927923738665426,
      "grad_norm": 30.636455535888672,
      "learning_rate": 1.338138417422305e-05,
      "loss": 2.96,
      "step": 8540
    },
    {
      "epoch": 0.9939548942106486,
      "grad_norm": 25.697458267211914,
      "learning_rate": 1.3373634038595678e-05,
      "loss": 2.7655,
      "step": 8550
    },
    {
      "epoch": 0.9951174145547547,
      "grad_norm": 36.8210334777832,
      "learning_rate": 1.3365883902968305e-05,
      "loss": 2.7467,
      "step": 8560
    },
    {
      "epoch": 0.9962799348988607,
      "grad_norm": 29.86701202392578,
      "learning_rate": 1.335813376734093e-05,
      "loss": 2.5352,
      "step": 8570
    },
    {
      "epoch": 0.9974424552429667,
      "grad_norm": 40.02394485473633,
      "learning_rate": 1.3350383631713557e-05,
      "loss": 2.6225,
      "step": 8580
    },
    {
      "epoch": 0.9986049755870727,
      "grad_norm": 25.99468994140625,
      "learning_rate": 1.3342633496086184e-05,
      "loss": 2.5065,
      "step": 8590
    },
    {
      "epoch": 0.9997674959311788,
      "grad_norm": 25.365798950195312,
      "learning_rate": 1.333488336045881e-05,
      "loss": 3.0455,
      "step": 8600
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.1433054357767105,
      "eval_loss": 2.9849588871002197,
      "eval_runtime": 1548.8324,
      "eval_samples_per_second": 0.617,
      "eval_steps_per_second": 0.617,
      "step": 8602
    }
  ],
  "logging_steps": 10,
  "max_steps": 25806,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2260582055018496.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
