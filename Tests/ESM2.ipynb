{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Exemple de séquences de protéines et leurs labels correspondants pour l'entraînement\n",
    "train_sequences = [\n",
    "    \"MALWMRLLPLLALLALWGPGPGLSGLALLLAVAP\",  # Mitochondrion\n",
    "    \"MGLSDGEWQLVLNVWGKVEADIPGHGQEVLIRLFK\",  # Cytoplasm\n",
    "    # Ajouter d'autres séquences et leurs labels\n",
    "]\n",
    "train_labels = [\n",
    "    [1, 0],  # LABEL_0, par exemple Mitochondrion\n",
    "    [0, 1],  # LABEL_1, par exemple Cytoplasm\n",
    "    # Ajouter d'autres labels correspondant aux séquences, en one-hot encoding\n",
    "]\n",
    "\n",
    "# Exemple de séquences de protéines et leurs labels correspondants pour l'évaluation\n",
    "eval_sequences = [\n",
    "    \"MLAKKKPQKPLLPLTPEELPAELTDLT\",  # Mitochondrion\n",
    "    \"MDDIAALVVDNGSGMCKAGFAGDDAPR\",  # Cytoplasm\n",
    "    # Ajouter d'autres séquences et leurs labels\n",
    "]\n",
    "eval_labels = [\n",
    "    [1, 0],  # LABEL_0, par exemple Mitochondrion\n",
    "    [0, 1],  # LABEL_1, par exemple Cytoplasm\n",
    "    # Ajouter d'autres labels correspondant aux séquences, en one-hot encoding\n",
    "]\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        item = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.float)  # Convertir les labels en type Float\n",
    "        return item\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "train_dataset = ProteinDataset(train_sequences, train_labels, tokenizer)\n",
    "eval_dataset = ProteinDataset(eval_sequences, eval_labels, tokenizer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([item['input_ids'] for item in batch], batch_first=True)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence([item['attention_mask'] for item in batch], batch_first=True)\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "                                     \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "  0%|          | 0/3 [02:01<?, ?it/s]        \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6912202835083008, 'eval_runtime': 0.041, 'eval_samples_per_second': 48.812, 'eval_steps_per_second': 24.406, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                     \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "  0%|          | 0/3 [02:02<?, ?it/s]        \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6912007331848145, 'eval_runtime': 0.029, 'eval_samples_per_second': 68.873, 'eval_steps_per_second': 34.436, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                     \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "  0%|          | 0/3 [02:02<?, ?it/s]        \n",
      "\u001b[A\n",
      "                                     \n",
      "100%|██████████| 3/3 [00:00<00:00,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6911584734916687, 'eval_runtime': 0.026, 'eval_samples_per_second': 76.939, 'eval_steps_per_second': 38.469, 'epoch': 3.0}\n",
      "{'train_runtime': 0.6543, 'train_samples_per_second': 9.171, 'train_steps_per_second': 4.585, 'train_loss': 0.6931606928507487, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.6931606928507487, metrics={'train_runtime': 0.6543, 'train_samples_per_second': 9.171, 'train_steps_per_second': 4.585, 'total_flos': 9992508156.0, 'train_loss': 0.6931606928507487, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EsmForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2, problem_type=\"multi_label_classification\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Fournir le dataset d'évaluation\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[0.0322, 0.0120]])\n",
      "Predicted Class IDs: tensor([0, 1])\n",
      "Predicted Labels: ['LABEL_0', 'LABEL_1']\n"
     ]
    }
   ],
   "source": [
    "# Exemple de séquence de protéine pour les tests\n",
    "test_sequence = \"MALWMRLLPLLALLALWGPGPGLSGLALLLAVAP\"\n",
    "\n",
    "# Tokeniser la séquence de protéine\n",
    "inputs = tokenizer(test_sequence, return_tensors=\"pt\")\n",
    "\n",
    "# Effectuer une prédiction sans gradients\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Calculer les classes prédites\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Predicted Class IDs:\", predicted_class_ids)\n",
    "\n",
    "# Si vous voulez les labels correspondants\n",
    "labels = [model.config.id2label[class_id.item()] for class_id in predicted_class_ids]\n",
    "print(\"Predicted Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to Label mapping: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "Label to ID mapping: {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "EsmConfig {\n",
      "  \"_name_or_path\": \"facebook/esm2_t6_8M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"EsmForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 320,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1280,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"esm\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.43.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Charger la configuration du modèle\n",
    "config = AutoConfig.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "# Vérifier les mappings des labels\n",
    "id2label = config.id2label\n",
    "label2id = config.label2id\n",
    "\n",
    "print(\"ID to Label mapping:\", id2label)\n",
    "print(\"Label to ID mapping:\", label2id)\n",
    "\n",
    "# Afficher les détails de la configuration\n",
    "print(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
