{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03631b2c-7816-4e1e-9b6e-e5b74e353abe",
   "metadata": {},
   "source": [
    "Install Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cd99d5-ee0c-4c2e-b178-abdf4b3fb1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\anaconda\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\anaconda\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\anaconda\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\anaconda\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\anaconda\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in d:\\anaconda\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in d:\\anaconda\\lib\\site-packages (from datasets) (0.23.5)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (4.42.4)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\anaconda\\lib\\site-packages (from transformers) (0.23.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\anaconda\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\anaconda\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: tokenizers in d:\\anaconda\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\anaconda\\lib\\site-packages (from tokenizers) (0.23.5)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.1-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Downloading tbb-2021.13.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.3.1-cp312-cp312-win_amd64.whl (159.7 MB)\n",
      "   ---------------------------------------- 0.0/159.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/159.7 MB 4.1 MB/s eta 0:00:39\n",
      "   ---------------------------------------- 0.6/159.7 MB 6.0 MB/s eta 0:00:27\n",
      "   ---------------------------------------- 1.0/159.7 MB 7.3 MB/s eta 0:00:22\n",
      "   ---------------------------------------- 1.3/159.7 MB 6.7 MB/s eta 0:00:24\n",
      "   ---------------------------------------- 1.9/159.7 MB 8.2 MB/s eta 0:00:20\n",
      "    --------------------------------------- 2.7/159.7 MB 9.5 MB/s eta 0:00:17\n",
      "    --------------------------------------- 3.6/159.7 MB 11.0 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 4.5/159.7 MB 11.9 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 5.6/159.7 MB 13.3 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 6.7/159.7 MB 14.3 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 7.5/159.7 MB 15.0 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 8.3/159.7 MB 14.8 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 9.2/159.7 MB 15.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 10.5/159.7 MB 17.3 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 11.5/159.7 MB 20.5 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 12.7/159.7 MB 21.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 13.7/159.7 MB 21.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 15.1/159.7 MB 23.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 16.5/159.7 MB 24.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 18.1/159.7 MB 26.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 19.6/159.7 MB 28.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 21.2/159.7 MB 29.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 22.4/159.7 MB 31.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 24.1/159.7 MB 32.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 25.5/159.7 MB 32.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 27.4/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 29.1/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 30.7/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 32.2/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 34.2/159.7 MB 36.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 35.6/159.7 MB 36.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 37.4/159.7 MB 36.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 38.8/159.7 MB 36.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 40.6/159.7 MB 36.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 42.0/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 43.8/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 45.5/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 46.9/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 48.4/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 50.6/159.7 MB 36.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 51.8/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 53.4/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 54.8/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 56.3/159.7 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 58.1/159.7 MB 34.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 59.7/159.7 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 61.5/159.7 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 63.2/159.7 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 64.7/159.7 MB 36.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 65.9/159.7 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 67.1/159.7 MB 32.8 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 68.1/159.7 MB 31.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 69.4/159.7 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 70.8/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 71.8/159.7 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 73.2/159.7 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 74.4/159.7 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 75.1/159.7 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 76.2/159.7 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 77.4/159.7 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 78.7/159.7 MB 26.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 80.0/159.7 MB 25.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 81.0/159.7 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 82.3/159.7 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 83.4/159.7 MB 24.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 84.6/159.7 MB 24.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 85.8/159.7 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 87.4/159.7 MB 26.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 88.9/159.7 MB 28.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 90.2/159.7 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 92.0/159.7 MB 28.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 93.6/159.7 MB 31.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 95.0/159.7 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 96.7/159.7 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 98.3/159.7 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 99.9/159.7 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 101.1/159.7 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 102.5/159.7 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 103.6/159.7 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 104.8/159.7 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 105.8/159.7 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 107.0/159.7 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 108.3/159.7 MB 27.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 109.1/159.7 MB 25.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 110.1/159.7 MB 25.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 111.2/159.7 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 112.4/159.7 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 113.0/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 114.3/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 115.5/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 116.8/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 118.1/159.7 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 119.4/159.7 MB 24.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 120.7/159.7 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 122.0/159.7 MB 25.1 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 123.5/159.7 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 124.8/159.7 MB 28.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 126.1/159.7 MB 28.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 127.3/159.7 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 128.7/159.7 MB 28.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 129.9/159.7 MB 28.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 131.3/159.7 MB 29.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 132.3/159.7 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 133.2/159.7 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 134.2/159.7 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 135.1/159.7 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 136.5/159.7 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 137.7/159.7 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 139.1/159.7 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 140.6/159.7 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 141.5/159.7 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 142.7/159.7 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 143.9/159.7 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.2/159.7 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.6/159.7 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.9/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 147.1/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 148.4/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 149.6/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 151.1/159.7 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 152.4/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 153.6/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 155.0/159.7 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  156.4/159.7 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  157.6/159.7 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  158.9/159.7 MB 28.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.7/159.7 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.7/159.7 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/228.5 MB 28.3 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 2.6/228.5 MB 33.6 MB/s eta 0:00:07\n",
      "    --------------------------------------- 4.2/228.5 MB 29.8 MB/s eta 0:00:08\n",
      "    --------------------------------------- 5.5/228.5 MB 29.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 7.2/228.5 MB 30.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 8.5/228.5 MB 30.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 10.0/228.5 MB 30.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.5/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 12.9/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 14.4/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 15.7/228.5 MB 32.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 17.2/228.5 MB 29.7 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 18.5/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 19.7/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 21.2/228.5 MB 31.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 22.7/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 24.2/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 25.8/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 27.2/228.5 MB 29.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 28.7/228.5 MB 29.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 30.2/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 31.6/228.5 MB 32.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 32.9/228.5 MB 32.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 34.4/228.5 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 36.1/228.5 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 37.5/228.5 MB 32.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 38.8/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 40.6/228.5 MB 32.7 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 41.1/228.5 MB 31.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 41.9/228.5 MB 28.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 42.9/228.5 MB 27.3 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 43.9/228.5 MB 26.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 45.2/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 46.4/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 47.9/228.5 MB 25.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 48.9/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 50.1/228.5 MB 23.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 51.0/228.5 MB 22.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 52.3/228.5 MB 25.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 52.9/228.5 MB 23.4 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 54.0/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 55.2/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 56.6/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 57.7/228.5 MB 24.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 59.2/228.5 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 60.3/228.5 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 61.7/228.5 MB 25.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 62.9/228.5 MB 28.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 64.3/228.5 MB 27.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 65.5/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 66.7/228.5 MB 28.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 68.1/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 69.4/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 70.5/228.5 MB 26.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 71.9/228.5 MB 28.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 73.2/228.5 MB 28.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 74.5/228.5 MB 28.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 75.7/228.5 MB 28.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 77.1/228.5 MB 28.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 78.4/228.5 MB 28.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 79.4/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 80.6/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 82.1/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 83.2/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 84.6/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 85.8/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 87.2/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 88.4/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 89.5/228.5 MB 26.2 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 90.9/228.5 MB 28.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 92.1/228.5 MB 27.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 93.6/228.5 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 94.8/228.5 MB 28.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 96.2/228.5 MB 28.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 98.1/228.5 MB 29.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 99.2/228.5 MB 28.5 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 100.5/228.5 MB 29.7 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 101.9/228.5 MB 29.7 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 102.9/228.5 MB 28.5 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 104.3/228.5 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 105.8/228.5 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 107.0/228.5 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 108.5/228.5 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 109.8/228.5 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 111.3/228.5 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 112.9/228.5 MB 31.2 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 114.3/228.5 MB 31.2 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 115.5/228.5 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 117.1/228.5 MB 29.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 118.7/228.5 MB 31.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 120.2/228.5 MB 32.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 121.7/228.5 MB 32.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 123.0/228.5 MB 31.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 124.5/228.5 MB 31.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 126.0/228.5 MB 32.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 127.0/228.5 MB 29.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 128.3/228.5 MB 29.7 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 129.6/228.5 MB 28.4 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 130.6/228.5 MB 27.3 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 131.8/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 133.2/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 134.6/228.5 MB 28.5 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 136.0/228.5 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 137.3/228.5 MB 27.3 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 139.1/228.5 MB 28.4 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 140.2/228.5 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 141.6/228.5 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 143.2/228.5 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 144.8/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 146.2/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 147.8/228.5 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 149.4/228.5 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 150.9/228.5 MB 32.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 152.4/228.5 MB 34.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 154.1/228.5 MB 32.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 155.8/228.5 MB 34.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 157.2/228.5 MB 32.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 158.6/228.5 MB 32.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 160.1/228.5 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 161.1/228.5 MB 32.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 162.4/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 163.7/228.5 MB 31.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 165.1/228.5 MB 28.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 166.7/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 168.1/228.5 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 169.6/228.5 MB 29.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 171.2/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 172.8/228.5 MB 32.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 173.5/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 175.1/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 176.6/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 177.9/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 179.4/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 181.0/228.5 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 182.5/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 184.0/228.5 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 185.5/228.5 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 187.0/228.5 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 188.5/228.5 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 190.1/228.5 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 191.3/228.5 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 192.6/228.5 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 193.6/228.5 MB 29.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 195.0/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 196.3/228.5 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 198.1/228.5 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 199.6/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 201.0/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 202.6/228.5 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 204.1/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 205.5/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 206.4/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 208.1/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 209.6/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 211.2/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 212.7/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 214.6/228.5 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 216.0/228.5 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 217.5/228.5 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 219.3/228.5 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 220.9/228.5 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 221.9/228.5 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  223.6/228.5 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  224.5/228.5 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  225.6/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  226.8/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  226.8/228.5 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.2/228.5 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 24.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.1/3.5 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.5/3.5 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 24.9 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.13.0-py3-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.9/286.9 kB 17.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tbb, intel-openmp, mkl, torch\n",
      "Successfully installed intel-openmp-2021.4.0 mkl-2021.4.0 tbb-2021.13.0 torch-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install tokenizers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77fa65-3cfa-4e23-801f-c7a3821c1800",
   "metadata": {},
   "source": [
    "*import* libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "646138c1-fa3e-410d-beb9-9dfbe5830666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94033cc6-0013-482b-af6d-6baf0c6e5368",
   "metadata": {},
   "source": [
    "Picking and DL a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25e929ad-f72e-49ab-8e3a-035cdc7a43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and prepare cc_news dataset\n",
    "dataset = load_dataset(\"cc_news\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e3558-f5aa-44e4-8c99-143a2934461c",
   "metadata": {},
   "source": [
    "Splitting training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3d64dd9-34c0-4190-bd29-ed6eb0e295d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 637416\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 70825\n",
       " }))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset into training (90%) and testing (10%)\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "d[\"train\"], d[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e60e1706-d64c-4a15-9d5f-29f88da7fc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 637416\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 70825\n",
       " }))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset into training (90%) and testing (10%)\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "d[\"train\"], d[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520908b-243e-4297-9565-8b63bb9abe10",
   "metadata": {},
   "source": [
    "Training the Tokenizer\n",
    "save the dataset object as text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "446cf292-a13f-4333-ad09-a57420982c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    \"\"\"Utility function to save dataset text to disk,\n",
    "    useful for using the texts to train the tokenizer\n",
    "    (as the tokenizer accepts files)\"\"\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)\n",
    "\n",
    "# Assuming d is your dataset dictionary\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(d[\"train\"], \"train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(d[\"test\"], \"test.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2090229-6aa2-431c-b4ca-a770c8f255e4",
   "metadata": {},
   "source": [
    "defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "376197f8-cf49-4a0a-af34-4cb13883ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eecde2-f25b-475c-96de-3ef71b5f9fa4",
   "metadata": {},
   "source": [
    "Train the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b340ea5-27d8-4252-a009-56514efdd4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train the tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11a958-d1c0-4b80-b94c-548f2395e588",
   "metadata": {},
   "source": [
    "Saving the trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "448ca7b0-edce-43cb-91d8-586e8eb6b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pretrained-bert\"\n",
    "\n",
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "  os.mkdir(model_path)\n",
    "# save the tokenizer\n",
    "tokenizer.save_model(model_path)\n",
    "# dumping some of the tokenizer config to config file,\n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "  tokenizer_cfg = {\n",
    "      \"do_lower_case\": True,\n",
    "      \"unk_token\": \"[UNK]\",\n",
    "      \"sep_token\": \"[SEP]\",\n",
    "      \"pad_token\": \"[PAD]\",\n",
    "      \"cls_token\": \"[CLS]\",\n",
    "      \"mask_token\": \"[MASK]\",\n",
    "      \"model_max_length\": max_length,\n",
    "      \"max_len\": max_length,\n",
    "  }\n",
    "  json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6313e-cbf0-4ea7-8c17-959362fa3a18",
   "metadata": {},
   "source": [
    "Loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74c61334-b8fb-4491-8491-03846ea1d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file pretrained-bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file pretrained-bert\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained-bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676bbe8-8ab0-436b-a78f-3adb59895194",
   "metadata": {},
   "source": [
    "Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98949f26-89f0-4df8-ac79-4b6d7d6fd7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbfc7b79c3540b2b047cc6d11510b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/637416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'BertWordPieceTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m encode \u001b[38;5;241m=\u001b[39m encode_with_truncation \u001b[38;5;28;01mif\u001b[39;00m truncate_longer_samples \u001b[38;5;28;01melse\u001b[39;00m encode_without_truncation\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# tokenizing the train dataset\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(encode, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# tokenizing the testing dataset\u001b[39;00m\n\u001b[0;32m     15\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(encode, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3163\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:3552\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3548\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3549\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3550\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3552\u001b[0m     batch \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3553\u001b[0m         batch,\n\u001b[0;32m   3554\u001b[0m         indices,\n\u001b[0;32m   3555\u001b[0m         check_same_num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shard\u001b[38;5;241m.\u001b[39mlist_indexes()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   3556\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[0;32m   3557\u001b[0m     )\n\u001b[0;32m   3558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3561\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:3421\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3420\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3421\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3423\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3424\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3425\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m, in \u001b[0;36mencode_without_truncation\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_without_truncation\u001b[39m(examples):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_special_tokens_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'BertWordPieceTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)\n",
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "  train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "  # remove other columns, and remain them as Python lists\n",
    "  test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "  train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b52e3b-6dcc-4a66-878c-45d4a7cf8733",
   "metadata": {},
   "source": [
    "we need to join our untruncated samples together and cut them into fixed-size vectors since the model expects a fixed-sized sequence during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a803d8-53e8-4279-984e-c1f4b5334894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "  train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                  desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  # convert them from lists to torch tensors\n",
    "  train_dataset.set_format(\"torch\")\n",
    "  test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914d61d-7b19-4624-b154-496afd75e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c4c0a-6608-4692-a44f-65f51e32387e",
   "metadata": {},
   "source": [
    "Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d13b2-1096-4fe0-8b34-29881618d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca647501-39f4-43d0-8e01-b97434c1c1de",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6e146-3adc-4099-873d-6f8538e02a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29dfca-fcd9-427c-ab60-c5829a2beaf5",
   "metadata": {},
   "source": [
    "let's initialize our training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09c56f-57f7-41e5-9531-3fc9876b5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b623d-596d-4fe2-88e1-7162fa3bb2e9",
   "metadata": {},
   "source": [
    "Let's make our trainer now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfab78-261b-42a7-b18b-55af5ae9dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc68197-0e6d-4782-8566-840d43312e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
