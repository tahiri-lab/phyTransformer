
#https://huggingface.co/facebook/esm2_t6_8M_UR50D
#https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=9c3cf6da

from huggingface_hub import login

login(
  token="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", # ADD YOUR TOKEN HERE
  add_to_git_credential=True
)

import gzip
import pandas as pd
import os

# Définir les chemins des dossiers
datasets_dir = "../Datasets"
finetuning_dir = "Fine-tuning"

# Chemin complet du fichier .gz
file_path = os.path.join(datasets_dir, "Uniprot_subcellular_location.gz")

# Charger le fichier .gz et lire le contenu (supposons que c'est un fichier CSV)
with gzip.open(file_path, 'rt') as f:
    df = pd.read_csv(f, delimiter='\t')  # Exemple avec tabulation comme délimiteur


# Afficher les premières lignes pour vérifier le chargement
#print(df.head())

# Chemin où vous souhaitez enregistrer le dataset décompressé ou traité (facultatif)
output_path = os.path.join(datasets_dir, "Uniprot_subcellular_location.csv")
# Sauvegarder le dataset décompressé si nécessaire
df.to_csv(output_path, index=False)

df = df.dropna()  # Drop proteins with missing columns

cytosolic = df['Subcellular location [CC]'].str.contains("Cytosol") | df['Subcellular location [CC]'].str.contains("Cytoplasm")
membrane = df['Subcellular location [CC]'].str.contains("Membrane") | df['Subcellular location [CC]'].str.contains("Cell membrane")

cytosolic_df = df[cytosolic & ~membrane]
#print(cytosolic_df)

membrane_df = df[membrane & ~cytosolic]
#print(membrane_df)

cytosolic_sequences = cytosolic_df["Sequence"].tolist()
cytosolic_labels = [0 for protein in cytosolic_sequences]

membrane_sequences = membrane_df["Sequence"].tolist()
membrane_labels = [1 for protein in membrane_sequences]

sequences = cytosolic_sequences + membrane_sequences
labels = cytosolic_labels + membrane_labels

# Quick check to make sure we got it right
len(sequences) == len(labels)

from sklearn.model_selection import train_test_split

train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)

from transformers import AutoTokenizer

model_checkpoint = "facebook/esm2_t6_8M_UR50D"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

train_tokenized = tokenizer(train_sequences)
test_tokenized = tokenizer(test_sequences)

#tokenizer(train_sequences[0])

from datasets import Dataset
train_dataset = Dataset.from_dict(train_tokenized)
test_dataset = Dataset.from_dict(test_tokenized)
#print(train_dataset)

train_dataset = train_dataset.add_column("labels", train_labels)
test_dataset = test_dataset.add_column("labels", test_labels)
#print(train_dataset)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

num_labels = max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

model_name = model_checkpoint.split("/")[-1]
batch_size = 8

args = TrainingArguments(
    f"{model_name}-finetuned-localization",
    eval_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    push_to_hub=True,
)

from evaluate import load
import numpy as np

metric = load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()