{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paul\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"jonghyunlee/UniProt_function_text_descriptions\")\n",
    "\n",
    "# Diviser les donn√©es en train, validation et test\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U1263'), dtype('<U1263')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# display % of training data with label=1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprotein_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U1263'), dtype('<U1263')) -> None"
     ]
    }
   ],
   "source": [
    "# display % of training data with label=1\n",
    "np.array(dataset['train']['protein_name']).sum()/len(dataset['train']['protein_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464395/464395 [00:23<00:00, 19495.50 examples/s]\n",
      "c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Paul\\.cache\\huggingface\\hub\\models--dmis-lab--biobert-base-cased-v1.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# D√©finir le point de contr√¥le du mod√®le\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "\n",
    "# Extraire les s√©quences et les noms de prot√©ines\n",
    "sequences = dataset['train']['sequence']\n",
    "protein_names = dataset['train']['protein_name']\n",
    "\n",
    "# Cr√©er une liste unique des noms de prot√©ines (labels)\n",
    "label_names = list(set(protein_names))\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {label: i for i, label in enumerate(label_names)}\n",
    "\n",
    "# Ajouter les labels au jeu de donn√©es\n",
    "dataset = dataset.map(lambda x: {'labels': label2id[x['protein_name']]})\n",
    "\n",
    "# Charger le mod√®le\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=len(label_names), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=116815, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dotan1111/BioTokenizer-BFD-BPE-100\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # Extract text\n",
    "    text = examples[\"sequence\"]\n",
    "    \n",
    "    # Tokenize and truncate text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464395/464395 [00:28<00:00, 16337.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['entry', 'entry_name', 'protein_name', 'sequence', 'function', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 464395\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply untrained model to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "MVLSPADKTNVKAAW - Ribosomal protein S6 kinase beta-2 (S6K-beta-2) (S6K2) (EC 2.7.11.1) (70 kDa ribosomal protein S6 kinase 2) (p70 ribosomal S6 kinase beta) (p70 S6 kinase beta) (p70 S6K-beta) (p70 S6KB)\n",
      "GAGGAGAAGGTGTGGCG - Magnetosome protein MamA (MM24.3)\n",
      "MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNEKAGA - Ribosomal protein S6 kinase beta-2 (S6K-beta-2) (S6K2) (EC 2.7.11.1) (70 kDa ribosomal protein S6 kinase 2) (p70 ribosomal S6 kinase beta) (p70 S6 kinase beta) (p70 S6K-beta) (p70 S6KB)\n"
     ]
    }
   ],
   "source": [
    "# Define list of examples\n",
    "text_list = [\"MVLSPADKTNVKAAW\", \"GAGGAGAAGGTGTGGCG\", \"MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNEKAGA\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # Compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # Convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "    \n",
    "    print(text + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                         r=4,\n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.01,\n",
    "                         target_modules=['q_lin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=4, target_modules={'q_lin'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de configuration PEFT ajust√©e avec LoraConfig\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQUENCE_CLASSIFICATION\",\n",
    "    target_modules=[\n",
    "        \"bert.encoder.layer.0.attention.self.query\",\n",
    "        \"bert.encoder.layer.0.attention.self.key\",\n",
    "        \"bert.encoder.layer.0.attention.self.value\",\n",
    "        \"bert.encoder.layer.0.output.dense\",\n",
    "        \"bert.encoder.layer.1.attention.self.query\",\n",
    "        \"bert.encoder.layer.1.attention.self.key\",\n",
    "        \"bert.encoder.layer.1.attention.self.value\",\n",
    "        \"bert.encoder.layer.1.output.dense\",\n",
    "        \"bert.encoder.layer.2.attention.self.query\",\n",
    "        \"bert.encoder.layer.2.attention.self.key\",\n",
    "        \"bert.encoder.layer.2.attention.self.value\",\n",
    "        \"bert.encoder.layer.2.output.dense\",\n",
    "        \"bert.encoder.layer.3.attention.self.query\",\n",
    "        \"bert.encoder.layer.3.attention.self.key\",\n",
    "        \"bert.encoder.layer.3.attention.self.value\",\n",
    "        \"bert.encoder.layer.3.output.dense\",\n",
    "        # Continuez pour les autres couches si n√©cessaire\n",
    "    ],\n",
    "    r=8,  # Configuration sp√©cifique √† LoRA\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 270,336 || all params: 198,411,343 || trainable%: 0.1363\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bert\n",
      "bert.embeddings\n",
      "bert.embeddings.word_embeddings\n",
      "bert.embeddings.position_embeddings\n",
      "bert.embeddings.token_type_embeddings\n",
      "bert.embeddings.LayerNorm\n",
      "bert.embeddings.dropout\n",
      "bert.encoder\n",
      "bert.encoder.layer\n",
      "bert.encoder.layer.0\n",
      "bert.encoder.layer.0.attention\n",
      "bert.encoder.layer.0.attention.self\n",
      "bert.encoder.layer.0.attention.self.query\n",
      "bert.encoder.layer.0.attention.self.key\n",
      "bert.encoder.layer.0.attention.self.value\n",
      "bert.encoder.layer.0.attention.self.dropout\n",
      "bert.encoder.layer.0.attention.output\n",
      "bert.encoder.layer.0.attention.output.dense\n",
      "bert.encoder.layer.0.attention.output.LayerNorm\n",
      "bert.encoder.layer.0.attention.output.dropout\n",
      "bert.encoder.layer.0.intermediate\n",
      "bert.encoder.layer.0.intermediate.dense\n",
      "bert.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.0.output\n",
      "bert.encoder.layer.0.output.dense\n",
      "bert.encoder.layer.0.output.LayerNorm\n",
      "bert.encoder.layer.0.output.dropout\n",
      "bert.encoder.layer.1\n",
      "bert.encoder.layer.1.attention\n",
      "bert.encoder.layer.1.attention.self\n",
      "bert.encoder.layer.1.attention.self.query\n",
      "bert.encoder.layer.1.attention.self.key\n",
      "bert.encoder.layer.1.attention.self.value\n",
      "bert.encoder.layer.1.attention.self.dropout\n",
      "bert.encoder.layer.1.attention.output\n",
      "bert.encoder.layer.1.attention.output.dense\n",
      "bert.encoder.layer.1.attention.output.LayerNorm\n",
      "bert.encoder.layer.1.attention.output.dropout\n",
      "bert.encoder.layer.1.intermediate\n",
      "bert.encoder.layer.1.intermediate.dense\n",
      "bert.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.1.output\n",
      "bert.encoder.layer.1.output.dense\n",
      "bert.encoder.layer.1.output.LayerNorm\n",
      "bert.encoder.layer.1.output.dropout\n",
      "bert.encoder.layer.2\n",
      "bert.encoder.layer.2.attention\n",
      "bert.encoder.layer.2.attention.self\n",
      "bert.encoder.layer.2.attention.self.query\n",
      "bert.encoder.layer.2.attention.self.key\n",
      "bert.encoder.layer.2.attention.self.value\n",
      "bert.encoder.layer.2.attention.self.dropout\n",
      "bert.encoder.layer.2.attention.output\n",
      "bert.encoder.layer.2.attention.output.dense\n",
      "bert.encoder.layer.2.attention.output.LayerNorm\n",
      "bert.encoder.layer.2.attention.output.dropout\n",
      "bert.encoder.layer.2.intermediate\n",
      "bert.encoder.layer.2.intermediate.dense\n",
      "bert.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.2.output\n",
      "bert.encoder.layer.2.output.dense\n",
      "bert.encoder.layer.2.output.LayerNorm\n",
      "bert.encoder.layer.2.output.dropout\n",
      "bert.encoder.layer.3\n",
      "bert.encoder.layer.3.attention\n",
      "bert.encoder.layer.3.attention.self\n",
      "bert.encoder.layer.3.attention.self.query\n",
      "bert.encoder.layer.3.attention.self.key\n",
      "bert.encoder.layer.3.attention.self.value\n",
      "bert.encoder.layer.3.attention.self.dropout\n",
      "bert.encoder.layer.3.attention.output\n",
      "bert.encoder.layer.3.attention.output.dense\n",
      "bert.encoder.layer.3.attention.output.LayerNorm\n",
      "bert.encoder.layer.3.attention.output.dropout\n",
      "bert.encoder.layer.3.intermediate\n",
      "bert.encoder.layer.3.intermediate.dense\n",
      "bert.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.3.output\n",
      "bert.encoder.layer.3.output.dense\n",
      "bert.encoder.layer.3.output.LayerNorm\n",
      "bert.encoder.layer.3.output.dropout\n",
      "bert.encoder.layer.4\n",
      "bert.encoder.layer.4.attention\n",
      "bert.encoder.layer.4.attention.self\n",
      "bert.encoder.layer.4.attention.self.query\n",
      "bert.encoder.layer.4.attention.self.key\n",
      "bert.encoder.layer.4.attention.self.value\n",
      "bert.encoder.layer.4.attention.self.dropout\n",
      "bert.encoder.layer.4.attention.output\n",
      "bert.encoder.layer.4.attention.output.dense\n",
      "bert.encoder.layer.4.attention.output.LayerNorm\n",
      "bert.encoder.layer.4.attention.output.dropout\n",
      "bert.encoder.layer.4.intermediate\n",
      "bert.encoder.layer.4.intermediate.dense\n",
      "bert.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.4.output\n",
      "bert.encoder.layer.4.output.dense\n",
      "bert.encoder.layer.4.output.LayerNorm\n",
      "bert.encoder.layer.4.output.dropout\n",
      "bert.encoder.layer.5\n",
      "bert.encoder.layer.5.attention\n",
      "bert.encoder.layer.5.attention.self\n",
      "bert.encoder.layer.5.attention.self.query\n",
      "bert.encoder.layer.5.attention.self.key\n",
      "bert.encoder.layer.5.attention.self.value\n",
      "bert.encoder.layer.5.attention.self.dropout\n",
      "bert.encoder.layer.5.attention.output\n",
      "bert.encoder.layer.5.attention.output.dense\n",
      "bert.encoder.layer.5.attention.output.LayerNorm\n",
      "bert.encoder.layer.5.attention.output.dropout\n",
      "bert.encoder.layer.5.intermediate\n",
      "bert.encoder.layer.5.intermediate.dense\n",
      "bert.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.5.output\n",
      "bert.encoder.layer.5.output.dense\n",
      "bert.encoder.layer.5.output.LayerNorm\n",
      "bert.encoder.layer.5.output.dropout\n",
      "bert.encoder.layer.6\n",
      "bert.encoder.layer.6.attention\n",
      "bert.encoder.layer.6.attention.self\n",
      "bert.encoder.layer.6.attention.self.query\n",
      "bert.encoder.layer.6.attention.self.key\n",
      "bert.encoder.layer.6.attention.self.value\n",
      "bert.encoder.layer.6.attention.self.dropout\n",
      "bert.encoder.layer.6.attention.output\n",
      "bert.encoder.layer.6.attention.output.dense\n",
      "bert.encoder.layer.6.attention.output.LayerNorm\n",
      "bert.encoder.layer.6.attention.output.dropout\n",
      "bert.encoder.layer.6.intermediate\n",
      "bert.encoder.layer.6.intermediate.dense\n",
      "bert.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.6.output\n",
      "bert.encoder.layer.6.output.dense\n",
      "bert.encoder.layer.6.output.LayerNorm\n",
      "bert.encoder.layer.6.output.dropout\n",
      "bert.encoder.layer.7\n",
      "bert.encoder.layer.7.attention\n",
      "bert.encoder.layer.7.attention.self\n",
      "bert.encoder.layer.7.attention.self.query\n",
      "bert.encoder.layer.7.attention.self.key\n",
      "bert.encoder.layer.7.attention.self.value\n",
      "bert.encoder.layer.7.attention.self.dropout\n",
      "bert.encoder.layer.7.attention.output\n",
      "bert.encoder.layer.7.attention.output.dense\n",
      "bert.encoder.layer.7.attention.output.LayerNorm\n",
      "bert.encoder.layer.7.attention.output.dropout\n",
      "bert.encoder.layer.7.intermediate\n",
      "bert.encoder.layer.7.intermediate.dense\n",
      "bert.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.7.output\n",
      "bert.encoder.layer.7.output.dense\n",
      "bert.encoder.layer.7.output.LayerNorm\n",
      "bert.encoder.layer.7.output.dropout\n",
      "bert.encoder.layer.8\n",
      "bert.encoder.layer.8.attention\n",
      "bert.encoder.layer.8.attention.self\n",
      "bert.encoder.layer.8.attention.self.query\n",
      "bert.encoder.layer.8.attention.self.key\n",
      "bert.encoder.layer.8.attention.self.value\n",
      "bert.encoder.layer.8.attention.self.dropout\n",
      "bert.encoder.layer.8.attention.output\n",
      "bert.encoder.layer.8.attention.output.dense\n",
      "bert.encoder.layer.8.attention.output.LayerNorm\n",
      "bert.encoder.layer.8.attention.output.dropout\n",
      "bert.encoder.layer.8.intermediate\n",
      "bert.encoder.layer.8.intermediate.dense\n",
      "bert.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.8.output\n",
      "bert.encoder.layer.8.output.dense\n",
      "bert.encoder.layer.8.output.LayerNorm\n",
      "bert.encoder.layer.8.output.dropout\n",
      "bert.encoder.layer.9\n",
      "bert.encoder.layer.9.attention\n",
      "bert.encoder.layer.9.attention.self\n",
      "bert.encoder.layer.9.attention.self.query\n",
      "bert.encoder.layer.9.attention.self.key\n",
      "bert.encoder.layer.9.attention.self.value\n",
      "bert.encoder.layer.9.attention.self.dropout\n",
      "bert.encoder.layer.9.attention.output\n",
      "bert.encoder.layer.9.attention.output.dense\n",
      "bert.encoder.layer.9.attention.output.LayerNorm\n",
      "bert.encoder.layer.9.attention.output.dropout\n",
      "bert.encoder.layer.9.intermediate\n",
      "bert.encoder.layer.9.intermediate.dense\n",
      "bert.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.9.output\n",
      "bert.encoder.layer.9.output.dense\n",
      "bert.encoder.layer.9.output.LayerNorm\n",
      "bert.encoder.layer.9.output.dropout\n",
      "bert.encoder.layer.10\n",
      "bert.encoder.layer.10.attention\n",
      "bert.encoder.layer.10.attention.self\n",
      "bert.encoder.layer.10.attention.self.query\n",
      "bert.encoder.layer.10.attention.self.key\n",
      "bert.encoder.layer.10.attention.self.value\n",
      "bert.encoder.layer.10.attention.self.dropout\n",
      "bert.encoder.layer.10.attention.output\n",
      "bert.encoder.layer.10.attention.output.dense\n",
      "bert.encoder.layer.10.attention.output.LayerNorm\n",
      "bert.encoder.layer.10.attention.output.dropout\n",
      "bert.encoder.layer.10.intermediate\n",
      "bert.encoder.layer.10.intermediate.dense\n",
      "bert.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.10.output\n",
      "bert.encoder.layer.10.output.dense\n",
      "bert.encoder.layer.10.output.LayerNorm\n",
      "bert.encoder.layer.10.output.dropout\n",
      "bert.encoder.layer.11\n",
      "bert.encoder.layer.11.attention\n",
      "bert.encoder.layer.11.attention.self\n",
      "bert.encoder.layer.11.attention.self.query\n",
      "bert.encoder.layer.11.attention.self.key\n",
      "bert.encoder.layer.11.attention.self.value\n",
      "bert.encoder.layer.11.attention.self.dropout\n",
      "bert.encoder.layer.11.attention.output\n",
      "bert.encoder.layer.11.attention.output.dense\n",
      "bert.encoder.layer.11.attention.output.LayerNorm\n",
      "bert.encoder.layer.11.attention.output.dropout\n",
      "bert.encoder.layer.11.intermediate\n",
      "bert.encoder.layer.11.intermediate.dense\n",
      "bert.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "bert.encoder.layer.11.output\n",
      "bert.encoder.layer.11.output.dense\n",
      "bert.encoder.layer.11.output.LayerNorm\n",
      "bert.encoder.layer.11.output.dropout\n",
      "bert.pooler\n",
      "bert.pooler.dense\n",
      "bert.pooler.activation\n",
      "dropout\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "#pour voir les couches du modele\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create trainer object\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m----> 6\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m      7\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m      8\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator, \u001b[38;5;66;03m# this will dynamically pad examples in each batch to be equal length\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Paul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\dataset_dict.py:75\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     78\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     79\u001b[0m         ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "# Create trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu') # moving to cpu\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "    \n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
